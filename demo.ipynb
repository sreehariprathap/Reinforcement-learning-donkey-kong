{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Table after 50000 episodes:\n",
      "[[-0.0492369   0.02943397 -0.00288413  0.01757525]\n",
      " [-0.10233109  0.02385264 -0.0707276  -0.14933399]\n",
      " [ 0.02906495  0.00708541 -0.03501358  0.00748026]\n",
      " [-0.07649729 -0.02783598 -0.0403287  -0.0192394 ]\n",
      " [ 0.00932047  0.01452559 -0.04980954 -0.03145595]\n",
      " [-0.03979207 -0.02422997 -0.01137551 -0.04483987]\n",
      " [-0.00688771 -0.00296477 -0.0301759  -0.00843499]\n",
      " [-0.02094065  0.01035476 -0.06280117  0.02339754]\n",
      " [-0.03745759 -0.11238694  0.00853358 -0.13587449]\n",
      " [ 0.01746955 -0.09678515 -0.0150934   0.01448133]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Dummy Environment for Demonstration\n",
    "# ------------------------------------------------------------\n",
    "class DummyEnv:\n",
    "    \"\"\"\n",
    "    A simple dummy environment.\n",
    "    It assumes:\n",
    "    - The environment has 'num_states' discrete states.\n",
    "    - There are 'num_actions' possible actions.\n",
    "    - The 'step' function returns a random next state, a random reward,\n",
    "      and a 10% chance to terminate the episode.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.current_state = 0\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self.current_state = state\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Randomly transition to a new state\n",
    "        next_state = np.random.choice(range(self.num_states))\n",
    "        # Generate a random reward (for example, normally distributed)\n",
    "        reward = np.random.randn()\n",
    "        # End the episode with a 10% probability\n",
    "        done = np.random.rand() < 0.1\n",
    "        info = {}\n",
    "        return next_state, reward, done, info\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define State and Action Spaces\n",
    "# ------------------------------------------------------------\n",
    "num_states = 10      # Example: 10 discrete states\n",
    "num_actions = 4      # Example: 4 possible actions\n",
    "S = range(num_states)\n",
    "A = range(num_actions)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialize Q-Table and Other Variables\n",
    "# ------------------------------------------------------------\n",
    "# Q-table: one row per state and one column per action, initialized to zeros.\n",
    "Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# Arrays to accumulate returns for each state-action pair.\n",
    "returns_sum = np.zeros((num_states, num_actions))\n",
    "returns_count = np.zeros((num_states, num_actions))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Define the Epsilon-Greedy Policy\n",
    "# ------------------------------------------------------------\n",
    "def epsilon_greedy_policy(state, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Chooses a random action with probability epsilon,\n",
    "    otherwise returns the action with the highest Q-value for the given state.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(list(A))\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Implementing Exploring Starts\n",
    "# ------------------------------------------------------------\n",
    "def exploring_starts():\n",
    "    \"\"\"\n",
    "    Randomly selects a starting state and action,\n",
    "    ensuring every state-action pair can be explored.\n",
    "    \"\"\"\n",
    "    s0 = np.random.choice(list(S))\n",
    "    a0 = np.random.choice(list(A))\n",
    "    return s0, a0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Generate an Episode Using the Current Policy\n",
    "# ------------------------------------------------------------\n",
    "def generate_episode(env, start_state, start_action):\n",
    "    \"\"\"\n",
    "    Generates an episode starting from a given state and action.\n",
    "    Returns a list of (state, action, reward) tuples.\n",
    "    \"\"\"\n",
    "    env.set_state(start_state)\n",
    "    state = start_state\n",
    "    action = start_action\n",
    "    episode = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        if not done:\n",
    "            action = epsilon_greedy_policy(next_state)\n",
    "        state = next_state\n",
    "        \n",
    "    return episode\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialize the Environment\n",
    "# ------------------------------------------------------------\n",
    "env = DummyEnv(num_states, num_actions)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Monte Carlo Exploring Starts (MC ES) Main Loop\n",
    "# ------------------------------------------------------------\n",
    "num_episodes = 50000  # Example number of episodes\n",
    "\n",
    "for episode_i in range(num_episodes):\n",
    "    # 1. Choose a random starting state-action pair (Exploring Starts)\n",
    "    s0, a0 = exploring_starts()\n",
    "    \n",
    "    # 2. Generate an episode using the current policy\n",
    "    episode = generate_episode(env, s0, a0)\n",
    "    \n",
    "    # 3. Process the episode in reverse to compute returns and update Q-values.\n",
    "    G = 0\n",
    "    # Iterate over the episode in reverse order\n",
    "    for t in reversed(range(len(episode))):\n",
    "        s_t, a_t, reward = episode[t]\n",
    "        G += reward\n",
    "        \n",
    "        # First-visit check: update only if (s_t, a_t) is not seen earlier in the episode.\n",
    "        if (s_t, a_t) not in [(episode[i][0], episode[i][1]) for i in range(t)]:\n",
    "            returns_sum[s_t, a_t] += G\n",
    "            returns_count[s_t, a_t] += 1.0\n",
    "            Q[s_t, a_t] = returns_sum[s_t, a_t] / returns_count[s_t, a_t]\n",
    "\n",
    "    # 4. The policy is implicitly updated by the epsilon-greedy function,\n",
    "    #    which always selects the action with the highest Q-value.\n",
    "    \n",
    "# ------------------------------------------------------------\n",
    "# Final Output\n",
    "# ------------------------------------------------------------\n",
    "print(\"Final Q-Table after {} episodes:\".format(num_episodes))\n",
    "print(Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
