{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Atari Donkey Kong with DQN\n",
    "\n",
    "This notebook implements a DQN agent to play the Atari game Donkey Kong, with the following features:\n",
    "- Parallel training across multiple game environments\n",
    "- Game frame preprocessing for improved training efficiency\n",
    "- Prioritized experience replay to enhance training quality\n",
    "- Training log recording\n",
    "- Regular model saving\n",
    "- Periodic evaluation and game video recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "%pip install stable-baselines3[extra] gymnasium[atari] numpy matplotlib opencv-python tensorboard autorom[accept-rom-license] ipywidgets gymnasium[other]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, FrameStackObservation\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "import ale_py\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment parameters\n",
    "ENV_NAME = \"ALE/DonkeyKong-v5\"\n",
    "NUM_ENVS = 4  # Number of parallel environments\n",
    "FRAME_SKIP = 4  # Frame skip, make decisions every 4 frames\n",
    "ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]  # Valid actions\n",
    "\n",
    "# Model parameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # Discount factor\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 100000  # Experience replay buffer size\n",
    "TARGET_UPDATE = 10000  # Target network update frequency\n",
    "\n",
    "# Training parameters\n",
    "NUM_FRAMES = 10_000_000  # Total training frames\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 6_000_000\n",
    "DEMO_PATH = \"./demo/dk_demo_20250325_192148.pkl\"\n",
    "\n",
    "# Save and evaluation parameters\n",
    "SAVE_INTERVAL = 100_000  # Model save interval (frames)\n",
    "EVAL_INTERVAL = 20_000   # Model evaluation interval (frames)\n",
    "EVAL_EPISODES = 3        # Number of episodes per evaluation\n",
    "\n",
    "# Create directories for saving models and logs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_PATH = f\"./models/donkey_kong_{timestamp}\"\n",
    "LOG_PATH = f\"./logs/donkey_kong_{timestamp}\"\n",
    "VIDEO_PATH = f\"./videos/donkey_kong_{timestamp}\"\n",
    "\n",
    "for path in [SAVE_PATH, LOG_PATH, VIDEO_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict action space, reduce agent's useless actions\n",
    "class ActionRestrictWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, allowed_actions):\n",
    "        super().__init__(env)\n",
    "        self.allowed_actions = allowed_actions\n",
    "        self.action_space = spaces.Discrete(len(self.allowed_actions))\n",
    "\n",
    "    def action(self, act):\n",
    "        # Map the action index output by the agent to the original action number\n",
    "        return self.allowed_actions[act]\n",
    "\n",
    "    def reverse_action(self, act):\n",
    "        return self.allowed_actions.index(act)\n",
    "\n",
    "# Wrapper that forces the first action to be FIRE\n",
    "class ForceFirstFireWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.first_action_done = False\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.first_action_done = False\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # If it's the first action and not RIGHT FIRE, force replace it with RIGHT FIRE\n",
    "        if not self.first_action_done:\n",
    "            self.first_action_done = True\n",
    "            # Use RIGHT FIRE action\n",
    "            action_idx = ALLOWED_ACTIONS.index(11)\n",
    "            return self.env.step(action_idx)\n",
    "        return self.env.step(action)\n",
    "\n",
    "# Function to detect player position based on color\n",
    "def get_agent_position(frame): \n",
    "    \"\"\" Detect player position by color, return (x, y) coordinates. Returns None if not detected. \"\"\"\n",
    "    # Ensure frame is numpy array with correct format\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Target color (BGR format)\n",
    "    target_bgr = np.array([194, 64, 82], dtype=np.uint8)\n",
    "\n",
    "    # Tolerance range (adjustable, usually 20~40 works well)\n",
    "    tolerance = 30\n",
    "    lower = np.array([max(0, c - tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "    upper = np.array([min(255, c + tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "\n",
    "    # Generate mask\n",
    "    mask = cv2.inRange(frame, lower, upper)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # Find the largest contour by area\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "    M = cv2.moments(largest)\n",
    "\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "    return (cx, cy)\n",
    "\n",
    "# Custom video display wrapper for showing actions and agent position\n",
    "class VideoDisplayWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_action = None\n",
    "        self.action_names = {\n",
    "            0: \"\",\n",
    "            1: \"Jump\",\n",
    "            2: \"Up\",\n",
    "            3: \"Right\",\n",
    "            4: \"Left\",\n",
    "            5: \"Down\",\n",
    "            11: \"Jump R\",\n",
    "            12: \"Jump L\"\n",
    "        }\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Record current action\n",
    "        self.current_action = action\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.current_action = None\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def render(self):\n",
    "        # Get original rendered frame\n",
    "        frame = self.env.render()\n",
    "        \n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        # Ensure frame is RGB format\n",
    "        if len(frame.shape) == 2:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # 1. Display current action in top right corner\n",
    "        if self.current_action is not None:\n",
    "            action_name = self.action_names.get(self.current_action, f\"ACTION_{self.current_action}\")\n",
    "            cv2.putText(frame, \n",
    "                       action_name, \n",
    "                       (frame.shape[1] - 85, 28), # Top right position\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.3,\n",
    "                       (255, 255, 255), # White text\n",
    "                       1, \n",
    "                       cv2.LINE_AA)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "# Custom reward wrapper to adjust rewards based on agent position changes\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, y_static_penalty=0.1, up_success_reward=10,\n",
    "                 up_fail_penalty=0, x_static_penalty=0,\n",
    "                 y_threshold=20, x_threshold=3, \n",
    "                 y_static_frames=30, x_static_frames=30):\n",
    "        super().__init__(env)\n",
    "        # Reward parameters\n",
    "        self.y_static_penalty = y_static_penalty  # Vertical static penalty\n",
    "        self.up_success_reward = up_success_reward  # Successful upward movement reward\n",
    "        self.up_fail_penalty = up_fail_penalty  # Failed upward movement penalty\n",
    "        self.x_static_penalty = x_static_penalty  # Horizontal static penalty\n",
    "        \n",
    "        # Threshold parameters\n",
    "        self.y_threshold = y_threshold  # Vertical movement threshold\n",
    "        self.x_threshold = x_threshold  # Horizontal movement threshold\n",
    "        self.y_static_frames = y_static_frames  # Vertical static frame count\n",
    "        self.x_static_frames = x_static_frames  # Horizontal static frame count\n",
    "        \n",
    "        # State tracking\n",
    "        self.prev_positions = []  # Store past positions [(x, y), ...]\n",
    "        self.y_static_count = 0  # Vertical static counter\n",
    "        self.x_static_count = 0  # Horizontal static counter\n",
    "        self.prev_action = None  # Previous action\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Reset state tracking\n",
    "        self.prev_positions = []\n",
    "        self.y_static_count = 0\n",
    "        self.x_static_count = 0\n",
    "        self.prev_action = None\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Record current action\n",
    "        self.prev_action = action\n",
    "        \n",
    "        # Execute environment step\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Extract RGB frame from observation\n",
    "        frame = None\n",
    "        try:\n",
    "            if isinstance(obs, np.ndarray):\n",
    "                if len(obs.shape) == 4:  # (stack, height, width, channel)\n",
    "                    frame = obs[-1]  # Last frame\n",
    "                elif len(obs.shape) == 3:  # (height, width, channel)\n",
    "                    frame = obs\n",
    "                elif len(obs.shape) == 2:  # (height, width)\n",
    "                    frame = obs\n",
    "            elif hasattr(obs, '__getitem__'):\n",
    "                # For FrameStackObservation\n",
    "                try:\n",
    "                    frame = obs[-1]\n",
    "                except:\n",
    "                    try:\n",
    "                        frame = obs[3]  # Assuming 4 frame stack\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # If above attempts fail, try rendering the environment\n",
    "            if frame is None:\n",
    "                try:\n",
    "                    frame = self.env.render()\n",
    "                except:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract frame from observation: {e}\")\n",
    "            frame = None\n",
    "        \n",
    "        # Detect Agent position\n",
    "        position = None\n",
    "        if frame is not None:\n",
    "            position = get_agent_position(frame)\n",
    "        \n",
    "        # If position detected, update position history and calculate reward adjustment\n",
    "        additional_reward = 0\n",
    "        \n",
    "        if position is not None:\n",
    "            x, y = position\n",
    "            self.prev_positions.append((x, y))\n",
    "            \n",
    "            # Keep history at reasonable size\n",
    "            if len(self.prev_positions) > max(self.y_static_frames, self.x_static_frames):\n",
    "                self.prev_positions.pop(0)\n",
    "            \n",
    "            # Need at least two position records to determine movement\n",
    "            if len(self.prev_positions) >= 2:\n",
    "                prev_x, prev_y = self.prev_positions[-2]\n",
    "                \n",
    "                # 1. Check if vertically static\n",
    "                if abs(y - prev_y) < self.y_threshold:\n",
    "                    self.y_static_count += 1\n",
    "                    if self.y_static_count >= self.y_static_frames:\n",
    "                        # Linearly increasing penalty\n",
    "                        additional_reward -= self.y_static_penalty * (self.y_static_count - self.y_static_frames + 1)\n",
    "                else:\n",
    "                    self.y_static_count = 0\n",
    "                \n",
    "                # 2. Check UP action effect\n",
    "                if self.prev_action == 2:  # Assuming 2 is UP action\n",
    "                    if (prev_y - y) > self.y_threshold:  # Successful upward movement\n",
    "                        additional_reward += self.up_success_reward\n",
    "                    else:  # Failed upward movement\n",
    "                        additional_reward -= self.up_fail_penalty\n",
    "                \n",
    "                # 3. Check if horizontally static\n",
    "                if abs(x - prev_x) < self.x_threshold:\n",
    "                    self.x_static_count += 1\n",
    "                    if self.x_static_count >= self.x_static_frames:\n",
    "                        # Linearly increasing penalty\n",
    "                        additional_reward -= self.x_static_penalty * (self.x_static_count - self.x_static_frames + 1)\n",
    "                else:\n",
    "                    self.x_static_count = 0\n",
    "        \n",
    "        # Apply reward adjustment\n",
    "        adjusted_reward = reward + additional_reward\n",
    "        \n",
    "        return obs, adjusted_reward, terminated, truncated, info\n",
    "\n",
    "# Function to create preprocessed environment\n",
    "def make_env(env_id, idx, capture_video=False, run_name=None):\n",
    "    def thunk():\n",
    "        import ale_py\n",
    "        \n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            # Add video display wrapper\n",
    "            env = VideoDisplayWrapper(env)\n",
    "            env = RecordVideo(\n",
    "                env,\n",
    "                VIDEO_PATH,\n",
    "                episode_trigger=lambda x: True,\n",
    "                name_prefix=f\"{run_name}\"\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        env = ActionRestrictWrapper(env, ALLOWED_ACTIONS)\n",
    "        env = ForceFirstFireWrapper(env)\n",
    "        env = CustomRewardWrapper(env)\n",
    "        env = AtariWrapper(env, terminal_on_life_loss=True, frame_skip=FRAME_SKIP)\n",
    "        env = FrameStackObservation(env, 4)  # Stack 4 frames to capture temporal information\n",
    "            \n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# Create parallel environments\n",
    "def make_vec_env(env_id, num_envs, seed=SEED):\n",
    "    env_fns = [make_env(env_id, i) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(env_fns)\n",
    "    envs.seed(seed)\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DQN Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, stack_frames, height, width)\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use prioritized experience replay to improve training efficiency\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Controls the degree of prioritization\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1  # Current frame, used for beta calculation\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        # beta increases linearly from beta_start to 1.0\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # Add new experience\n",
    "        max_prio = np.max(self.priorities) if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(self.Transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.position] = self.Transition(*args)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        # Calculate sampling probabilities\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # Convert to batch processing format\n",
    "        batch = self.Transition(*zip(*samples))\n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.tensor(batch.action, device=device)\n",
    "        rewards = torch.tensor(batch.reward, device=device, dtype=torch.float32)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        dones = torch.tensor(batch.done, device=device, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # Update priorities\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Create policy network and target network\n",
    "        self.policy_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target network doesn't need gradient calculation\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Create experience replay buffer\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # Training related parameters\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # Logger\n",
    "        self.writer = SummaryWriter(LOG_PATH)\n",
    "    \n",
    "    def select_action(self, state, eval_mode=False):\n",
    "        # ε-greedy policy for action selection\n",
    "        sample = random.random()\n",
    "        # In evaluation mode, always choose the best action\n",
    "        if eval_mode:\n",
    "            eps_threshold = 0.05  # Use small epsilon in eval mode for some exploration\n",
    "        else:\n",
    "            # Linear epsilon decay\n",
    "            self.epsilon = max(EPSILON_END, EPSILON_START - self.steps_done / EPSILON_DECAY)\n",
    "            eps_threshold = self.epsilon\n",
    "            \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0  # Not enough samples in buffer\n",
    "        \n",
    "        # Sample from experience replay buffer\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # Calculate current Q values\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Use Double DQN to calculate next state Q values\n",
    "        # Use policy network to select actions\n",
    "        next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        # Use target network to evaluate actions\n",
    "        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "        # Set next Q values for terminal states to 0\n",
    "        next_q_values = next_q_values * (1 - dones)\n",
    "        # Calculate target Q values\n",
    "        target_q_values = rewards + GAMMA * next_q_values\n",
    "        \n",
    "        # Calculate loss (TD error)\n",
    "        td_error = torch.abs(q_values - target_q_values).detach().cpu().numpy()\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values, reduction='none') * weights\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities\n",
    "        self.memory.update_priorities(indices, td_error + 1e-5)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps_done': self.steps_done,\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Observation Preprocessing and State Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # Convert stacked 4 frames to PyTorch input format\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    return tensor / 255.0  # Normalize\n",
    "\n",
    "def preprocess_batch_observation(obs):\n",
    "    # Process batch observation data\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "    return tensor / 255.0  # Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env_id, num_episodes=5, video_prefix=\"evaluation\"):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # Create a new environment instance for each evaluation episode\n",
    "    for i in range(num_episodes):\n",
    "        # Create new environment instance for each game round\n",
    "        env = make_env(env_id, 0, capture_video=True, run_name=f\"{video_prefix}_episode_{i}\")()\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        env.close()  # Close the environment after each episode\n",
    "\n",
    "        # Modify video filename, remove extra suffix\n",
    "        video_path = os.path.abspath(os.path.join(VIDEO_PATH, f\"{video_prefix}_episode_{i}-episode-0.mp4\"))\n",
    "        if os.path.exists(video_path):\n",
    "            new_video_path = video_path.replace(\"-episode-0.mp4\", \".mp4\")\n",
    "            os.rename(video_path, new_video_path)\n",
    "    \n",
    "    return np.mean(episode_rewards), np.std(episode_rewards), episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, envs, num_frames):\n",
    "    # Initialize environment and progress bar\n",
    "    obs = envs.reset()\n",
    "    obs_tensor = preprocess_batch_observation(obs)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = np.zeros(NUM_ENVS)\n",
    "    episode_length = np.zeros(NUM_ENVS)\n",
    "    \n",
    "    progress_bar = tqdm(range(1, num_frames + 1), desc=\"Training\")\n",
    "    \n",
    "    # Training loop\n",
    "    for frame_idx in progress_bar:\n",
    "        # Select actions\n",
    "        actions = []\n",
    "        for i in range(NUM_ENVS):\n",
    "            action = agent.select_action(obs_tensor[i:i+1])\n",
    "            actions.append(action.item())\n",
    "        \n",
    "        # Execute actions\n",
    "        next_obs, rewards, terminateds, truncateds = envs.step(actions)\n",
    "        \n",
    "        # Process data for each environment\n",
    "        dones = []\n",
    "        for t, tr in zip(terminateds, truncateds):\n",
    "            if isinstance(tr, dict):\n",
    "                done = t or tr.get(\"TimeLimit.truncated\", False)\n",
    "            else:\n",
    "                done = t or tr\n",
    "            dones.append(done)\n",
    "\n",
    "        next_obs_tensor = preprocess_batch_observation(next_obs)\n",
    "        \n",
    "        # Update cumulative rewards and episode length\n",
    "        episode_reward += rewards\n",
    "        episode_length += 1\n",
    "        \n",
    "        # Store data in experience replay buffer\n",
    "        for i in range(NUM_ENVS):\n",
    "            agent.memory.push(\n",
    "                obs_tensor[i:i+1],\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_obs_tensor[i:i+1],\n",
    "                float(dones[i])\n",
    "            )\n",
    "        \n",
    "        # Update observations\n",
    "        obs = next_obs\n",
    "        obs_tensor = next_obs_tensor\n",
    "        \n",
    "        # Optimize model\n",
    "        loss = agent.optimize_model()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Check for episode completion\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                # Record episode results\n",
    "                agent.writer.add_scalar(\"train/episode_reward\", episode_reward[i], agent.steps_done)\n",
    "                agent.writer.add_scalar(\"train/episode_length\", episode_length[i], agent.steps_done)\n",
    "                all_rewards.append(episode_reward[i])\n",
    "                \n",
    "                # Reset episode statistics\n",
    "                episode_reward[i] = 0\n",
    "                episode_length[i] = 0\n",
    "        \n",
    "        # Update target network\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Record training statistics\n",
    "        if frame_idx % 1000 == 0:\n",
    "            mean_reward = np.mean(all_rewards[-100:]) if all_rewards else 0\n",
    "            mean_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            agent.writer.add_scalar(\"train/epsilon\", agent.epsilon, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/loss\", mean_loss, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/mean_reward_100\", mean_reward, frame_idx)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"avg_reward\": f\"{mean_reward:.2f}\",\n",
    "                \"loss\": f\"{mean_loss:.5f}\",\n",
    "                \"epsilon\": f\"{agent.epsilon:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # Save model\n",
    "        if frame_idx % SAVE_INTERVAL == 0:\n",
    "            save_path = os.path.join(SAVE_PATH, f\"model_{frame_idx}.pt\")\n",
    "            agent.save_model(save_path)\n",
    "            print(f\"\\nFrame {frame_idx}: Model saved to {save_path}\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        if frame_idx % EVAL_INTERVAL == 0:\n",
    "            print(f\"\\nFrame {frame_idx}: Evaluating...\")\n",
    "            eval_reward, eval_std, _ = evaluate(\n",
    "                agent,\n",
    "                ENV_NAME,\n",
    "                num_episodes=EVAL_EPISODES,\n",
    "                video_prefix=f\"eval_{frame_idx}\"\n",
    "            )\n",
    "            agent.writer.add_scalar(\"eval/mean_reward\", eval_reward, frame_idx)\n",
    "            agent.writer.add_scalar(\"eval/reward_std\", eval_std, frame_idx)\n",
    "            print(f\"Evaluation results: Mean reward = {eval_reward:.2f} ± {eval_std:.2f}\")\n",
    "        \n",
    "        # Update agent's step counter\n",
    "        agent.steps_done += 1\n",
    "    \n",
    "    # Save final model after training\n",
    "    final_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "    agent.save_model(final_path)\n",
    "    print(f\"\\nFinal model saved to: {final_path}\")\n",
    "\n",
    "\n",
    "def load_demonstrations(agent, filepath):\n",
    "    \"\"\"Load demonstration trajectories from file and inject them into the agent's replay buffer\"\"\"\n",
    "    import pickle\n",
    "\n",
    "    # Use the same ALLOWED_ACTIONS as in training\n",
    "    ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]\n",
    "    action_to_index = {a: i for i, a in enumerate(ALLOWED_ACTIONS)}\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        all_trajectories = pickle.load(f)\n",
    "\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "    for traj in all_trajectories:\n",
    "        for s, a, r, ns, d in traj:\n",
    "            if a not in action_to_index:\n",
    "                print(f\"Skipping illegal action: {a}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            action_index = action_to_index[a]  # Map to 0~7\n",
    "\n",
    "            agent.memory.push(\n",
    "                s.to(device),\n",
    "                action_index,\n",
    "                r.to(device),\n",
    "                ns.to(device),\n",
    "                d.to(device)\n",
    "            )\n",
    "            count += 1\n",
    "\n",
    "    print(f\"\\nDemonstrations imported, {count} transitions added to replay buffer. Skipped {skipped} illegal actions.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parallel environments\n",
    "envs = make_vec_env(ENV_NAME, NUM_ENVS)\n",
    "\n",
    "# Get environment information\n",
    "obs_shape = (4, 84, 84)  # 4 stacked frames, each 84x84\n",
    "n_actions = envs.action_space.n\n",
    "\n",
    "print(f\"\\nObservation shape: {obs_shape}\")\n",
    "print(f\"Action space size: {n_actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DQN agent\n",
    "agent = DQNAgent(obs_shape, n_actions)\n",
    "\n",
    "# Load demonstration trajectories if available\n",
    "if DEMO_PATH and os.path.exists(DEMO_PATH):\n",
    "    Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    load_demonstrations(agent, DEMO_PATH)\n",
    "else:\n",
    "    print(f\"Demonstration file not found at {DEMO_PATH}, skipping demonstration loading.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\nTraining started...\\n\")\n",
    "\n",
    "# Enable cudnn benchmark mode to improve training speed\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "train(agent, envs, NUM_FRAMES)\n",
    "\n",
    "# Close environments\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load and Test Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_video(model_path, env_id, num_episodes=5):\n",
    "    # Create agent and load model\n",
    "    obs_shape = (4, 84, 84)  # 4 stacked frames, each 84x84\n",
    "    env = make_env(env_id, 0)()\n",
    "    n_actions = env.action_space.n\n",
    "    agent = DQNAgent(obs_shape, n_actions)\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    # Test trained agent\n",
    "    mean, std, rewards = evaluate(agent, env_id, num_episodes=num_episodes, video_prefix=\"final_test\")\n",
    "\n",
    "    for i, reward in enumerate(rewards):\n",
    "        print(f\"Episode {i+1}: Reward = {reward}\")\n",
    "        \n",
    "    print(f\"\\nAverage reward: {mean:.2f} ± {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the final model\n",
    "# model_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "model_path = \"./model_final.pt\"\n",
    "play_and_record_video(model_path, ENV_NAME, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def continue_training(model_path, envs, additional_frames=1_000_000, frames_per_session=200_000):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create agent with the same configuration\n",
    "    obs_shape = (4, 84, 84)\n",
    "    n_actions = envs.action_space.n\n",
    "    agent = DQNAgent(obs_shape, n_actions)\n",
    "    \n",
    "    # Load the existing model\n",
    "    print(f\"Loading model from {model_path}\")\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    # Create new save paths for the continued training\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_path = f\"./models/donkey_kong_continued_{timestamp}\"\n",
    "    log_path = f\"./logs/donkey_kong_continued_{timestamp}\"\n",
    "    video_path = f\"./videos/donkey_kong_continued_{timestamp}\"\n",
    "    \n",
    "    for path in [save_path, log_path, video_path]:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    # Setup logging\n",
    "    agent.writer = SummaryWriter(log_path)\n",
    "    \n",
    "    # Modify the train function to work with our custom save path\n",
    "    global SAVE_PATH, LOG_PATH, VIDEO_PATH\n",
    "    original_save_path = SAVE_PATH\n",
    "    original_log_path = LOG_PATH\n",
    "    original_video_path = VIDEO_PATH\n",
    "    \n",
    "    # Track training statistics\n",
    "    episode_count = 0\n",
    "    best_reward = float('-inf')\n",
    "    \n",
    "    # Use our custom paths for this training session\n",
    "    SAVE_PATH = save_path\n",
    "    LOG_PATH = log_path\n",
    "    VIDEO_PATH = video_path\n",
    "    \n",
    "    # Metrics tracking for plotting\n",
    "    metrics = {\n",
    "        'episode_rewards': [],\n",
    "        'episode_lengths': [],\n",
    "        'losses': [],\n",
    "        'mean_rewards_100': [],\n",
    "        'eval_rewards': [],\n",
    "        'epsilons': [],\n",
    "        'frames': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nContinuing training for {additional_frames} frames from step {agent.steps_done}...\\n\")\n",
    "    print(f\"Training in sessions of {frames_per_session} frames each to manage memory usage\")\n",
    "    \n",
    "    # Calculate how many sessions needed\n",
    "    total_sessions = additional_frames // frames_per_session\n",
    "    if additional_frames % frames_per_session > 0:\n",
    "        total_sessions += 1\n",
    "    \n",
    "    # Train in smaller sessions to avoid memory issues\n",
    "    frames_remaining = additional_frames\n",
    "    session_start_time = time.time()\n",
    "    overall_start_time = session_start_time\n",
    "    \n",
    "    for session in range(1, total_sessions + 1):\n",
    "        # Find latest checkpoint if this isn't the first session\n",
    "        if session > 1:\n",
    "            # Get latest model file\n",
    "            model_files = [f for f in os.listdir(save_path) if f.startswith(\"model_\") and f.endswith(\".pt\")]\n",
    "            if model_files:\n",
    "                # Modified sorting function to handle 'model_best.pt'\n",
    "                def get_model_number(filename):\n",
    "                    parts = filename.split(\"_\")[1].split(\".\")[0]\n",
    "                    if parts == \"best\":\n",
    "                        return float('inf')  # Make 'best' sort to the end\n",
    "                    try:\n",
    "                        return int(parts)\n",
    "                    except ValueError:\n",
    "                        return -1  # For any other non-integer names\n",
    "                \n",
    "                latest_model = sorted(model_files, key=get_model_number)[-1]\n",
    "                latest_model_path = os.path.join(save_path, latest_model)\n",
    "                print(f\"\\nResuming from checkpoint: {latest_model_path}\")\n",
    "                agent.load_model(latest_model_path)\n",
    "        \n",
    "        # Rest of the code remains the same until the training loop...\n",
    "        \n",
    "        # Calculate frames for this session\n",
    "        session_frames = min(frames_per_session, frames_remaining)\n",
    "        print(f\"\\nStarting training session {session}/{total_sessions} ({session_frames} frames)\")\n",
    "        \n",
    "        # Initialize environment and progress bar\n",
    "        obs = envs.reset()\n",
    "        obs_tensor = preprocess_batch_observation(obs)\n",
    "        \n",
    "        losses = []\n",
    "        all_rewards = []\n",
    "        episode_reward = np.zeros(NUM_ENVS)\n",
    "        episode_length = np.zeros(NUM_ENVS)\n",
    "        \n",
    "        progress_bar = tqdm(range(1, session_frames + 1), desc=f\"Session {session}/{total_sessions}\")\n",
    "        \n",
    "        try:\n",
    "            # Training loop\n",
    "            for frame_idx in progress_bar:\n",
    "                # Code remains the same until the episode completion check...\n",
    "                \n",
    "                # Select actions\n",
    "                actions = []\n",
    "                for i in range(NUM_ENVS):\n",
    "                    action = agent.select_action(obs_tensor[i:i+1])\n",
    "                    actions.append(action.item())\n",
    "                \n",
    "                # Execute actions\n",
    "                next_obs, rewards, terminateds, truncateds = envs.step(actions)\n",
    "                \n",
    "                # Process data for each environment\n",
    "                dones = []\n",
    "                for t, tr in zip(terminateds, truncateds):\n",
    "                    if isinstance(tr, dict):\n",
    "                        done = t or tr.get(\"TimeLimit.truncated\", False)\n",
    "                    else:\n",
    "                        done = t or tr\n",
    "                    dones.append(done)\n",
    "\n",
    "                next_obs_tensor = preprocess_batch_observation(next_obs)\n",
    "                \n",
    "                # Update cumulative rewards and episode length\n",
    "                episode_reward += rewards\n",
    "                episode_length += 1\n",
    "                \n",
    "                # Store data in experience replay buffer\n",
    "                for i in range(NUM_ENVS):\n",
    "                    agent.memory.push(\n",
    "                        obs_tensor[i:i+1],\n",
    "                        actions[i],\n",
    "                        rewards[i],\n",
    "                        next_obs_tensor[i:i+1],\n",
    "                        float(dones[i])\n",
    "                    )\n",
    "                \n",
    "                # Update observations\n",
    "                obs = next_obs\n",
    "                obs_tensor = next_obs_tensor\n",
    "                \n",
    "                # Optimize model\n",
    "                loss = agent.optimize_model()\n",
    "                if loss is not None:\n",
    "                    losses.append(loss)\n",
    "                \n",
    "                # Check for episode completion\n",
    "                for i, done in enumerate(dones):\n",
    "                    if done:\n",
    "                        # Record episode results\n",
    "                        agent.writer.add_scalar(\"train/episode_reward\", episode_reward[i], agent.steps_done)\n",
    "                        agent.writer.add_scalar(\"train/episode_length\", episode_length[i], agent.steps_done)\n",
    "                        all_rewards.append(episode_reward[i])\n",
    "                        \n",
    "                        # Store metrics for plotting\n",
    "                        metrics['episode_rewards'].append(episode_reward[i])\n",
    "                        metrics['episode_lengths'].append(episode_length[i])\n",
    "                        metrics['frames'].append(agent.steps_done)\n",
    "                        \n",
    "                        episode_count += 1\n",
    "                        \n",
    "                        # Track best reward\n",
    "                        if episode_reward[i] > best_reward:\n",
    "                            best_reward = episode_reward[i]\n",
    "                            print(f\"\\nNew best reward: {best_reward:.2f} at episode {episode_count}\")\n",
    "                            # Save best model\n",
    "                            best_model_path = os.path.join(save_path, \"model_best.pt\")\n",
    "                            agent.save_model(best_model_path)\n",
    "\n",
    "                        # Print detailed episode information\n",
    "                        elapsed = time.time() - overall_start_time\n",
    "                        print(f\"\\nEpisode {episode_count} completed in env {i}:\")\n",
    "                        print(f\"  Steps: {episode_length[i]}\")\n",
    "                        print(f\"  Reward: {episode_reward[i]:.2f}\")\n",
    "                        print(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "                        if losses:\n",
    "                            print(f\"  Loss: {np.mean(losses[-100:]):.6f}\")\n",
    "                        print(f\"  Total frames: {agent.steps_done}\")\n",
    "                        print(f\"  Elapsed time: {elapsed/60:.2f} minutes\")\n",
    "                        print(f\"  Frames per second: {agent.steps_done/elapsed:.2f}\")\n",
    "                        \n",
    "                        # Generate and save plots periodically\n",
    "                        if episode_count % 10 == 0:\n",
    "                            plot_training_progress(metrics, save_path)\n",
    "                        \n",
    "                        # Reset episode statistics\n",
    "                        episode_reward[i] = 0\n",
    "                        episode_length[i] = 0\n",
    "                \n",
    "                # Update target network\n",
    "                if agent.steps_done % TARGET_UPDATE == 0:\n",
    "                    agent.update_target_network()\n",
    "                    print(f\"\\nFrame {agent.steps_done}: Target network updated\")\n",
    "                \n",
    "                # Record training statistics\n",
    "                if frame_idx % 1000 == 0:\n",
    "                    mean_reward = np.mean(all_rewards[-100:]) if all_rewards else 0\n",
    "                    mean_loss = np.mean(losses[-100:]) if losses else 0\n",
    "                    \n",
    "                    # Store for plotting\n",
    "                    metrics['mean_rewards_100'].append(mean_reward)\n",
    "                    metrics['losses'].append(mean_loss)\n",
    "                    metrics['epsilons'].append(agent.epsilon)\n",
    "                    \n",
    "                    agent.writer.add_scalar(\"train/epsilon\", agent.epsilon, agent.steps_done)\n",
    "                    agent.writer.add_scalar(\"train/loss\", mean_loss, agent.steps_done)\n",
    "                    agent.writer.add_scalar(\"train/mean_reward_100\", mean_reward, agent.steps_done)\n",
    "                    \n",
    "                    progress_bar.set_postfix({\n",
    "                        \"avg_reward\": f\"{mean_reward:.2f}\",\n",
    "                        \"loss\": f\"{mean_loss:.5f}\",\n",
    "                        \"epsilon\": f\"{agent.epsilon:.2f}\"\n",
    "                    })\n",
    "                \n",
    "                # Save model every SAVE_INTERVAL steps and also at the end of each session\n",
    "                if agent.steps_done % SAVE_INTERVAL == 0:\n",
    "                    save_path_checkpoint = os.path.join(save_path, f\"model_{agent.steps_done}.pt\")\n",
    "                    agent.save_model(save_path_checkpoint)\n",
    "                    print(f\"\\nFrame {agent.steps_done}: Model saved to {save_path_checkpoint}\")\n",
    "                \n",
    "                # Evaluate model\n",
    "                if agent.steps_done % EVAL_INTERVAL == 0:\n",
    "                    print(f\"\\nFrame {agent.steps_done}: Evaluating...\")\n",
    "                    eval_reward, eval_std, eval_rewards = evaluate(\n",
    "                        agent,\n",
    "                        ENV_NAME,\n",
    "                        num_episodes=EVAL_EPISODES,\n",
    "                        video_prefix=f\"eval_{agent.steps_done}\"\n",
    "                    )\n",
    "                    \n",
    "                    # Store evaluation metrics\n",
    "                    metrics['eval_rewards'].append(eval_reward)\n",
    "                    \n",
    "                    agent.writer.add_scalar(\"eval/mean_reward\", eval_reward, agent.steps_done)\n",
    "                    agent.writer.add_scalar(\"eval/reward_std\", eval_std, agent.steps_done)\n",
    "                    \n",
    "                    print(f\"Evaluation results: Mean reward = {eval_reward:.2f} ± {eval_std:.2f}\")\n",
    "                    for i, r in enumerate(eval_rewards):\n",
    "                        print(f\"  Eval episode {i+1}: Reward = {r:.2f}\")\n",
    "                \n",
    "                # Update agent's step counter\n",
    "                agent.steps_done += 1\n",
    "            \n",
    "            # Save model at the end of each session\n",
    "            session_end_model_path = os.path.join(save_path, f\"model_{agent.steps_done}.pt\")\n",
    "            agent.save_model(session_end_model_path)\n",
    "            print(f\"\\nSession {session} complete. Model saved to {session_end_model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Handle any exceptions (like CUDA OOM) by saving current progress\n",
    "            print(f\"\\nError encountered: {e}\")\n",
    "            error_model_path = os.path.join(save_path, f\"model_error_{agent.steps_done}.pt\")\n",
    "            agent.save_model(error_model_path)\n",
    "            print(f\"Model saved at error point: {error_model_path}\")\n",
    "            print(\"You can resume training from this checkpoint.\")\n",
    "        \n",
    "        # Update frames_remaining for next session\n",
    "        frames_remaining -= session_frames\n",
    "        \n",
    "        # Print session summary\n",
    "        session_time = time.time() - session_start_time\n",
    "        print(f\"\\nSession {session} Summary:\")\n",
    "        print(f\"Frames processed: {session_frames}\")\n",
    "        print(f\"Session time: {session_time/60:.2f} minutes\")\n",
    "        print(f\"Frames per second: {session_frames/session_time:.2f}\")\n",
    "        \n",
    "        # Generate and save plots at the end of each session\n",
    "        plot_training_progress(metrics, save_path)\n",
    "        \n",
    "        # Reset for next session\n",
    "        session_start_time = time.time()\n",
    "    \n",
    "    # Save final model after all training\n",
    "    final_path = os.path.join(save_path, \"model_final.pt\")\n",
    "    agent.save_model(final_path)\n",
    "    print(f\"\\nFinal model saved to: {final_path}\")\n",
    "    \n",
    "    # Generate final comprehensive plots\n",
    "    plot_training_progress(metrics, save_path, final=True)\n",
    "    \n",
    "    # Save metrics as CSV for later analysis\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'frames': metrics['frames'],\n",
    "        'episode_rewards': metrics['episode_rewards'],\n",
    "        'episode_lengths': metrics['episode_lengths']\n",
    "    })\n",
    "    metrics_df.to_csv(os.path.join(save_path, 'training_metrics.csv'), index=False)\n",
    "    \n",
    "    # Print overall training summary\n",
    "    elapsed_time = time.time() - overall_start_time\n",
    "    print(\"\\n===== Training Summary =====\")\n",
    "    print(f\"Total episodes: {episode_count}\")\n",
    "    print(f\"Total frames: {agent.steps_done}\")\n",
    "    print(f\"Best reward: {best_reward:.2f}\")\n",
    "    print(f\"Average reward (last 100): {np.mean(all_rewards[-100:]):.2f}\")\n",
    "    print(f\"Final epsilon: {agent.epsilon:.4f}\")\n",
    "    print(f\"Total training time: {elapsed_time/60:.2f} minutes\")\n",
    "    print(f\"Frames per second: {agent.steps_done/elapsed_time:.2f}\")\n",
    "    \n",
    "    # Restore original paths\n",
    "    SAVE_PATH = original_save_path\n",
    "    LOG_PATH = original_log_path\n",
    "    VIDEO_PATH = original_video_path\n",
    "    \n",
    "    return final_path, metrics_df\n",
    "\n",
    "def plot_training_progress(metrics, save_path, final=False):\n",
    "    \"\"\"Generate and save plots showing training progress.\"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Plot episode rewards\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(metrics['frames'], metrics['episode_rewards'], 'b-')\n",
    "    plt.title('Episode Rewards Over Time')\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot mean rewards (100-episode rolling average)\n",
    "    if len(metrics['mean_rewards_100']) > 0:\n",
    "        plt.subplot(3, 2, 2)\n",
    "        x_frames = [i * 1000 for i in range(len(metrics['mean_rewards_100']))]\n",
    "        plt.plot(x_frames, metrics['mean_rewards_100'], 'g-')\n",
    "        plt.title('Mean Reward (Last 100 Episodes)')\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Mean Reward')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # Plot episode lengths\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(metrics['frames'], metrics['episode_lengths'], 'r-')\n",
    "    plt.title('Episode Lengths Over Time')\n",
    "    plt.xlabel('Frames')\n",
    "    plt.ylabel('Steps')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot losses\n",
    "    if len(metrics['losses']) > 0:\n",
    "        plt.subplot(3, 2, 4)\n",
    "        x_frames = [i * 1000 for i in range(len(metrics['losses']))]\n",
    "        plt.plot(x_frames, metrics['losses'], 'm-')\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # Plot epsilon decay\n",
    "    if len(metrics['epsilons']) > 0:\n",
    "        plt.subplot(3, 2, 5)\n",
    "        x_frames = [i * 1000 for i in range(len(metrics['epsilons']))]\n",
    "        plt.plot(x_frames, metrics['epsilons'], 'k-')\n",
    "        plt.title('Epsilon Decay')\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Epsilon')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    # Plot evaluation rewards\n",
    "    if len(metrics['eval_rewards']) > 0:\n",
    "        plt.subplot(3, 2, 6)\n",
    "        x_frames = [i * EVAL_INTERVAL for i in range(len(metrics['eval_rewards']))]\n",
    "        plt.plot(x_frames, metrics['eval_rewards'], 'c-')\n",
    "        plt.title('Evaluation Rewards')\n",
    "        plt.xlabel('Frames')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_filename = \"final_training_plots.png\" if final else f\"training_plots_{len(metrics['frames'])}.png\"\n",
    "    plt.savefig(os.path.join(save_path, plot_filename))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## How to Continue Training\n",
    "\n",
    "To continue training from an existing model, uncomment the code in the cell below. This will create a new parallel environment, load the specified model checkpoint, and continue training for the desired number of frames. The training will be divided into smaller sessions to manage memory usage better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # Create parallel environments\n",
    "# envs = make_vec_env(ENV_NAME, NUM_ENVS)\n",
    "\n",
    "# print(f\"Number of actions: {envs.action_space.n}\")\n",
    "\n",
    "# # Continue training from your existing model\n",
    "# model_path = \"./models/donkey_kong_continued_20250410_174640/model_error_10021320.pt\"\n",
    "# final_model_path = continue_training(model_path, envs, additional_frames=1_000_000, frames_per_session=200_000)\n",
    "\n",
    "# # Close environments\n",
    "# envs.close()\n",
    "\n",
    "# print(f\"\\nContinued training complete. Final model saved at {final_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
