{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium\n",
    "\n",
    "Gymnasium is a project that provides an API (application programming interface) for various single-agent reinforcement learning environments. It includes implementations of common environments such as CartPole, Pendulum, MountainCar, MuJoCo, Atari, and more. This page outlines the basics of how to use Gymnasium, focusing on its four key functions: `make()`, `Env.reset()`, `Env.step()`, and `Env.render()`.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "At the core of Gymnasium is the `Env` class, a high-level Python class representing a Markov Decision Process (MDP) from reinforcement learning theory. Note that this is not a perfect reconstruction and is missing several components of MDPs. The `Env` class provides users with the ability to:\n",
    "\n",
    "- Generate an initial state\n",
    "- Transition/move to new states given an action\n",
    "- Visualize the environment\n",
    "\n",
    "Additionally, Gymnasium provides `Wrapper` classes to help augment or modify the environment, particularly the agent's observations, rewards, and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Functions\n",
    "\n",
    "#### `make()`\n",
    "This function will return an `Env` for users to interact with. To see all environments you can create, use `pprint_registry()`. Furthermore, `make()` provides a number of additional arguments for specifying keywords to the environment, adding more or less wrappers, etc. See `make()` for more information.\n",
    "\n",
    "#### `Env.reset()`\n",
    "Generates an initial state for the environment. This function is used to reset the environment to its initial state, which is useful for starting a new episode.\n",
    "\n",
    "#### `Env.step()`\n",
    "Transitions/moves to new states given an action. This function takes an action as input and returns the next state, reward, done (a boolean indicating if the episode has ended), and additional info.\n",
    "\n",
    "#### `Env.render()`\n",
    "Visualizes the environment. This function is used to render the current state of the environment, which can be useful for debugging and understanding the agent's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with the Environment\n",
    "\n",
    "In reinforcement learning, the classic “agent-environment loop” pictured below is a simplified representation of how an agent and environment interact with each other. The agent receives an observation about the environment, the agent then selects an action, which the environment uses to determine the reward and the next observation. The cycle then repeats itself until the environment ends (terminates).\n",
    "\n",
    "![Agent-Environment Loop](../Assets//AE_loop_dark.png)\n",
    "\n",
    "For Gymnasium, the “agent-environment-loop” is implemented below for a single episode (until the environment ends). See the next section for a line-by-line explanation. Note that running this code requires installing swig (`pip install swig` or download) along with `pip install \"gymnasium[box2d]\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    episode_over = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining the Code\n",
    "\n",
    "First, an environment is created using `make()` with the `render_mode` keyword to specify visualization. In this example, we use the \"LunarLander\" environment where the agent controls a spaceship that needs to land safely.\n",
    "\n",
    "After initializing the environment, we `Env.reset()` to get the first observation and additional info. To initialize with a specific random seed or options, use the `seed` or `options` parameters with `reset()`.\n",
    "\n",
    "We define `episode_over` to know when to stop interacting with the environment and use a while loop that checks this variable.\n",
    "\n",
    "The agent performs an action in the environment, and `Env.step()` executes this action (randomly chosen with `env.action_space.sample()`). This updates the environment, providing a new observation and a reward. This action-observation exchange is called a timestep.\n",
    "\n",
    "The environment may end after some timesteps, reaching a terminal state. If the environment has terminated, `step()` returns `terminated` as `True`. Similarly, the environment may issue a `truncated` signal after a fixed number of timesteps. If either `terminated` or `truncated` is `True`, the episode ends. To restart, use `env.reset()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action and Observation Spaces\n",
    "\n",
    "Every environment specifies the format of valid actions and observations with the `action_space` and `observation_space` attributes. This helps in understanding the expected input and output of the environment. In the example above, we sampled random actions via `env.action_space.sample()` instead of using an agent policy.\n",
    "\n",
    "`Env.action_space` and `Env.observation_space` are instances of `Space`, a high-level Python class with key functions: `Space.contains()` and `Space.sample()`. Gymnasium supports various spaces:\n",
    "\n",
    "- **Box**: Bounded space with upper and lower limits of any n-dimensional shape.\n",
    "- **Discrete**: Discrete space where {0, 1, ..., n-1} are the possible values.\n",
    "- **MultiBinary**: Binary space of any n-dimensional shape.\n",
    "- **MultiDiscrete**: Series of Discrete action spaces with different numbers of actions.\n",
    "- **Text**: String space with a minimum and maximum length.\n",
    "- **Dict**: Dictionary of simpler spaces.\n",
    "- **Tuple**: Tuple of simple spaces.\n",
    "- **Graph**: Mathematical graph with interlinking nodes and edges.\n",
    "- **Sequence**: Variable length of simpler space elements.\n",
    "\n",
    "For example usage of spaces, see their documentation along with utility functions. There are also niche spaces like Graph, Sequence, and Text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying the Environment\n",
    "\n",
    "Wrappers are a convenient way to modify an existing environment without altering the underlying code directly. Using wrappers helps avoid boilerplate code and makes your environment more modular. Wrappers can also be chained to combine their effects. Most environments generated via `gymnasium.make()` are already wrapped by default using `TimeLimit`, `OrderEnforcing`, and `PassiveEnvChecker`.\n",
    "\n",
    "To wrap an environment, first initialize a base environment. Then pass this environment along with optional parameters to the wrapper’s constructor:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
