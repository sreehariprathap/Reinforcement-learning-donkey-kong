{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用DQN训练Atari Donkey Kong\n",
    "\n",
    "本notebook实现了一个DQN代理来玩Atari游戏Donkey Kong，并包含以下特性：\n",
    "- 并行训练多个游戏环境\n",
    "- 预处理游戏帧以提高训练效率\n",
    "- 使用优先经验回放提高训练质量\n",
    "- 训练日志记录\n",
    "- 定期保存模型\n",
    "- 定期评估并录制游戏视频"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装必要的依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的库\n",
    "# %pip install stable-baselines3[extra] gymnasium[atari] numpy matplotlib opencv-python tensorboard autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, FrameStackObservation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "import ale_py\n",
    "from gymnasium import spaces\n",
    "\n",
    "# 设置随机种子，保证实验可复现\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 环境参数\n",
    "ENV_NAME = \"ALE/DonkeyKong-v5\"\n",
    "NUM_ENVS = 4  # 并行环境数量\n",
    "FRAME_SKIP = 4  # 跳帧数，每隔4帧进行一次决策\n",
    "ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]  # 有效动作\n",
    "\n",
    "# 模型参数\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # 折扣因子\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 100000  # 经验回放缓冲区大小\n",
    "TARGET_UPDATE = 10000  # 目标网络更新频率\n",
    "\n",
    "# 训练参数\n",
    "NUM_FRAMES = 10_000_000  # 总训练帧数\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 6_000_000\n",
    "DEMO_PATH = \"./demo/dk_demo_20250325_192148.pkl\"\n",
    "\n",
    "# 保存和评估参数\n",
    "SAVE_INTERVAL = 100_000  # 保存模型的间隔（帧数）\n",
    "EVAL_INTERVAL = 20_000   # 评估模型的间隔（帧数）\n",
    "EVAL_EPISODES = 3       # 每次评估的游戏局数\n",
    "\n",
    "# 创建保存模型和日志的目录\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_PATH = f\"./models/donkey_kong_{timestamp}\"\n",
    "LOG_PATH = f\"./logs/donkey_kong_{timestamp}\"\n",
    "VIDEO_PATH = f\"./videos/donkey_kong_{timestamp}\"\n",
    "\n",
    "for path in [SAVE_PATH, LOG_PATH, VIDEO_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# 设置设备（GPU或CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 环境预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 限制动作空间,减少 agent 的无用动作\n",
    "class ActionRestrictWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, allowed_actions):\n",
    "        super().__init__(env)\n",
    "        self.allowed_actions = allowed_actions\n",
    "        self.action_space = spaces.Discrete(len(self.allowed_actions))\n",
    "\n",
    "    def action(self, act):\n",
    "        # 把 agent 输出的动作索引映射成原动作编号\n",
    "        return self.allowed_actions[act]\n",
    "\n",
    "    def reverse_action(self, act):\n",
    "        return self.allowed_actions.index(act)\n",
    "\n",
    "# 强制首个动作为FIRE的包装器\n",
    "class ForceFirstFireWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.first_action_done = False\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.first_action_done = False\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 如果是首个动作且不是 RIGHT FIRE，则强制替换为 RIGHT FIRE\n",
    "        if not self.first_action_done:\n",
    "            self.first_action_done = True\n",
    "            # 使用 RIGHT FIRE 动作\n",
    "            action_idx = ALLOWED_ACTIONS.index(11)\n",
    "            return self.env.step(action_idx)\n",
    "        return self.env.step(action)\n",
    "\n",
    "# 根据颜色检测人物位置的函数\n",
    "def get_agent_position(frame): \n",
    "    \"\"\" 根据颜色检测人物位置，返回 (x, y) 坐标。未检测到则返回 None。 \"\"\"\n",
    "    # 确保frame是numpy数组且格式正确\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # 目标颜色（BGR 格式）\n",
    "    target_bgr = np.array([194, 64, 82], dtype=np.uint8)\n",
    "\n",
    "    # 容差范围（可调，20~40 一般比较合适）\n",
    "    tolerance = 30\n",
    "    lower = np.array([max(0, c - tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "    upper = np.array([min(255, c + tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "\n",
    "    # 生成掩码\n",
    "    mask = cv2.inRange(frame, lower, upper)\n",
    "    \n",
    "    # 查找轮廓\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # 找面积最大轮廓\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "    M = cv2.moments(largest)\n",
    "\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "    return (cx, cy)\n",
    "\n",
    "# 自定义视频显示包装器，用于在视频中显示动作和代理位置\n",
    "class VideoDisplayWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_action = None\n",
    "        self.action_names = {\n",
    "            0: \"\",\n",
    "            1: \"Jump\",\n",
    "            2: \"Up\",\n",
    "            3: \"Right\",\n",
    "            4: \"Left\",\n",
    "            5: \"Down\",\n",
    "            11: \"Jump R\",\n",
    "            12: \"Jump L\"\n",
    "        }\n",
    "        \n",
    "    def step(self, action):\n",
    "        # 记录当前动作\n",
    "        self.current_action = action\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.current_action = None\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def render(self):\n",
    "        # 获取原始渲染帧\n",
    "        frame = self.env.render()\n",
    "        \n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        # 确保帧是RGB格式\n",
    "        if len(frame.shape) == 2:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # 1. 在右上角显示当前动作\n",
    "        if self.current_action is not None:\n",
    "            action_name = self.action_names.get(self.current_action, f\"ACTION_{self.current_action}\")\n",
    "            cv2.putText(frame, \n",
    "                       action_name, \n",
    "                       (frame.shape[1] - 85, 28), # 右上角位置\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.3,\n",
    "                       (255, 255, 255), # 白色文本\n",
    "                       1, \n",
    "                       cv2.LINE_AA)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "# 自定义奖励包装器，用于根据Agent的位置变化调整奖励\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, y_static_penalty=0.1, up_success_reward=10,\n",
    "                 up_fail_penalty=0, x_static_penalty=0,\n",
    "                 y_threshold=20, x_threshold=3, \n",
    "                 y_static_frames=30, x_static_frames=30):\n",
    "        super().__init__(env)\n",
    "        # 奖励参数\n",
    "        self.y_static_penalty = y_static_penalty  # 垂直静止惩罚\n",
    "        self.up_success_reward = up_success_reward  # 成功向上移动奖励\n",
    "        self.up_fail_penalty = up_fail_penalty  # 向上失败惩罚\n",
    "        self.x_static_penalty = x_static_penalty  # 水平静止惩罚\n",
    "        \n",
    "        # 阈值参数\n",
    "        self.y_threshold = y_threshold  # 垂直移动阈值\n",
    "        self.x_threshold = x_threshold  # 水平移动阈值\n",
    "        self.y_static_frames = y_static_frames  # 垂直静止判定帧数\n",
    "        self.x_static_frames = x_static_frames  # 水平静止判定帧数\n",
    "        \n",
    "        # 状态记录\n",
    "        self.prev_positions = []  # 存储过去的位置 [(x, y), ...]\n",
    "        self.y_static_count = 0  # 垂直静止计数\n",
    "        self.x_static_count = 0  # 水平静止计数\n",
    "        self.prev_action = None  # 上一个动作\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # 重置状态记录\n",
    "        self.prev_positions = []\n",
    "        self.y_static_count = 0\n",
    "        self.x_static_count = 0\n",
    "        self.prev_action = None\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 记录当前动作\n",
    "        self.prev_action = action\n",
    "        \n",
    "        # 执行环境步骤\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # 从观察中提取RGB帧\n",
    "        frame = None\n",
    "        try:\n",
    "            if isinstance(obs, np.ndarray):\n",
    "                if len(obs.shape) == 4:  # (stack, height, width, channel)\n",
    "                    frame = obs[-1]  # 最后一帧\n",
    "                elif len(obs.shape) == 3:  # (height, width, channel)\n",
    "                    frame = obs\n",
    "                elif len(obs.shape) == 2:  # (height, width)\n",
    "                    frame = obs\n",
    "            elif hasattr(obs, '__getitem__'):\n",
    "                # 对于 FrameStackObservation\n",
    "                try:\n",
    "                    frame = obs[-1]\n",
    "                except:\n",
    "                    try:\n",
    "                        frame = obs[3]  # 假设是4帧堆叠\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # 如果上述尝试都失败，尝试渲染环境\n",
    "            if frame is None:\n",
    "                try:\n",
    "                    frame = self.env.render()\n",
    "                except:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract frame from observation: {e}\")\n",
    "            frame = None\n",
    "        \n",
    "        # 检测Agent位置\n",
    "        position = None\n",
    "        if frame is not None:\n",
    "            position = get_agent_position(frame)\n",
    "        \n",
    "        # 如果检测到位置，则更新位置历史并计算奖励调整\n",
    "        additional_reward = 0\n",
    "        \n",
    "        if position is not None:\n",
    "            x, y = position\n",
    "            self.prev_positions.append((x, y))\n",
    "            \n",
    "            # 保持历史记录在合理大小\n",
    "            if len(self.prev_positions) > max(self.y_static_frames, self.x_static_frames):\n",
    "                self.prev_positions.pop(0)\n",
    "            \n",
    "            # 至少有两个位置记录才能判断移动\n",
    "            if len(self.prev_positions) >= 2:\n",
    "                prev_x, prev_y = self.prev_positions[-2]\n",
    "                \n",
    "                # 1. 检查垂直方向是否静止\n",
    "                if abs(y - prev_y) < self.y_threshold:\n",
    "                    self.y_static_count += 1\n",
    "                    if self.y_static_count >= self.y_static_frames:\n",
    "                        # 线性增加惩罚\n",
    "                        additional_reward -= self.y_static_penalty * (self.y_static_count - self.y_static_frames + 1)\n",
    "                else:\n",
    "                    self.y_static_count = 0\n",
    "                \n",
    "                # 2. 检查UP动作的效果\n",
    "                if self.prev_action == 2:  # 假设2是UP动作\n",
    "                    if (prev_y - y) > self.y_threshold:  # 成功向上移动\n",
    "                        additional_reward += self.up_success_reward\n",
    "                    else:  # 未成功向上移动\n",
    "                        additional_reward -= self.up_fail_penalty\n",
    "                \n",
    "                # 3. 检查水平方向是否静止\n",
    "                if abs(x - prev_x) < self.x_threshold:\n",
    "                    self.x_static_count += 1\n",
    "                    if self.x_static_count >= self.x_static_frames:\n",
    "                        # 线性增加惩罚\n",
    "                        additional_reward -= self.x_static_penalty * (self.x_static_count - self.x_static_frames + 1)\n",
    "                else:\n",
    "                    self.x_static_count = 0\n",
    "        \n",
    "        # 应用奖励调整\n",
    "        adjusted_reward = reward + additional_reward\n",
    "        \n",
    "        return obs, adjusted_reward, terminated, truncated, info\n",
    "\n",
    "# 创建预处理后的环境的函数\n",
    "def make_env(env_id, idx, capture_video=False, run_name=None):\n",
    "    def thunk():\n",
    "        import ale_py\n",
    "        \n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            # 添加视频显示包装器\n",
    "            env = VideoDisplayWrapper(env)\n",
    "            env = RecordVideo(\n",
    "                env,\n",
    "                VIDEO_PATH,\n",
    "                episode_trigger=lambda x: True,\n",
    "                name_prefix=f\"{run_name}\"\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        env = ActionRestrictWrapper(env, ALLOWED_ACTIONS)\n",
    "        env = ForceFirstFireWrapper(env)\n",
    "        env = CustomRewardWrapper(env)\n",
    "        env = AtariWrapper(env, terminal_on_life_loss=True, frame_skip=FRAME_SKIP)\n",
    "        env = FrameStackObservation(env, 4)  # 堆叠4帧以捕获时间信息\n",
    "            \n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# 创建并行环境\n",
    "def make_vec_env(env_id, num_envs, seed=SEED):\n",
    "    env_fns = [make_env(env_id, i) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(env_fns)\n",
    "    envs.seed(seed)\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DQN网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 输入形状: (batch, stack_frames, height, width)\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 优先经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用优先经验回放提高训练效率\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # 控制优先级的程度\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1  # 当前帧，用于beta计算\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        # beta从beta_start线性增加到1.0\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # 添加新的经验\n",
    "        max_prio = np.max(self.priorities) if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(self.Transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.position] = self.Transition(*args)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        # 计算采样概率\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # 计算重要性采样权重\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # 转换为批量处理格式\n",
    "        batch = self.Transition(*zip(*samples))\n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.tensor(batch.action, device=device)\n",
    "        rewards = torch.tensor(batch.reward, device=device, dtype=torch.float32)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        dones = torch.tensor(batch.done, device=device, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # 更新优先级\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DQN代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # 创建策略网络和目标网络\n",
    "        self.policy_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # 目标网络不需要计算梯度\n",
    "        \n",
    "        # 设置优化器\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # 创建经验回放缓冲区\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # 训练相关参数\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # 日志记录器\n",
    "        self.writer = SummaryWriter(LOG_PATH)\n",
    "    \n",
    "    def select_action(self, state, eval_mode=False):\n",
    "        # ε-贪婪策略选择动作\n",
    "        sample = random.random()\n",
    "        # 在评估模式下，始终选择最佳动作\n",
    "        if eval_mode:\n",
    "            eps_threshold = 0.05  # 评估时使用小的epsilon，增加一些探索性\n",
    "        else:\n",
    "            # 线性衰减epsilon\n",
    "            self.epsilon = max(EPSILON_END, EPSILON_START - self.steps_done / EPSILON_DECAY)\n",
    "            eps_threshold = self.epsilon\n",
    "            \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0  # 缓冲区中的样本不足\n",
    "        \n",
    "        # 从经验回放缓冲区中采样\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # 计算当前Q值\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # 使用Double DQN计算下一个状态的Q值\n",
    "        # 使用策略网络选择动作\n",
    "        next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        # 使用目标网络评估动作\n",
    "        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "        # 将终止状态的下一个Q值设为0\n",
    "        next_q_values = next_q_values * (1 - dones)\n",
    "        # 计算目标Q值\n",
    "        target_q_values = rewards + GAMMA * next_q_values\n",
    "        \n",
    "        # 计算损失（TD误差）\n",
    "        td_error = torch.abs(q_values - target_q_values).detach().cpu().numpy()\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values, reduction='none') * weights\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # 优化模型\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 更新优先级\n",
    "        self.memory.update_priorities(indices, td_error + 1e-5)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps_done': self.steps_done,\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 预处理和状态转换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # 把堆叠的4帧图像转换为PyTorch的输入格式\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    return tensor / 255.0  # 归一化\n",
    "\n",
    "def preprocess_batch_observation(obs):\n",
    "    # 处理批量观察数据\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "    return tensor / 255.0  # 归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env_id, num_episodes=5, video_prefix=\"evaluation\"):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # 对每轮评估都创建一个新的环境实例\n",
    "    for i in range(num_episodes):\n",
    "        # 每轮游戏创建新的环境实例\n",
    "        env = make_env(env_id, 0, capture_video=True, run_name=f\"{video_prefix}_episode_{i}\")()\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        env.close()  # 每轮结束后关闭环境\n",
    "\n",
    "        # 修改视频文件名，去除多余的后缀\n",
    "        video_path = os.path.abspath(os.path.join(VIDEO_PATH, f\"{video_prefix}_episode_{i}-episode-0.mp4\"))\n",
    "        if os.path.exists(video_path):\n",
    "            new_video_path = video_path.replace(\"-episode-0.mp4\", \".mp4\")\n",
    "            os.rename(video_path, new_video_path)\n",
    "    \n",
    "    return np.mean(episode_rewards), np.std(episode_rewards), episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, envs, num_frames):\n",
    "    # 初始化环境和进度条\n",
    "    obs = envs.reset()\n",
    "    obs_tensor = preprocess_batch_observation(obs)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = np.zeros(NUM_ENVS)\n",
    "    episode_length = np.zeros(NUM_ENVS)\n",
    "    \n",
    "    progress_bar = tqdm(range(1, num_frames + 1), desc=\"Training\")\n",
    "    \n",
    "    # 训练循环\n",
    "    for frame_idx in progress_bar:\n",
    "        # 选择动作\n",
    "        actions = []\n",
    "        for i in range(NUM_ENVS):\n",
    "            action = agent.select_action(obs_tensor[i:i+1])\n",
    "            actions.append(action.item())\n",
    "        \n",
    "        # 执行动作\n",
    "        next_obs, rewards, terminateds, truncateds = envs.step(actions)\n",
    "        \n",
    "        # 处理每个环境的数据\n",
    "        dones = []\n",
    "        for t, tr in zip(terminateds, truncateds):\n",
    "            if isinstance(tr, dict):\n",
    "                done = t or tr.get(\"TimeLimit.truncated\", False)\n",
    "            else:\n",
    "                done = t or tr\n",
    "            dones.append(done)\n",
    "\n",
    "        next_obs_tensor = preprocess_batch_observation(next_obs)\n",
    "        \n",
    "        # 更新累计奖励和回合长度\n",
    "        episode_reward += rewards\n",
    "        episode_length += 1\n",
    "        \n",
    "        # 将数据存入经验回放缓冲区\n",
    "        for i in range(NUM_ENVS):\n",
    "            agent.memory.push(\n",
    "                obs_tensor[i:i+1],\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_obs_tensor[i:i+1],\n",
    "                float(dones[i])\n",
    "            )\n",
    "        \n",
    "        # 更新观察\n",
    "        obs = next_obs\n",
    "        obs_tensor = next_obs_tensor\n",
    "        \n",
    "        # 优化模型\n",
    "        loss = agent.optimize_model()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # 检查是否有回合结束\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                # 记录回合结果\n",
    "                agent.writer.add_scalar(\"train/episode_reward\", episode_reward[i], agent.steps_done)\n",
    "                agent.writer.add_scalar(\"train/episode_length\", episode_length[i], agent.steps_done)\n",
    "                all_rewards.append(episode_reward[i])\n",
    "                \n",
    "                # 重置回合统计\n",
    "                episode_reward[i] = 0\n",
    "                episode_length[i] = 0\n",
    "        \n",
    "        # 更新目标网络\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # 记录训练统计信息\n",
    "        if frame_idx % 1000 == 0:\n",
    "            mean_reward = np.mean(all_rewards[-100:]) if all_rewards else 0\n",
    "            mean_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            agent.writer.add_scalar(\"train/epsilon\", agent.epsilon, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/loss\", mean_loss, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/mean_reward_100\", mean_reward, frame_idx)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"avg_reward\": f\"{mean_reward:.2f}\",\n",
    "                \"loss\": f\"{mean_loss:.5f}\",\n",
    "                \"epsilon\": f\"{agent.epsilon:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # 保存模型\n",
    "        if frame_idx % SAVE_INTERVAL == 0:\n",
    "            save_path = os.path.join(SAVE_PATH, f\"model_{frame_idx}.pt\")\n",
    "            agent.save_model(save_path)\n",
    "            print(f\"\\nFrame {frame_idx}: Model saved to {save_path}\")\n",
    "        \n",
    "        # 评估模型\n",
    "        if frame_idx % EVAL_INTERVAL == 0:\n",
    "            print(f\"\\nFrame {frame_idx}: Evaluating...\")\n",
    "            eval_reward, eval_std, _ = evaluate(\n",
    "                agent,\n",
    "                ENV_NAME,\n",
    "                num_episodes=EVAL_EPISODES,\n",
    "                video_prefix=f\"eval_{frame_idx}\"\n",
    "            )\n",
    "            agent.writer.add_scalar(\"eval/mean_reward\", eval_reward, frame_idx)\n",
    "            agent.writer.add_scalar(\"eval/reward_std\", eval_std, frame_idx)\n",
    "            print(f\"Evaluation results: Mean reward = {eval_reward:.2f} ± {eval_std:.2f}\")\n",
    "        \n",
    "        # 更新代理的步数计数器\n",
    "        agent.steps_done += 1\n",
    "    \n",
    "    # 训练结束，保存最终模型\n",
    "    final_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "    agent.save_model(final_path)\n",
    "    print(f\"\\nFinal model saved to: {final_path}\")\n",
    "\n",
    "\n",
    "def load_demonstrations(agent, filepath):\n",
    "    \"\"\"加载示范轨迹文件并注入 agent 的 replay buffer\"\"\"\n",
    "    import pickle\n",
    "\n",
    "    # 和训练中使用的 ALLOWED_ACTIONS 保持一致\n",
    "    ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]\n",
    "    action_to_index = {a: i for i, a in enumerate(ALLOWED_ACTIONS)}\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        all_trajectories = pickle.load(f)\n",
    "\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "    for traj in all_trajectories:\n",
    "        for s, a, r, ns, d in traj:\n",
    "            if a not in action_to_index:\n",
    "                print(f\"Skipping illegal action: {a}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            action_index = action_to_index[a]  # 映射成 0~7\n",
    "\n",
    "            agent.memory.push(\n",
    "                s.to(device),\n",
    "                action_index,\n",
    "                r.to(device),\n",
    "                ns.to(device),\n",
    "                d.to(device)\n",
    "            )\n",
    "            count += 1\n",
    "\n",
    "    print(f\"\\nDemonstrations imported, {count} transitions added to replay buffer. Skipped {skipped} illegal actions.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 主训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation shape: (4, 84, 84)\n",
      "Action space size: 8\n",
      "\n",
      "Demonstrations imported, 2448 transitions added to replay buffer. Skipped 0 illegal actions.\n",
      "\n",
      "\n",
      "Training started...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcf00df1f204469b81626d6e2239d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Frame 20000: Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/brendan/workspace/rl-final-project/Ataris/videos/donkey_kong_20250326_050707 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/brendan/workspace/rl-final-project/Ataris/videos/donkey_kong_20250326_050707 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: Mean reward = -57.33 ± 1.25\n",
      "\n",
      "Frame 40000: Evaluating...\n",
      "Evaluation results: Mean reward = -75.00 ± 8.83\n",
      "\n",
      "Frame 60000: Evaluating...\n",
      "Evaluation results: Mean reward = -54.33 ± 7.59\n",
      "\n",
      "Frame 80000: Evaluating...\n",
      "Evaluation results: Mean reward = -62.00 ± 3.27\n",
      "\n",
      "Frame 100000: Model saved to ./models/donkey_kong_20250326_050707/model_100000.pt\n",
      "\n",
      "Frame 100000: Evaluating...\n",
      "Evaluation results: Mean reward = -57.67 ± 11.03\n",
      "\n",
      "Frame 120000: Evaluating...\n",
      "Evaluation results: Mean reward = -53.00 ± 10.68\n",
      "\n",
      "Frame 140000: Evaluating...\n",
      "Evaluation results: Mean reward = -59.67 ± 1.70\n",
      "\n",
      "Frame 160000: Evaluating...\n",
      "Evaluation results: Mean reward = -60.33 ± 0.94\n",
      "\n",
      "Frame 180000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.67 ± 0.47\n",
      "\n",
      "Frame 200000: Model saved to ./models/donkey_kong_20250326_050707/model_200000.pt\n",
      "\n",
      "Frame 200000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.33 ± 4.92\n",
      "\n",
      "Frame 220000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.00 ± 7.26\n",
      "\n",
      "Frame 240000: Evaluating...\n",
      "Evaluation results: Mean reward = -54.00 ± 4.97\n",
      "\n",
      "Frame 260000: Evaluating...\n",
      "Evaluation results: Mean reward = -61.00 ± 11.52\n",
      "\n",
      "Frame 280000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.67 ± 5.44\n",
      "\n",
      "Frame 300000: Model saved to ./models/donkey_kong_20250326_050707/model_300000.pt\n",
      "\n",
      "Frame 300000: Evaluating...\n",
      "Evaluation results: Mean reward = -54.33 ± 6.24\n",
      "\n",
      "Frame 320000: Evaluating...\n",
      "Evaluation results: Mean reward = -58.67 ± 4.03\n",
      "\n",
      "Frame 340000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.00 ± 1.41\n",
      "\n",
      "Frame 360000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.67 ± 4.64\n",
      "\n",
      "Frame 380000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 3.77\n",
      "\n",
      "Frame 400000: Model saved to ./models/donkey_kong_20250326_050707/model_400000.pt\n",
      "\n",
      "Frame 400000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.00 ± 4.08\n",
      "\n",
      "Frame 420000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.33 ± 0.94\n",
      "\n",
      "Frame 440000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.67 ± 2.05\n",
      "\n",
      "Frame 460000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.33 ± 3.09\n",
      "\n",
      "Frame 480000: Evaluating...\n",
      "Evaluation results: Mean reward = -51.67 ± 9.46\n",
      "\n",
      "Frame 500000: Model saved to ./models/donkey_kong_20250326_050707/model_500000.pt\n",
      "\n",
      "Frame 500000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.00 ± 1.41\n",
      "\n",
      "Frame 520000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.67 ± 11.56\n",
      "\n",
      "Frame 540000: Evaluating...\n",
      "Evaluation results: Mean reward = -53.67 ± 6.94\n",
      "\n",
      "Frame 560000: Evaluating...\n",
      "Evaluation results: Mean reward = -57.00 ± 10.23\n",
      "\n",
      "Frame 580000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.67 ± 5.91\n",
      "\n",
      "Frame 600000: Model saved to ./models/donkey_kong_20250326_050707/model_600000.pt\n",
      "\n",
      "Frame 600000: Evaluating...\n",
      "Evaluation results: Mean reward = -63.00 ± 8.52\n",
      "\n",
      "Frame 620000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.33 ± 3.77\n",
      "\n",
      "Frame 640000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.33 ± 8.81\n",
      "\n",
      "Frame 660000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.33 ± 4.78\n",
      "\n",
      "Frame 680000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 1.70\n",
      "\n",
      "Frame 700000: Model saved to ./models/donkey_kong_20250326_050707/model_700000.pt\n",
      "\n",
      "Frame 700000: Evaluating...\n",
      "Evaluation results: Mean reward = -56.67 ± 9.29\n",
      "\n",
      "Frame 720000: Evaluating...\n",
      "Evaluation results: Mean reward = -55.67 ± 6.18\n",
      "\n",
      "Frame 740000: Evaluating...\n",
      "Evaluation results: Mean reward = -58.00 ± 1.41\n",
      "\n",
      "Frame 760000: Evaluating...\n",
      "Evaluation results: Mean reward = -58.67 ± 10.66\n",
      "\n",
      "Frame 780000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 6.60\n",
      "\n",
      "Frame 800000: Model saved to ./models/donkey_kong_20250326_050707/model_800000.pt\n",
      "\n",
      "Frame 800000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.00 ± 7.26\n",
      "\n",
      "Frame 820000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 2.45\n",
      "\n",
      "Frame 840000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.33 ± 7.59\n",
      "\n",
      "Frame 860000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.33 ± 4.71\n",
      "\n",
      "Frame 880000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.67 ± 3.86\n",
      "\n",
      "Frame 900000: Model saved to ./models/donkey_kong_20250326_050707/model_900000.pt\n",
      "\n",
      "Frame 900000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.33 ± 0.94\n",
      "\n",
      "Frame 920000: Evaluating...\n",
      "Evaluation results: Mean reward = -53.33 ± 2.49\n",
      "\n",
      "Frame 940000: Evaluating...\n",
      "Evaluation results: Mean reward = -55.33 ± 0.47\n",
      "\n",
      "Frame 960000: Evaluating...\n",
      "Evaluation results: Mean reward = -51.67 ± 6.55\n",
      "\n",
      "Frame 980000: Evaluating...\n",
      "Evaluation results: Mean reward = -58.00 ± 5.72\n",
      "\n",
      "Frame 1000000: Model saved to ./models/donkey_kong_20250326_050707/model_1000000.pt\n",
      "\n",
      "Frame 1000000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.00 ± 3.56\n",
      "\n",
      "Frame 1020000: Evaluating...\n",
      "Evaluation results: Mean reward = -59.33 ± 5.44\n",
      "\n",
      "Frame 1040000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.00 ± 0.82\n",
      "\n",
      "Frame 1060000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.00 ± 1.41\n",
      "\n",
      "Frame 1080000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 0.47\n",
      "\n",
      "Frame 1100000: Model saved to ./models/donkey_kong_20250326_050707/model_1100000.pt\n",
      "\n",
      "Frame 1100000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.67 ± 12.26\n",
      "\n",
      "Frame 1120000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.00 ± 2.83\n",
      "\n",
      "Frame 1140000: Evaluating...\n",
      "Evaluation results: Mean reward = -57.00 ± 3.74\n",
      "\n",
      "Frame 1160000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 2.16\n",
      "\n",
      "Frame 1180000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.00 ± 0.82\n",
      "\n",
      "Frame 1200000: Model saved to ./models/donkey_kong_20250326_050707/model_1200000.pt\n",
      "\n",
      "Frame 1200000: Evaluating...\n",
      "Evaluation results: Mean reward = -64.00 ± 11.31\n",
      "\n",
      "Frame 1220000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.00 ± 4.24\n",
      "\n",
      "Frame 1240000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.00 ± 9.90\n",
      "\n",
      "Frame 1260000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.67 ± 4.92\n",
      "\n",
      "Frame 1280000: Evaluating...\n",
      "Evaluation results: Mean reward = -53.00 ± 8.83\n",
      "\n",
      "Frame 1300000: Model saved to ./models/donkey_kong_20250326_050707/model_1300000.pt\n",
      "\n",
      "Frame 1300000: Evaluating...\n",
      "Evaluation results: Mean reward = -57.67 ± 15.63\n",
      "\n",
      "Frame 1320000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.33 ± 3.09\n",
      "\n",
      "Frame 1340000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.33 ± 6.85\n",
      "\n",
      "Frame 1360000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.00 ± 7.48\n",
      "\n",
      "Frame 1380000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 0.94\n",
      "\n",
      "Frame 1400000: Model saved to ./models/donkey_kong_20250326_050707/model_1400000.pt\n",
      "\n",
      "Frame 1400000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 7.79\n",
      "\n",
      "Frame 1420000: Evaluating...\n",
      "Evaluation results: Mean reward = -55.67 ± 14.64\n",
      "\n",
      "Frame 1440000: Evaluating...\n",
      "Evaluation results: Mean reward = -61.00 ± 14.76\n",
      "\n",
      "Frame 1460000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.67 ± 6.13\n",
      "\n",
      "Frame 1480000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.33 ± 3.86\n",
      "\n",
      "Frame 1500000: Model saved to ./models/donkey_kong_20250326_050707/model_1500000.pt\n",
      "\n",
      "Frame 1500000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.33 ± 1.70\n",
      "\n",
      "Frame 1520000: Evaluating...\n",
      "Evaluation results: Mean reward = -59.00 ± 12.75\n",
      "\n",
      "Frame 1540000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.00 ± 3.27\n",
      "\n",
      "Frame 1560000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.33 ± 4.19\n",
      "\n",
      "Frame 1580000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.33 ± 1.25\n",
      "\n",
      "Frame 1600000: Model saved to ./models/donkey_kong_20250326_050707/model_1600000.pt\n",
      "\n",
      "Frame 1600000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 2.16\n",
      "\n",
      "Frame 1620000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.67 ± 0.94\n",
      "\n",
      "Frame 1640000: Evaluating...\n",
      "Evaluation results: Mean reward = -59.00 ± 6.53\n",
      "\n",
      "Frame 1660000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.67 ± 1.89\n",
      "\n",
      "Frame 1680000: Evaluating...\n",
      "Evaluation results: Mean reward = -53.67 ± 5.25\n",
      "\n",
      "Frame 1700000: Model saved to ./models/donkey_kong_20250326_050707/model_1700000.pt\n",
      "\n",
      "Frame 1700000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.00 ± 19.87\n",
      "\n",
      "Frame 1720000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.00 ± 12.57\n",
      "\n",
      "Frame 1740000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 8.96\n",
      "\n",
      "Frame 1760000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.67 ± 1.70\n",
      "\n",
      "Frame 1780000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.67 ± 0.47\n",
      "\n",
      "Frame 1800000: Model saved to ./models/donkey_kong_20250326_050707/model_1800000.pt\n",
      "\n",
      "Frame 1800000: Evaluating...\n",
      "Evaluation results: Mean reward = -54.67 ± 5.91\n",
      "\n",
      "Frame 1820000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 4.64\n",
      "\n",
      "Frame 1840000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.00 ± 4.97\n",
      "\n",
      "Frame 1860000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.00 ± 0.82\n",
      "\n",
      "Frame 1880000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.00 ± 0.82\n",
      "\n",
      "Frame 1900000: Model saved to ./models/donkey_kong_20250326_050707/model_1900000.pt\n",
      "\n",
      "Frame 1900000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 2.16\n",
      "\n",
      "Frame 1920000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.67 ± 0.94\n",
      "\n",
      "Frame 1940000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.33 ± 0.94\n",
      "\n",
      "Frame 1960000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.00 ± 4.97\n",
      "\n",
      "Frame 1980000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.00 ± 5.72\n",
      "\n",
      "Frame 2000000: Model saved to ./models/donkey_kong_20250326_050707/model_2000000.pt\n",
      "\n",
      "Frame 2000000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.87\n",
      "\n",
      "Frame 2020000: Evaluating...\n",
      "Evaluation results: Mean reward = -53.00 ± 4.97\n",
      "\n",
      "Frame 2040000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.33 ± 6.55\n",
      "\n",
      "Frame 2060000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.67 ± 3.30\n",
      "\n",
      "Frame 2080000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.67 ± 8.73\n",
      "\n",
      "Frame 2100000: Model saved to ./models/donkey_kong_20250326_050707/model_2100000.pt\n",
      "\n",
      "Frame 2100000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.33 ± 4.78\n",
      "\n",
      "Frame 2120000: Evaluating...\n",
      "Evaluation results: Mean reward = -72.00 ± 35.33\n",
      "\n",
      "Frame 2140000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.33 ± 11.61\n",
      "\n",
      "Frame 2160000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.67 ± 6.18\n",
      "\n",
      "Frame 2180000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.33 ± 6.02\n",
      "\n",
      "Frame 2200000: Model saved to ./models/donkey_kong_20250326_050707/model_2200000.pt\n",
      "\n",
      "Frame 2200000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.00 ± 2.16\n",
      "\n",
      "Frame 2220000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 2.16\n",
      "\n",
      "Frame 2240000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.33 ± 0.47\n",
      "\n",
      "Frame 2260000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.67 ± 6.13\n",
      "\n",
      "Frame 2280000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 0.82\n",
      "\n",
      "Frame 2300000: Model saved to ./models/donkey_kong_20250326_050707/model_2300000.pt\n",
      "\n",
      "Frame 2300000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.00 ± 11.43\n",
      "\n",
      "Frame 2320000: Evaluating...\n",
      "Evaluation results: Mean reward = -53.67 ± 6.60\n",
      "\n",
      "Frame 2340000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.00 ± 0.00\n",
      "\n",
      "Frame 2360000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.67 ± 1.25\n",
      "\n",
      "Frame 2380000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.67 ± 6.13\n",
      "\n",
      "Frame 2400000: Model saved to ./models/donkey_kong_20250326_050707/model_2400000.pt\n",
      "\n",
      "Frame 2400000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.33 ± 6.65\n",
      "\n",
      "Frame 2420000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.33 ± 8.01\n",
      "\n",
      "Frame 2440000: Evaluating...\n",
      "Evaluation results: Mean reward = -52.33 ± 7.72\n",
      "\n",
      "Frame 2460000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.33 ± 3.77\n",
      "\n",
      "Frame 2480000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 8.73\n",
      "\n",
      "Frame 2500000: Model saved to ./models/donkey_kong_20250326_050707/model_2500000.pt\n",
      "\n",
      "Frame 2500000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 2.05\n",
      "\n",
      "Frame 2520000: Evaluating...\n",
      "Evaluation results: Mean reward = -50.67 ± 6.24\n",
      "\n",
      "Frame 2540000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.67 ± 5.44\n",
      "\n",
      "Frame 2560000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 0.94\n",
      "\n",
      "Frame 2580000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.67 ± 7.59\n",
      "\n",
      "Frame 2600000: Model saved to ./models/donkey_kong_20250326_050707/model_2600000.pt\n",
      "\n",
      "Frame 2600000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 0.82\n",
      "\n",
      "Frame 2620000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.33 ± 2.87\n",
      "\n",
      "Frame 2640000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 8.60\n",
      "\n",
      "Frame 2660000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.00 ± 8.16\n",
      "\n",
      "Frame 2680000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.33 ± 2.62\n",
      "\n",
      "Frame 2700000: Model saved to ./models/donkey_kong_20250326_050707/model_2700000.pt\n",
      "\n",
      "Frame 2700000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.00 ± 5.66\n",
      "\n",
      "Frame 2720000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 4.50\n",
      "\n",
      "Frame 2740000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 2.94\n",
      "\n",
      "Frame 2760000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.00 ± 5.89\n",
      "\n",
      "Frame 2780000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.67 ± 6.55\n",
      "\n",
      "Frame 2800000: Model saved to ./models/donkey_kong_20250326_050707/model_2800000.pt\n",
      "\n",
      "Frame 2800000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.67 ± 4.71\n",
      "\n",
      "Frame 2820000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.67 ± 5.91\n",
      "\n",
      "Frame 2840000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.67 ± 5.73\n",
      "\n",
      "Frame 2860000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 6.60\n",
      "\n",
      "Frame 2880000: Evaluating...\n",
      "Evaluation results: Mean reward = -57.67 ± 4.99\n",
      "\n",
      "Frame 2900000: Model saved to ./models/donkey_kong_20250326_050707/model_2900000.pt\n",
      "\n",
      "Frame 2900000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.00 ± 10.98\n",
      "\n",
      "Frame 2920000: Evaluating...\n",
      "Evaluation results: Mean reward = -49.33 ± 11.47\n",
      "\n",
      "Frame 2940000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.67 ± 10.21\n",
      "\n",
      "Frame 2960000: Evaluating...\n",
      "Evaluation results: Mean reward = -56.00 ± 10.61\n",
      "\n",
      "Frame 2980000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.67 ± 5.73\n",
      "\n",
      "Frame 3000000: Model saved to ./models/donkey_kong_20250326_050707/model_3000000.pt\n",
      "\n",
      "Frame 3000000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 5.66\n",
      "\n",
      "Frame 3020000: Evaluating...\n",
      "Evaluation results: Mean reward = -31.67 ± 3.40\n",
      "\n",
      "Frame 3040000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 2.36\n",
      "\n",
      "Frame 3060000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.33 ± 6.94\n",
      "\n",
      "Frame 3080000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 4.03\n",
      "\n",
      "Frame 3100000: Model saved to ./models/donkey_kong_20250326_050707/model_3100000.pt\n",
      "\n",
      "Frame 3100000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.33 ± 7.32\n",
      "\n",
      "Frame 3120000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 3.56\n",
      "\n",
      "Frame 3140000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 5.66\n",
      "\n",
      "Frame 3160000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.67 ± 5.91\n",
      "\n",
      "Frame 3180000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 6.48\n",
      "\n",
      "Frame 3200000: Model saved to ./models/donkey_kong_20250326_050707/model_3200000.pt\n",
      "\n",
      "Frame 3200000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 0.47\n",
      "\n",
      "Frame 3220000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.00 ± 2.45\n",
      "\n",
      "Frame 3240000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 2.83\n",
      "\n",
      "Frame 3260000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 3.77\n",
      "\n",
      "Frame 3280000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 5.31\n",
      "\n",
      "Frame 3300000: Model saved to ./models/donkey_kong_20250326_050707/model_3300000.pt\n",
      "\n",
      "Frame 3300000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.00 ± 4.32\n",
      "\n",
      "Frame 3320000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 3.09\n",
      "\n",
      "Frame 3340000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 0.47\n",
      "\n",
      "Frame 3360000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 3.68\n",
      "\n",
      "Frame 3380000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 0.47\n",
      "\n",
      "Frame 3400000: Model saved to ./models/donkey_kong_20250326_050707/model_3400000.pt\n",
      "\n",
      "Frame 3400000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.00 ± 10.61\n",
      "\n",
      "Frame 3420000: Evaluating...\n",
      "Evaluation results: Mean reward = -47.33 ± 6.13\n",
      "\n",
      "Frame 3440000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.33 ± 6.13\n",
      "\n",
      "Frame 3460000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.33 ± 6.13\n",
      "\n",
      "Frame 3480000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 3.74\n",
      "\n",
      "Frame 3500000: Model saved to ./models/donkey_kong_20250326_050707/model_3500000.pt\n",
      "\n",
      "Frame 3500000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 1.63\n",
      "\n",
      "Frame 3520000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 0.00\n",
      "\n",
      "Frame 3540000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 7.32\n",
      "\n",
      "Frame 3560000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 0.94\n",
      "\n",
      "Frame 3580000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 2.87\n",
      "\n",
      "Frame 3600000: Model saved to ./models/donkey_kong_20250326_050707/model_3600000.pt\n",
      "\n",
      "Frame 3600000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 3.09\n",
      "\n",
      "Frame 3620000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 7.79\n",
      "\n",
      "Frame 3640000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.00 ± 6.38\n",
      "\n",
      "Frame 3660000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 3.30\n",
      "\n",
      "Frame 3680000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.00 ± 5.66\n",
      "\n",
      "Frame 3700000: Model saved to ./models/donkey_kong_20250326_050707/model_3700000.pt\n",
      "\n",
      "Frame 3700000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.00 ± 3.74\n",
      "\n",
      "Frame 3720000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 0.94\n",
      "\n",
      "Frame 3740000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 0.47\n",
      "\n",
      "Frame 3760000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.67 ± 5.91\n",
      "\n",
      "Frame 3780000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 2.16\n",
      "\n",
      "Frame 3800000: Model saved to ./models/donkey_kong_20250326_050707/model_3800000.pt\n",
      "\n",
      "Frame 3800000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.67 ± 1.25\n",
      "\n",
      "Frame 3820000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 2.49\n",
      "\n",
      "Frame 3840000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 7.48\n",
      "\n",
      "Frame 3860000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.67 ± 3.30\n",
      "\n",
      "Frame 3880000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.00 ± 10.71\n",
      "\n",
      "Frame 3900000: Model saved to ./models/donkey_kong_20250326_050707/model_3900000.pt\n",
      "\n",
      "Frame 3900000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 0.00\n",
      "\n",
      "Frame 3920000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 0.82\n",
      "\n",
      "Frame 3940000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 10.37\n",
      "\n",
      "Frame 3960000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 2.45\n",
      "\n",
      "Frame 3980000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.33 ± 2.62\n",
      "\n",
      "Frame 4000000: Model saved to ./models/donkey_kong_20250326_050707/model_4000000.pt\n",
      "\n",
      "Frame 4000000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.67 ± 2.87\n",
      "\n",
      "Frame 4020000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 3.30\n",
      "\n",
      "Frame 4040000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 5.31\n",
      "\n",
      "Frame 4060000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 3.56\n",
      "\n",
      "Frame 4080000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 2.83\n",
      "\n",
      "Frame 4100000: Model saved to ./models/donkey_kong_20250326_050707/model_4100000.pt\n",
      "\n",
      "Frame 4100000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 4.50\n",
      "\n",
      "Frame 4120000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.33 ± 3.09\n",
      "\n",
      "Frame 4140000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 3.86\n",
      "\n",
      "Frame 4160000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.00 ± 4.97\n",
      "\n",
      "Frame 4180000: Evaluating...\n",
      "Evaluation results: Mean reward = -46.00 ± 11.05\n",
      "\n",
      "Frame 4200000: Model saved to ./models/donkey_kong_20250326_050707/model_4200000.pt\n",
      "\n",
      "Frame 4200000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.00 ± 5.66\n",
      "\n",
      "Frame 4220000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.05\n",
      "\n",
      "Frame 4240000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 3.09\n",
      "\n",
      "Frame 4260000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 3.09\n",
      "\n",
      "Frame 4280000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 4.92\n",
      "\n",
      "Frame 4300000: Model saved to ./models/donkey_kong_20250326_050707/model_4300000.pt\n",
      "\n",
      "Frame 4300000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.36\n",
      "\n",
      "Frame 4320000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.62\n",
      "\n",
      "Frame 4340000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 3.30\n",
      "\n",
      "Frame 4360000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 1.89\n",
      "\n",
      "Frame 4380000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 1.25\n",
      "\n",
      "Frame 4400000: Model saved to ./models/donkey_kong_20250326_050707/model_4400000.pt\n",
      "\n",
      "Frame 4400000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 3.27\n",
      "\n",
      "Frame 4420000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.67 ± 3.30\n",
      "\n",
      "Frame 4440000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 6.60\n",
      "\n",
      "Frame 4460000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 8.58\n",
      "\n",
      "Frame 4480000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 6.38\n",
      "\n",
      "Frame 4500000: Model saved to ./models/donkey_kong_20250326_050707/model_4500000.pt\n",
      "\n",
      "Frame 4500000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.33 ± 5.44\n",
      "\n",
      "Frame 4520000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 7.04\n",
      "\n",
      "Frame 4540000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 1.70\n",
      "\n",
      "Frame 4560000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.33 ± 5.31\n",
      "\n",
      "Frame 4580000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 2.05\n",
      "\n",
      "Frame 4600000: Model saved to ./models/donkey_kong_20250326_050707/model_4600000.pt\n",
      "\n",
      "Frame 4600000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 0.82\n",
      "\n",
      "Frame 4620000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.33 ± 5.73\n",
      "\n",
      "Frame 4640000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 4.50\n",
      "\n",
      "Frame 4660000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 0.47\n",
      "\n",
      "Frame 4680000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.16\n",
      "\n",
      "Frame 4700000: Model saved to ./models/donkey_kong_20250326_050707/model_4700000.pt\n",
      "\n",
      "Frame 4700000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.62\n",
      "\n",
      "Frame 4720000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 4.50\n",
      "\n",
      "Frame 4740000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 0.47\n",
      "\n",
      "Frame 4760000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 0.82\n",
      "\n",
      "Frame 4780000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 3.74\n",
      "\n",
      "Frame 4800000: Model saved to ./models/donkey_kong_20250326_050707/model_4800000.pt\n",
      "\n",
      "Frame 4800000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 3.56\n",
      "\n",
      "Frame 4820000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.33 ± 5.44\n",
      "\n",
      "Frame 4840000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.00 ± 5.72\n",
      "\n",
      "Frame 4860000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 1.70\n",
      "\n",
      "Frame 4880000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 6.55\n",
      "\n",
      "Frame 4900000: Model saved to ./models/donkey_kong_20250326_050707/model_4900000.pt\n",
      "\n",
      "Frame 4900000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 3.40\n",
      "\n",
      "Frame 4920000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 0.47\n",
      "\n",
      "Frame 4940000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 4.50\n",
      "\n",
      "Frame 4960000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.00 ± 2.16\n",
      "\n",
      "Frame 4980000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.33 ± 7.32\n",
      "\n",
      "Frame 5000000: Model saved to ./models/donkey_kong_20250326_050707/model_5000000.pt\n",
      "\n",
      "Frame 5000000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 6.38\n",
      "\n",
      "Frame 5020000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 0.47\n",
      "\n",
      "Frame 5040000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.62\n",
      "\n",
      "Frame 5060000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 3.86\n",
      "\n",
      "Frame 5080000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 2.45\n",
      "\n",
      "Frame 5100000: Model saved to ./models/donkey_kong_20250326_050707/model_5100000.pt\n",
      "\n",
      "Frame 5100000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 2.94\n",
      "\n",
      "Frame 5120000: Evaluating...\n",
      "Evaluation results: Mean reward = -45.00 ± 6.38\n",
      "\n",
      "Frame 5140000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 2.83\n",
      "\n",
      "Frame 5160000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 0.82\n",
      "\n",
      "Frame 5180000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.00 ± 5.89\n",
      "\n",
      "Frame 5200000: Model saved to ./models/donkey_kong_20250326_050707/model_5200000.pt\n",
      "\n",
      "Frame 5200000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 5.44\n",
      "\n",
      "Frame 5220000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.00 ± 3.56\n",
      "\n",
      "Frame 5240000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 1.63\n",
      "\n",
      "Frame 5260000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 4.64\n",
      "\n",
      "Frame 5280000: Evaluating...\n",
      "Evaluation results: Mean reward = -44.00 ± 10.03\n",
      "\n",
      "Frame 5300000: Model saved to ./models/donkey_kong_20250326_050707/model_5300000.pt\n",
      "\n",
      "Frame 5300000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 5320000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 0.47\n",
      "\n",
      "Frame 5340000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 5.73\n",
      "\n",
      "Frame 5360000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 1.63\n",
      "\n",
      "Frame 5380000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 5.31\n",
      "\n",
      "Frame 5400000: Model saved to ./models/donkey_kong_20250326_050707/model_5400000.pt\n",
      "\n",
      "Frame 5400000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.05\n",
      "\n",
      "Frame 5420000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.67 ± 0.94\n",
      "\n",
      "Frame 5440000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 1.25\n",
      "\n",
      "Frame 5460000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 5.91\n",
      "\n",
      "Frame 5480000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 0.00\n",
      "\n",
      "Frame 5500000: Model saved to ./models/donkey_kong_20250326_050707/model_5500000.pt\n",
      "\n",
      "Frame 5500000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 1.25\n",
      "\n",
      "Frame 5520000: Evaluating...\n",
      "Evaluation results: Mean reward = -43.00 ± 7.12\n",
      "\n",
      "Frame 5540000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 1.63\n",
      "\n",
      "Frame 5560000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 8.50\n",
      "\n",
      "Frame 5580000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 7.48\n",
      "\n",
      "Frame 5600000: Model saved to ./models/donkey_kong_20250326_050707/model_5600000.pt\n",
      "\n",
      "Frame 5600000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 0.47\n",
      "\n",
      "Frame 5620000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.33 ± 5.91\n",
      "\n",
      "Frame 5640000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 5.91\n",
      "\n",
      "Frame 5660000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.62\n",
      "\n",
      "Frame 5680000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 3.86\n",
      "\n",
      "Frame 5700000: Model saved to ./models/donkey_kong_20250326_050707/model_5700000.pt\n",
      "\n",
      "Frame 5700000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 0.82\n",
      "\n",
      "Frame 5720000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 5740000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 2.16\n",
      "\n",
      "Frame 5760000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 8.06\n",
      "\n",
      "Frame 5780000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.36\n",
      "\n",
      "Frame 5800000: Model saved to ./models/donkey_kong_20250326_050707/model_5800000.pt\n",
      "\n",
      "Frame 5800000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 0.47\n",
      "\n",
      "Frame 5820000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.00 ± 4.90\n",
      "\n",
      "Frame 5840000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 2.87\n",
      "\n",
      "Frame 5860000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 5880000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 5.79\n",
      "\n",
      "Frame 5900000: Model saved to ./models/donkey_kong_20250326_050707/model_5900000.pt\n",
      "\n",
      "Frame 5900000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 0.94\n",
      "\n",
      "Frame 5920000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 2.16\n",
      "\n",
      "Frame 5940000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 0.94\n",
      "\n",
      "Frame 5960000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 1.25\n",
      "\n",
      "Frame 5980000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.62\n",
      "\n",
      "Frame 6000000: Model saved to ./models/donkey_kong_20250326_050707/model_6000000.pt\n",
      "\n",
      "Frame 6000000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.36\n",
      "\n",
      "Frame 6020000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 3.56\n",
      "\n",
      "Frame 6040000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 4.55\n",
      "\n",
      "Frame 6060000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 0.47\n",
      "\n",
      "Frame 6080000: Evaluating...\n",
      "Evaluation results: Mean reward = -30.67 ± 6.60\n",
      "\n",
      "Frame 6100000: Model saved to ./models/donkey_kong_20250326_050707/model_6100000.pt\n",
      "\n",
      "Frame 6100000: Evaluating...\n",
      "Evaluation results: Mean reward = -48.33 ± 1.25\n",
      "\n",
      "Frame 6120000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 6.38\n",
      "\n",
      "Frame 6140000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 6.38\n",
      "\n",
      "Frame 6160000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 1.63\n",
      "\n",
      "Frame 6180000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 6.24\n",
      "\n",
      "Frame 6200000: Model saved to ./models/donkey_kong_20250326_050707/model_6200000.pt\n",
      "\n",
      "Frame 6200000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 0.47\n",
      "\n",
      "Frame 6220000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 0.47\n",
      "\n",
      "Frame 6240000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.05\n",
      "\n",
      "Frame 6260000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 1.89\n",
      "\n",
      "Frame 6280000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.00 ± 3.56\n",
      "\n",
      "Frame 6300000: Model saved to ./models/donkey_kong_20250326_050707/model_6300000.pt\n",
      "\n",
      "Frame 6300000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 1.25\n",
      "\n",
      "Frame 6320000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 1.89\n",
      "\n",
      "Frame 6340000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.67 ± 4.64\n",
      "\n",
      "Frame 6360000: Evaluating...\n",
      "Evaluation results: Mean reward = -42.33 ± 4.03\n",
      "\n",
      "Frame 6380000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 1.63\n",
      "\n",
      "Frame 6400000: Model saved to ./models/donkey_kong_20250326_050707/model_6400000.pt\n",
      "\n",
      "Frame 6400000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 1.25\n",
      "\n",
      "Frame 6420000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 2.05\n",
      "\n",
      "Frame 6440000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 1.70\n",
      "\n",
      "Frame 6460000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 1.25\n",
      "\n",
      "Frame 6480000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 2.36\n",
      "\n",
      "Frame 6500000: Model saved to ./models/donkey_kong_20250326_050707/model_6500000.pt\n",
      "\n",
      "Frame 6500000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 5.35\n",
      "\n",
      "Frame 6520000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 7.48\n",
      "\n",
      "Frame 6540000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.94\n",
      "\n",
      "Frame 6560000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 1.70\n",
      "\n",
      "Frame 6580000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 4.92\n",
      "\n",
      "Frame 6600000: Model saved to ./models/donkey_kong_20250326_050707/model_6600000.pt\n",
      "\n",
      "Frame 6600000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.62\n",
      "\n",
      "Frame 6620000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 0.00\n",
      "\n",
      "Frame 6640000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 1.89\n",
      "\n",
      "Frame 6660000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 1.41\n",
      "\n",
      "Frame 6680000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.67 ± 1.89\n",
      "\n",
      "Frame 6700000: Model saved to ./models/donkey_kong_20250326_050707/model_6700000.pt\n",
      "\n",
      "Frame 6700000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 5.91\n",
      "\n",
      "Frame 6720000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 2.36\n",
      "\n",
      "Frame 6740000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 2.16\n",
      "\n",
      "Frame 6760000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 6.60\n",
      "\n",
      "Frame 6780000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 4.55\n",
      "\n",
      "Frame 6800000: Model saved to ./models/donkey_kong_20250326_050707/model_6800000.pt\n",
      "\n",
      "Frame 6800000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 3.30\n",
      "\n",
      "Frame 6820000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 1.63\n",
      "\n",
      "Frame 6840000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 1.25\n",
      "\n",
      "Frame 6860000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 0.47\n",
      "\n",
      "Frame 6880000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 6.38\n",
      "\n",
      "Frame 6900000: Model saved to ./models/donkey_kong_20250326_050707/model_6900000.pt\n",
      "\n",
      "Frame 6900000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 6.65\n",
      "\n",
      "Frame 6920000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 1.63\n",
      "\n",
      "Frame 6940000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.00 ± 4.24\n",
      "\n",
      "Frame 6960000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 1.89\n",
      "\n",
      "Frame 6980000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 4.32\n",
      "\n",
      "Frame 7000000: Model saved to ./models/donkey_kong_20250326_050707/model_7000000.pt\n",
      "\n",
      "Frame 7000000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.16\n",
      "\n",
      "Frame 7020000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 3.09\n",
      "\n",
      "Frame 7040000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.33 ± 4.50\n",
      "\n",
      "Frame 7060000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 5.91\n",
      "\n",
      "Frame 7080000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 0.82\n",
      "\n",
      "Frame 7100000: Model saved to ./models/donkey_kong_20250326_050707/model_7100000.pt\n",
      "\n",
      "Frame 7100000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.00 ± 3.56\n",
      "\n",
      "Frame 7120000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 1.89\n",
      "\n",
      "Frame 7140000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 0.94\n",
      "\n",
      "Frame 7160000: Evaluating...\n",
      "Evaluation results: Mean reward = -30.67 ± 4.03\n",
      "\n",
      "Frame 7180000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 3.09\n",
      "\n",
      "Frame 7200000: Model saved to ./models/donkey_kong_20250326_050707/model_7200000.pt\n",
      "\n",
      "Frame 7200000: Evaluating...\n",
      "Evaluation results: Mean reward = -31.67 ± 2.62\n",
      "\n",
      "Frame 7220000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 0.82\n",
      "\n",
      "Frame 7240000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.94\n",
      "\n",
      "Frame 7260000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.33 ± 3.30\n",
      "\n",
      "Frame 7280000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 0.94\n",
      "\n",
      "Frame 7300000: Model saved to ./models/donkey_kong_20250326_050707/model_7300000.pt\n",
      "\n",
      "Frame 7300000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 5.73\n",
      "\n",
      "Frame 7320000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 2.62\n",
      "\n",
      "Frame 7340000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 7360000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.33 ± 5.31\n",
      "\n",
      "Frame 7380000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 0.94\n",
      "\n",
      "Frame 7400000: Model saved to ./models/donkey_kong_20250326_050707/model_7400000.pt\n",
      "\n",
      "Frame 7400000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 1.41\n",
      "\n",
      "Frame 7420000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 2.62\n",
      "\n",
      "Frame 7440000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 3.77\n",
      "\n",
      "Frame 7460000: Evaluating...\n",
      "Evaluation results: Mean reward = -31.00 ± 4.32\n",
      "\n",
      "Frame 7480000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.16\n",
      "\n",
      "Frame 7500000: Model saved to ./models/donkey_kong_20250326_050707/model_7500000.pt\n",
      "\n",
      "Frame 7500000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 5.72\n",
      "\n",
      "Frame 7520000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 0.94\n",
      "\n",
      "Frame 7540000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 3.30\n",
      "\n",
      "Frame 7560000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 3.30\n",
      "\n",
      "Frame 7580000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 1.63\n",
      "\n",
      "Frame 7600000: Model saved to ./models/donkey_kong_20250326_050707/model_7600000.pt\n",
      "\n",
      "Frame 7600000: Evaluating...\n",
      "Evaluation results: Mean reward = -31.33 ± 3.86\n",
      "\n",
      "Frame 7620000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.05\n",
      "\n",
      "Frame 7640000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 1.70\n",
      "\n",
      "Frame 7660000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 2.16\n",
      "\n",
      "Frame 7680000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 0.82\n",
      "\n",
      "Frame 7700000: Model saved to ./models/donkey_kong_20250326_050707/model_7700000.pt\n",
      "\n",
      "Frame 7700000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 4.97\n",
      "\n",
      "Frame 7720000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.00 ± 2.94\n",
      "\n",
      "Frame 7740000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 3.09\n",
      "\n",
      "Frame 7760000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.36\n",
      "\n",
      "Frame 7780000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 0.00\n",
      "\n",
      "Frame 7800000: Model saved to ./models/donkey_kong_20250326_050707/model_7800000.pt\n",
      "\n",
      "Frame 7800000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.49\n",
      "\n",
      "Frame 7820000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 8.38\n",
      "\n",
      "Frame 7840000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.33 ± 5.31\n",
      "\n",
      "Frame 7860000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 7.59\n",
      "\n",
      "Frame 7880000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.45\n",
      "\n",
      "Frame 7900000: Model saved to ./models/donkey_kong_20250326_050707/model_7900000.pt\n",
      "\n",
      "Frame 7900000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 3.09\n",
      "\n",
      "Frame 7920000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 2.36\n",
      "\n",
      "Frame 7940000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 0.47\n",
      "\n",
      "Frame 7960000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 1.89\n",
      "\n",
      "Frame 7980000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.33 ± 6.60\n",
      "\n",
      "Frame 8000000: Model saved to ./models/donkey_kong_20250326_050707/model_8000000.pt\n",
      "\n",
      "Frame 8000000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 1.63\n",
      "\n",
      "Frame 8020000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 0.82\n",
      "\n",
      "Frame 8040000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.49\n",
      "\n",
      "Frame 8060000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.16\n",
      "\n",
      "Frame 8080000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.33 ± 3.86\n",
      "\n",
      "Frame 8100000: Model saved to ./models/donkey_kong_20250326_050707/model_8100000.pt\n",
      "\n",
      "Frame 8100000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.33 ± 2.36\n",
      "\n",
      "Frame 8120000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 2.16\n",
      "\n",
      "Frame 8140000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 2.83\n",
      "\n",
      "Frame 8160000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.16\n",
      "\n",
      "Frame 8180000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.49\n",
      "\n",
      "Frame 8200000: Model saved to ./models/donkey_kong_20250326_050707/model_8200000.pt\n",
      "\n",
      "Frame 8200000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 0.47\n",
      "\n",
      "Frame 8220000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 0.94\n",
      "\n",
      "Frame 8240000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.33 ± 3.09\n",
      "\n",
      "Frame 8260000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 2.16\n",
      "\n",
      "Frame 8280000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 8300000: Model saved to ./models/donkey_kong_20250326_050707/model_8300000.pt\n",
      "\n",
      "Frame 8300000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.36\n",
      "\n",
      "Frame 8320000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.33 ± 3.30\n",
      "\n",
      "Frame 8340000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 1.25\n",
      "\n",
      "Frame 8360000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.33 ± 1.70\n",
      "\n",
      "Frame 8380000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 1.63\n",
      "\n",
      "Frame 8400000: Model saved to ./models/donkey_kong_20250326_050707/model_8400000.pt\n",
      "\n",
      "Frame 8400000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.67 ± 2.62\n",
      "\n",
      "Frame 8420000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 1.41\n",
      "\n",
      "Frame 8440000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.00 ± 4.55\n",
      "\n",
      "Frame 8460000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 3.27\n",
      "\n",
      "Frame 8480000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 0.94\n",
      "\n",
      "Frame 8500000: Model saved to ./models/donkey_kong_20250326_050707/model_8500000.pt\n",
      "\n",
      "Frame 8500000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.36\n",
      "\n",
      "Frame 8520000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.94\n",
      "\n",
      "Frame 8540000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 8560000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.45\n",
      "\n",
      "Frame 8580000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 1.63\n",
      "\n",
      "Frame 8600000: Model saved to ./models/donkey_kong_20250326_050707/model_8600000.pt\n",
      "\n",
      "Frame 8600000: Evaluating...\n",
      "Evaluation results: Mean reward = -41.67 ± 3.09\n",
      "\n",
      "Frame 8620000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 4.97\n",
      "\n",
      "Frame 8640000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 0.82\n",
      "\n",
      "Frame 8660000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 1.63\n",
      "\n",
      "Frame 8680000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 6.38\n",
      "\n",
      "Frame 8700000: Model saved to ./models/donkey_kong_20250326_050707/model_8700000.pt\n",
      "\n",
      "Frame 8700000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 2.36\n",
      "\n",
      "Frame 8720000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 1.41\n",
      "\n",
      "Frame 8740000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 6.48\n",
      "\n",
      "Frame 8760000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 2.36\n",
      "\n",
      "Frame 8780000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.00 ± 6.38\n",
      "\n",
      "Frame 8800000: Model saved to ./models/donkey_kong_20250326_050707/model_8800000.pt\n",
      "\n",
      "Frame 8800000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.16\n",
      "\n",
      "Frame 8820000: Evaluating...\n",
      "Evaluation results: Mean reward = -39.33 ± 5.44\n",
      "\n",
      "Frame 8840000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 2.94\n",
      "\n",
      "Frame 8860000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 1.41\n",
      "\n",
      "Frame 8880000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 2.87\n",
      "\n",
      "Frame 8900000: Model saved to ./models/donkey_kong_20250326_050707/model_8900000.pt\n",
      "\n",
      "Frame 8900000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.00 ± 0.82\n",
      "\n",
      "Frame 8920000: Evaluating...\n",
      "Evaluation results: Mean reward = -40.67 ± 4.50\n",
      "\n",
      "Frame 8940000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 6.60\n",
      "\n",
      "Frame 8960000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 2.16\n",
      "\n",
      "Frame 8980000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 6.38\n",
      "\n",
      "Frame 9000000: Model saved to ./models/donkey_kong_20250326_050707/model_9000000.pt\n",
      "\n",
      "Frame 9000000: Evaluating...\n",
      "Evaluation results: Mean reward = -31.33 ± 5.19\n",
      "\n",
      "Frame 9020000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 0.94\n",
      "\n",
      "Frame 9040000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 1.41\n",
      "\n",
      "Frame 9060000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 1.70\n",
      "\n",
      "Frame 9080000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 0.47\n",
      "\n",
      "Frame 9100000: Model saved to ./models/donkey_kong_20250326_050707/model_9100000.pt\n",
      "\n",
      "Frame 9100000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 2.83\n",
      "\n",
      "Frame 9120000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.33 ± 2.36\n",
      "\n",
      "Frame 9140000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 2.62\n",
      "\n",
      "Frame 9160000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 3.40\n",
      "\n",
      "Frame 9180000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.49\n",
      "\n",
      "Frame 9200000: Model saved to ./models/donkey_kong_20250326_050707/model_9200000.pt\n",
      "\n",
      "Frame 9200000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 9220000: Evaluating...\n",
      "Evaluation results: Mean reward = -32.67 ± 1.25\n",
      "\n",
      "Frame 9240000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 1.70\n",
      "\n",
      "Frame 9260000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 0.47\n",
      "\n",
      "Frame 9280000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 4.50\n",
      "\n",
      "Frame 9300000: Model saved to ./models/donkey_kong_20250326_050707/model_9300000.pt\n",
      "\n",
      "Frame 9300000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 1.70\n",
      "\n",
      "Frame 9320000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.05\n",
      "\n",
      "Frame 9340000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 3.27\n",
      "\n",
      "Frame 9360000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 3.09\n",
      "\n",
      "Frame 9380000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.00 ± 4.32\n",
      "\n",
      "Frame 9400000: Model saved to ./models/donkey_kong_20250326_050707/model_9400000.pt\n",
      "\n",
      "Frame 9400000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 5.56\n",
      "\n",
      "Frame 9420000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 0.94\n",
      "\n",
      "Frame 9440000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 2.62\n",
      "\n",
      "Frame 9460000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.00 ± 1.63\n",
      "\n",
      "Frame 9480000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.33 ± 2.62\n",
      "\n",
      "Frame 9500000: Model saved to ./models/donkey_kong_20250326_050707/model_9500000.pt\n",
      "\n",
      "Frame 9500000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.94\n",
      "\n",
      "Frame 9520000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 2.62\n",
      "\n",
      "Frame 9540000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.67 ± 0.47\n",
      "\n",
      "Frame 9560000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 4.08\n",
      "\n",
      "Frame 9580000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 2.94\n",
      "\n",
      "Frame 9600000: Model saved to ./models/donkey_kong_20250326_050707/model_9600000.pt\n",
      "\n",
      "Frame 9600000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.94\n",
      "\n",
      "Frame 9620000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.67 ± 1.25\n",
      "\n",
      "Frame 9640000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.33 ± 3.40\n",
      "\n",
      "Frame 9660000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 4.55\n",
      "\n",
      "Frame 9680000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 2.16\n",
      "\n",
      "Frame 9700000: Model saved to ./models/donkey_kong_20250326_050707/model_9700000.pt\n",
      "\n",
      "Frame 9700000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 1.89\n",
      "\n",
      "Frame 9720000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 2.05\n",
      "\n",
      "Frame 9740000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 3.86\n",
      "\n",
      "Frame 9760000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.33 ± 1.70\n",
      "\n",
      "Frame 9780000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.67 ± 0.47\n",
      "\n",
      "Frame 9800000: Model saved to ./models/donkey_kong_20250326_050707/model_9800000.pt\n",
      "\n",
      "Frame 9800000: Evaluating...\n",
      "Evaluation results: Mean reward = -38.33 ± 0.47\n",
      "\n",
      "Frame 9820000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.67 ± 1.70\n",
      "\n",
      "Frame 9840000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 6.60\n",
      "\n",
      "Frame 9860000: Evaluating...\n",
      "Evaluation results: Mean reward = -33.33 ± 0.94\n",
      "\n",
      "Frame 9880000: Evaluating...\n",
      "Evaluation results: Mean reward = -34.00 ± 0.00\n",
      "\n",
      "Frame 9900000: Model saved to ./models/donkey_kong_20250326_050707/model_9900000.pt\n",
      "\n",
      "Frame 9900000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.00 ± 0.82\n",
      "\n",
      "Frame 9920000: Evaluating...\n",
      "Evaluation results: Mean reward = -36.67 ± 1.70\n",
      "\n",
      "Frame 9940000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 0.82\n",
      "\n",
      "Frame 9960000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.00 ± 4.08\n",
      "\n",
      "Frame 9980000: Evaluating...\n",
      "Evaluation results: Mean reward = -37.67 ± 1.89\n",
      "\n",
      "Frame 10000000: Model saved to ./models/donkey_kong_20250326_050707/model_10000000.pt\n",
      "\n",
      "Frame 10000000: Evaluating...\n",
      "Evaluation results: Mean reward = -35.33 ± 0.94\n",
      "\n",
      "Final model saved to: ./models/donkey_kong_20250326_050707/model_final.pt\n"
     ]
    }
   ],
   "source": [
    "# 创建并行环境\n",
    "envs = make_vec_env(ENV_NAME, NUM_ENVS)\n",
    "\n",
    "# 获取环境信息\n",
    "obs_shape = (4, 84, 84)  # 堆叠的4帧，每帧84x84\n",
    "n_actions = envs.action_space.n\n",
    "\n",
    "print(f\"\\nObservation shape: {obs_shape}\")\n",
    "print(f\"Action space size: {n_actions}\")\n",
    "\n",
    "# 创建DQN代理\n",
    "agent = DQNAgent(obs_shape, n_actions)\n",
    "\n",
    "# 加载示范轨迹\n",
    "if DEMO_PATH:\n",
    "    Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    load_demonstrations(agent, DEMO_PATH)\n",
    "\n",
    "# 开始训练\n",
    "print(\"\\nTraining started...\\n\")\n",
    "\n",
    "# 启用cudnn的benchmark模式，提高训练速度\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "train(agent, envs, NUM_FRAMES)\n",
    "\n",
    "# 关闭环境\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 加载和测试训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_video(model_path, env_id, num_episodes=5):\n",
    "    # 创建代理并加载模型\n",
    "    obs_shape = (4, 84, 84)  # 堆叠的4帧，每帧84x84\n",
    "    env = make_env(env_id, 0)()\n",
    "    n_actions = env.action_space.n\n",
    "    agent = DQNAgent(obs_shape, n_actions)\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    # 测试训练好的代理\n",
    "    mean, std, rewards = evaluate(agent, env_id, num_episodes=num_episodes, video_prefix=\"final_test\")\n",
    "\n",
    "    for i, reward in enumerate(rewards):\n",
    "        print(f\"Episode {i+1}: Reward = {reward}\")\n",
    "        \n",
    "    print(f\"\\nAverage reward: {mean:.2f} ± {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -39.0\n",
      "Episode 2: Reward = -35.0\n",
      "Episode 3: Reward = -35.0\n",
      "Episode 4: Reward = -32.0\n",
      "Episode 5: Reward = -34.0\n",
      "\n",
      "Average reward: -35.00 ± 2.28\n"
     ]
    }
   ],
   "source": [
    "# 加载并测试最终模型\n",
    "model_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "play_and_record_video(model_path, ENV_NAME, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 可视化训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To visualize training results, run the following command in the terminal:\n",
      "tensorboard --logdir=./logs/donkey_kong_20250326_050707\n"
     ]
    }
   ],
   "source": [
    "# 使用TensorBoard可视化训练结果\n",
    "print(f\"To visualize training results, run the following command in the terminal:\")\n",
    "print(f\"tensorboard --logdir={LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
