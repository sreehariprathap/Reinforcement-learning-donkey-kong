{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨DQNè®­ç»ƒAtari Donkey Kong\n",
    "\n",
    "æœ¬notebookå®ç°äº†ä¸€ä¸ªDQNä»£ç†æ¥ç©Atariæ¸¸æˆDonkey Kongï¼Œå¹¶åŒ…å«ä»¥ä¸‹ç‰¹æ€§ï¼š\n",
    "- å¹¶è¡Œè®­ç»ƒå¤šä¸ªæ¸¸æˆç¯å¢ƒ\n",
    "- é¢„å¤„ç†æ¸¸æˆå¸§ä»¥æé«˜è®­ç»ƒæ•ˆç‡\n",
    "- ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾æé«˜è®­ç»ƒè´¨é‡\n",
    "- è®­ç»ƒæ—¥å¿—è®°å½•\n",
    "- å®šæœŸä¿å­˜æ¨¡å‹\n",
    "- å®šæœŸè¯„ä¼°å¹¶å½•åˆ¶æ¸¸æˆè§†é¢‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å®‰è£…å¿…è¦çš„ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "# %pip install stable-baselines3[extra] gymnasium[atari] numpy matplotlib opencv-python tensorboard autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, FrameStackObservation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "import ale_py\n",
    "from gymnasium import spaces\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cuda\n"
     ]
    }
   ],
   "source": [
    "# ç¯å¢ƒå‚æ•°\n",
    "ENV_NAME = \"ALE/DonkeyKong-v5\"\n",
    "NUM_ENVS = 4  # å¹¶è¡Œç¯å¢ƒæ•°é‡\n",
    "FRAME_SKIP = 4  # è·³å¸§æ•°ï¼Œæ¯éš”4å¸§è¿›è¡Œä¸€æ¬¡å†³ç­–\n",
    "ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]  # æœ‰æ•ˆåŠ¨ä½œ\n",
    "\n",
    "# æ¨¡å‹å‚æ•°\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99  # æŠ˜æ‰£å› å­\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 100000  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°\n",
    "TARGET_UPDATE = 10000  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "NUM_FRAMES = 15_000_000  # æ€»è®­ç»ƒå¸§æ•°\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 10_000_000\n",
    "DEMO_PATH = \"./demo/dk_demo_20250325_182120.pkl\"\n",
    "\n",
    "# ä¿å­˜å’Œè¯„ä¼°å‚æ•°\n",
    "SAVE_INTERVAL = 100_000  # ä¿å­˜æ¨¡å‹çš„é—´éš”ï¼ˆå¸§æ•°ï¼‰\n",
    "EVAL_INTERVAL = 50_000   # è¯„ä¼°æ¨¡å‹çš„é—´éš”ï¼ˆå¸§æ•°ï¼‰\n",
    "EVAL_EPISODES = 3       # æ¯æ¬¡è¯„ä¼°çš„æ¸¸æˆå±€æ•°\n",
    "\n",
    "# åˆ›å»ºä¿å­˜æ¨¡å‹å’Œæ—¥å¿—çš„ç›®å½•\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_PATH = f\"./models/donkey_kong_{timestamp}\"\n",
    "LOG_PATH = f\"./logs/donkey_kong_{timestamp}\"\n",
    "VIDEO_PATH = f\"./videos/donkey_kong_{timestamp}\"\n",
    "\n",
    "for path in [SAVE_PATH, LOG_PATH, VIDEO_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç¯å¢ƒé¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é™åˆ¶åŠ¨ä½œç©ºé—´,å‡å°‘ agent çš„æ— ç”¨åŠ¨ä½œ\n",
    "class ActionRestrictWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, allowed_actions):\n",
    "        super().__init__(env)\n",
    "        self.allowed_actions = allowed_actions\n",
    "        self.action_space = spaces.Discrete(len(self.allowed_actions))\n",
    "\n",
    "    def action(self, act):\n",
    "        # æŠŠ agent è¾“å‡ºçš„åŠ¨ä½œç´¢å¼•æ˜ å°„æˆåŸåŠ¨ä½œç¼–å·\n",
    "        return self.allowed_actions[act]\n",
    "\n",
    "    def reverse_action(self, act):\n",
    "        return self.allowed_actions.index(act)\n",
    "\n",
    "# å¼ºåˆ¶é¦–ä¸ªåŠ¨ä½œä¸ºFIREçš„åŒ…è£…å™¨\n",
    "class ForceFirstFireWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.first_action_done = False\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.first_action_done = False\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # å¦‚æœæ˜¯é¦–ä¸ªåŠ¨ä½œä¸”ä¸æ˜¯FIRE(1)ï¼Œåˆ™å¼ºåˆ¶æ›¿æ¢ä¸ºFIRE\n",
    "        if not self.first_action_done:\n",
    "            self.first_action_done = True\n",
    "            # ä½¿ç”¨FIREåŠ¨ä½œ(ç´¢å¼•1)ä»£æ›¿ä¼ å…¥çš„åŠ¨ä½œ\n",
    "            return self.env.step(1)  # 1å¯¹åº”FIREåŠ¨ä½œ\n",
    "        return self.env.step(action)\n",
    "\n",
    "# æ ¹æ®é¢œè‰²æ£€æµ‹äººç‰©ä½ç½®çš„å‡½æ•°\n",
    "def get_agent_position(frame): \n",
    "    \"\"\" æ ¹æ®é¢œè‰²æ£€æµ‹äººç‰©ä½ç½®ï¼Œè¿”å› (x, y) åæ ‡ã€‚æœªæ£€æµ‹åˆ°åˆ™è¿”å› Noneã€‚ \"\"\"\n",
    "    # ç¡®ä¿frameæ˜¯numpyæ•°ç»„ä¸”æ ¼å¼æ­£ç¡®\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # ç›®æ ‡é¢œè‰²ï¼ˆBGR æ ¼å¼ï¼‰\n",
    "    target_bgr = np.array([194, 64, 82], dtype=np.uint8)\n",
    "\n",
    "    # å®¹å·®èŒƒå›´ï¼ˆå¯è°ƒï¼Œ20~40 ä¸€èˆ¬æ¯”è¾ƒåˆé€‚ï¼‰\n",
    "    tolerance = 30\n",
    "    lower = np.array([max(0, c - tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "    upper = np.array([min(255, c + tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "\n",
    "    # ç”Ÿæˆæ©ç \n",
    "    mask = cv2.inRange(frame, lower, upper)\n",
    "    \n",
    "    # æŸ¥æ‰¾è½®å»“\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # æ‰¾é¢ç§¯æœ€å¤§è½®å»“\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "    M = cv2.moments(largest)\n",
    "\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "    return (cx, cy)\n",
    "\n",
    "# è‡ªå®šä¹‰è§†é¢‘æ˜¾ç¤ºåŒ…è£…å™¨ï¼Œç”¨äºåœ¨è§†é¢‘ä¸­æ˜¾ç¤ºåŠ¨ä½œå’Œä»£ç†ä½ç½®\n",
    "class VideoDisplayWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_action = None\n",
    "        self.action_names = {\n",
    "            0: \"NOOP\",\n",
    "            1: \"FIRE\",\n",
    "            2: \"UP\",\n",
    "            3: \"RIGHT\",\n",
    "            4: \"LEFT\",\n",
    "            5: \"DOWN\",\n",
    "            11: \"RIGHT-FIRE\",\n",
    "            12: \"LEFT-FIRE\"\n",
    "        }\n",
    "        \n",
    "    def step(self, action):\n",
    "        # è®°å½•å½“å‰åŠ¨ä½œ\n",
    "        self.current_action = action\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.current_action = None\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def render(self):\n",
    "        # è·å–åŸå§‹æ¸²æŸ“å¸§\n",
    "        frame = self.env.render()\n",
    "        \n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        # ç¡®ä¿å¸§æ˜¯RGBæ ¼å¼\n",
    "        if len(frame.shape) == 2:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # 1. åœ¨å³ä¸Šè§’æ˜¾ç¤ºå½“å‰åŠ¨ä½œ\n",
    "        if self.current_action is not None:\n",
    "            action_name = self.action_names.get(self.current_action, f\"ACTION_{self.current_action}\")\n",
    "            cv2.putText(frame, \n",
    "                       action_name, \n",
    "                       (frame.shape[1] - 50, 10), # å³ä¸Šè§’ä½ç½®\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.3,\n",
    "                       (255, 255, 255), # ç™½è‰²æ–‡æœ¬\n",
    "                       1, \n",
    "                       cv2.LINE_AA)\n",
    "        \n",
    "        # 2. åœ¨ä»£ç†ä½ç½®ç»˜åˆ¶ç»¿è‰²åœ†ç‚¹\n",
    "        position = get_agent_position(frame)\n",
    "        if position:\n",
    "            x, y = position\n",
    "            cv2.circle(frame, \n",
    "                      (x, y), \n",
    "                      2, # åœ†ç‚¹åŠå¾„\n",
    "                      (0, 255, 0), # ç»¿è‰²\n",
    "                      -1) # å¡«å……åœ†\n",
    "        \n",
    "        return frame\n",
    "\n",
    "# è‡ªå®šä¹‰å¥–åŠ±åŒ…è£…å™¨ï¼Œç”¨äºæ ¹æ®Agentçš„ä½ç½®å˜åŒ–è°ƒæ•´å¥–åŠ±\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, y_static_penalty=0, up_success_reward=5,\n",
    "                 up_fail_penalty=0, x_static_penalty=0.01,\n",
    "                 y_threshold=3, x_threshold=3, \n",
    "                 y_static_frames=30, x_static_frames=30):\n",
    "        super().__init__(env)\n",
    "        # å¥–åŠ±å‚æ•°\n",
    "        self.y_static_penalty = y_static_penalty  # å‚ç›´é™æ­¢æƒ©ç½š\n",
    "        self.up_success_reward = up_success_reward  # æˆåŠŸå‘ä¸Šç§»åŠ¨å¥–åŠ±\n",
    "        self.up_fail_penalty = up_fail_penalty  # å‘ä¸Šå¤±è´¥æƒ©ç½š\n",
    "        self.x_static_penalty = x_static_penalty  # æ°´å¹³é™æ­¢æƒ©ç½š\n",
    "        \n",
    "        # é˜ˆå€¼å‚æ•°\n",
    "        self.y_threshold = y_threshold  # å‚ç›´ç§»åŠ¨é˜ˆå€¼\n",
    "        self.x_threshold = x_threshold  # æ°´å¹³ç§»åŠ¨é˜ˆå€¼\n",
    "        self.y_static_frames = y_static_frames  # å‚ç›´é™æ­¢åˆ¤å®šå¸§æ•°\n",
    "        self.x_static_frames = x_static_frames  # æ°´å¹³é™æ­¢åˆ¤å®šå¸§æ•°\n",
    "        \n",
    "        # çŠ¶æ€è®°å½•\n",
    "        self.prev_positions = []  # å­˜å‚¨è¿‡å»çš„ä½ç½® [(x, y), ...]\n",
    "        self.y_static_count = 0  # å‚ç›´é™æ­¢è®¡æ•°\n",
    "        self.x_static_count = 0  # æ°´å¹³é™æ­¢è®¡æ•°\n",
    "        self.prev_action = None  # ä¸Šä¸€ä¸ªåŠ¨ä½œ\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # é‡ç½®çŠ¶æ€è®°å½•\n",
    "        self.prev_positions = []\n",
    "        self.y_static_count = 0\n",
    "        self.x_static_count = 0\n",
    "        self.prev_action = None\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # è®°å½•å½“å‰åŠ¨ä½œ\n",
    "        self.prev_action = action\n",
    "        \n",
    "        # æ‰§è¡Œç¯å¢ƒæ­¥éª¤\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # ä»è§‚å¯Ÿä¸­æå–RGBå¸§\n",
    "        frame = None\n",
    "        try:\n",
    "            if isinstance(obs, np.ndarray):\n",
    "                if len(obs.shape) == 4:  # (stack, height, width, channel)\n",
    "                    frame = obs[-1]  # æœ€åä¸€å¸§\n",
    "                elif len(obs.shape) == 3:  # (height, width, channel)\n",
    "                    frame = obs\n",
    "                elif len(obs.shape) == 2:  # (height, width)\n",
    "                    frame = obs\n",
    "            elif hasattr(obs, '__getitem__'):\n",
    "                # å¯¹äº FrameStackObservation\n",
    "                try:\n",
    "                    frame = obs[-1]\n",
    "                except:\n",
    "                    try:\n",
    "                        frame = obs[3]  # å‡è®¾æ˜¯4å¸§å †å \n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # å¦‚æœä¸Šè¿°å°è¯•éƒ½å¤±è´¥ï¼Œå°è¯•æ¸²æŸ“ç¯å¢ƒ\n",
    "            if frame is None:\n",
    "                try:\n",
    "                    frame = self.env.render()\n",
    "                except:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(f\"ä»è§‚å¯Ÿä¸­æå–å¸§æ—¶å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "            frame = None\n",
    "        \n",
    "        # æ£€æµ‹Agentä½ç½®\n",
    "        position = None\n",
    "        if frame is not None:\n",
    "            position = get_agent_position(frame)\n",
    "        \n",
    "        # å¦‚æœæ£€æµ‹åˆ°ä½ç½®ï¼Œåˆ™æ›´æ–°ä½ç½®å†å²å¹¶è®¡ç®—å¥–åŠ±è°ƒæ•´\n",
    "        additional_reward = 0\n",
    "        \n",
    "        if position is not None:\n",
    "            x, y = position\n",
    "            self.prev_positions.append((x, y))\n",
    "            \n",
    "            # ä¿æŒå†å²è®°å½•åœ¨åˆç†å¤§å°\n",
    "            if len(self.prev_positions) > max(self.y_static_frames, self.x_static_frames):\n",
    "                self.prev_positions.pop(0)\n",
    "            \n",
    "            # è‡³å°‘æœ‰ä¸¤ä¸ªä½ç½®è®°å½•æ‰èƒ½åˆ¤æ–­ç§»åŠ¨\n",
    "            if len(self.prev_positions) >= 2:\n",
    "                prev_x, prev_y = self.prev_positions[-2]\n",
    "                \n",
    "                # 1. æ£€æŸ¥å‚ç›´æ–¹å‘æ˜¯å¦é™æ­¢\n",
    "                if abs(y - prev_y) < self.y_threshold:\n",
    "                    self.y_static_count += 1\n",
    "                    if self.y_static_count >= self.y_static_frames:\n",
    "                        # çº¿æ€§å¢åŠ æƒ©ç½š\n",
    "                        additional_reward -= self.y_static_penalty * (self.y_static_count - self.y_static_frames + 1)\n",
    "                else:\n",
    "                    self.y_static_count = 0\n",
    "                \n",
    "                # 2. æ£€æŸ¥UPåŠ¨ä½œçš„æ•ˆæœ\n",
    "                if self.prev_action == 2:  # å‡è®¾2æ˜¯UPåŠ¨ä½œ\n",
    "                    if (prev_y - y) > self.y_threshold:  # æˆåŠŸå‘ä¸Šç§»åŠ¨\n",
    "                        additional_reward += self.up_success_reward\n",
    "                    else:  # æœªæˆåŠŸå‘ä¸Šç§»åŠ¨\n",
    "                        additional_reward -= self.up_fail_penalty\n",
    "                \n",
    "                # 3. æ£€æŸ¥æ°´å¹³æ–¹å‘æ˜¯å¦é™æ­¢\n",
    "                if abs(x - prev_x) < self.x_threshold:\n",
    "                    self.x_static_count += 1\n",
    "                    if self.x_static_count >= self.x_static_frames:\n",
    "                        # çº¿æ€§å¢åŠ æƒ©ç½š\n",
    "                        additional_reward -= self.x_static_penalty * (self.x_static_count - self.x_static_frames + 1)\n",
    "                else:\n",
    "                    self.x_static_count = 0\n",
    "        \n",
    "        # åº”ç”¨å¥–åŠ±è°ƒæ•´\n",
    "        adjusted_reward = reward + additional_reward\n",
    "        \n",
    "        return obs, adjusted_reward, terminated, truncated, info\n",
    "\n",
    "# åˆ›å»ºé¢„å¤„ç†åçš„ç¯å¢ƒçš„å‡½æ•°\n",
    "def make_env(env_id, idx, capture_video=False, run_name=None):\n",
    "    def thunk():\n",
    "        import ale_py\n",
    "        \n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            # æ·»åŠ è§†é¢‘æ˜¾ç¤ºåŒ…è£…å™¨\n",
    "            env = VideoDisplayWrapper(env)\n",
    "            env = RecordVideo(\n",
    "                env,\n",
    "                VIDEO_PATH,\n",
    "                episode_trigger=lambda x: True,\n",
    "                name_prefix=f\"donkey_kong_{run_name}\"\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        env = ActionRestrictWrapper(env, ALLOWED_ACTIONS)\n",
    "        env = ForceFirstFireWrapper(env)\n",
    "        # æ·»åŠ è‡ªå®šä¹‰å¥–åŠ±åŒ…è£…å™¨\n",
    "        env = CustomRewardWrapper(env)\n",
    "        env = AtariWrapper(env, terminal_on_life_loss=True, frame_skip=FRAME_SKIP)\n",
    "        env = FrameStackObservation(env, 4)  # å †å 4å¸§ä»¥æ•è·æ—¶é—´ä¿¡æ¯\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# åˆ›å»ºå¹¶è¡Œç¯å¢ƒ\n",
    "def make_vec_env(env_id, num_envs, seed=SEED):\n",
    "    env_fns = [make_env(env_id, i) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(env_fns)\n",
    "    envs.seed(seed)\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DQNç½‘ç»œæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # è¾“å…¥å½¢çŠ¶: (batch, stack_frames, height, width)\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¼˜å…ˆç»éªŒå›æ”¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾æé«˜è®­ç»ƒæ•ˆç‡\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # æ§åˆ¶ä¼˜å…ˆçº§çš„ç¨‹åº¦\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1  # å½“å‰å¸§ï¼Œç”¨äºbetaè®¡ç®—\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        # betaä»beta_startçº¿æ€§å¢åŠ åˆ°1.0\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # æ·»åŠ æ–°çš„ç»éªŒ\n",
    "        max_prio = np.max(self.priorities) if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(self.Transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.position] = self.Transition(*args)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        # è®¡ç®—é‡‡æ ·æ¦‚ç‡\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºæ‰¹é‡å¤„ç†æ ¼å¼\n",
    "        batch = self.Transition(*zip(*samples))\n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.tensor(batch.action, device=device)\n",
    "        rewards = torch.tensor(batch.reward, device=device, dtype=torch.float32)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        dones = torch.tensor(batch.done, device=device, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # æ›´æ–°ä¼˜å…ˆçº§\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DQNä»£ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # åˆ›å»ºç­–ç•¥ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ\n",
    "        self.policy_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # ç›®æ ‡ç½‘ç»œä¸éœ€è¦è®¡ç®—æ¢¯åº¦\n",
    "        \n",
    "        # è®¾ç½®ä¼˜åŒ–å™¨\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # è®­ç»ƒç›¸å…³å‚æ•°\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # æ—¥å¿—è®°å½•å™¨\n",
    "        self.writer = SummaryWriter(LOG_PATH)\n",
    "    \n",
    "    def select_action(self, state, eval_mode=False):\n",
    "        # Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ\n",
    "        sample = random.random()\n",
    "        # åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œå§‹ç»ˆé€‰æ‹©æœ€ä½³åŠ¨ä½œ\n",
    "        if eval_mode:\n",
    "            eps_threshold = 0.05  # è¯„ä¼°æ—¶ä½¿ç”¨å°çš„epsilonï¼Œå¢åŠ ä¸€äº›æ¢ç´¢æ€§\n",
    "        else:\n",
    "            # çº¿æ€§è¡°å‡epsilon\n",
    "            self.epsilon = max(EPSILON_END, EPSILON_START - self.steps_done / EPSILON_DECAY)\n",
    "            eps_threshold = self.epsilon\n",
    "            \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0  # ç¼“å†²åŒºä¸­çš„æ ·æœ¬ä¸è¶³\n",
    "        \n",
    "        # ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # è®¡ç®—å½“å‰Qå€¼\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # ä½¿ç”¨Double DQNè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„Qå€¼\n",
    "        # ä½¿ç”¨ç­–ç•¥ç½‘ç»œé€‰æ‹©åŠ¨ä½œ\n",
    "        next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        # ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°åŠ¨ä½œ\n",
    "        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "        # å°†ç»ˆæ­¢çŠ¶æ€çš„ä¸‹ä¸€ä¸ªQå€¼è®¾ä¸º0\n",
    "        next_q_values = next_q_values * (1 - dones)\n",
    "        # è®¡ç®—ç›®æ ‡Qå€¼\n",
    "        target_q_values = rewards + GAMMA * next_q_values\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±ï¼ˆTDè¯¯å·®ï¼‰\n",
    "        td_error = torch.abs(q_values - target_q_values).detach().cpu().numpy()\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values, reduction='none') * weights\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # ä¼˜åŒ–æ¨¡å‹\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # æ›´æ–°ä¼˜å…ˆçº§\n",
    "        self.memory.update_priorities(indices, td_error + 1e-5)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps_done': self.steps_done,\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. é¢„å¤„ç†å’ŒçŠ¶æ€è½¬æ¢å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # æŠŠå †å çš„4å¸§å›¾åƒè½¬æ¢ä¸ºPyTorchçš„è¾“å…¥æ ¼å¼\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    return tensor / 255.0  # å½’ä¸€åŒ–\n",
    "\n",
    "def preprocess_batch_observation(obs):\n",
    "    # å¤„ç†æ‰¹é‡è§‚å¯Ÿæ•°æ®\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "    return tensor / 255.0  # å½’ä¸€åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. è¯„ä¼°å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env_id, num_episodes=5, video_prefix=\"evaluation\"):\n",
    "    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ŒåŒ…æ‹¬è§†é¢‘å½•åˆ¶\n",
    "    env = make_env(env_id, 0, capture_video=True, run_name=video_prefix)()  \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    env.close()\n",
    "    return np.mean(episode_rewards), np.std(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. è®­ç»ƒå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, envs, num_frames):\n",
    "    # åˆå§‹åŒ–ç¯å¢ƒå’Œè¿›åº¦æ¡\n",
    "    obs = envs.reset()\n",
    "    obs_tensor = preprocess_batch_observation(obs)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = np.zeros(NUM_ENVS)\n",
    "    episode_length = np.zeros(NUM_ENVS)\n",
    "    \n",
    "    progress_bar = tqdm(range(1, num_frames + 1), desc=\"Training\")\n",
    "    \n",
    "    # è®­ç»ƒå¾ªç¯\n",
    "    for frame_idx in progress_bar:\n",
    "        # é€‰æ‹©åŠ¨ä½œ\n",
    "        actions = []\n",
    "        for i in range(NUM_ENVS):\n",
    "            action = agent.select_action(obs_tensor[i:i+1])\n",
    "            actions.append(action.item())\n",
    "        \n",
    "        # æ‰§è¡ŒåŠ¨ä½œ\n",
    "        next_obs, rewards, terminateds, truncateds = envs.step(actions)\n",
    "        \n",
    "        # å¤„ç†æ¯ä¸ªç¯å¢ƒçš„æ•°æ®\n",
    "        dones = []\n",
    "        for t, tr in zip(terminateds, truncateds):\n",
    "            if isinstance(tr, dict):\n",
    "                done = t or tr.get(\"TimeLimit.truncated\", False)\n",
    "            else:\n",
    "                done = t or tr\n",
    "            dones.append(done)\n",
    "\n",
    "        next_obs_tensor = preprocess_batch_observation(next_obs)\n",
    "        \n",
    "        # æ›´æ–°ç´¯è®¡å¥–åŠ±å’Œå›åˆé•¿åº¦\n",
    "        episode_reward += rewards\n",
    "        episode_length += 1\n",
    "        \n",
    "        # å°†æ•°æ®å­˜å…¥ç»éªŒå›æ”¾ç¼“å†²åŒº\n",
    "        for i in range(NUM_ENVS):\n",
    "            agent.memory.push(\n",
    "                obs_tensor[i:i+1],\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_obs_tensor[i:i+1],\n",
    "                float(dones[i])\n",
    "            )\n",
    "        \n",
    "        # æ›´æ–°è§‚å¯Ÿ\n",
    "        obs = next_obs\n",
    "        obs_tensor = next_obs_tensor\n",
    "        \n",
    "        # ä¼˜åŒ–æ¨¡å‹\n",
    "        loss = agent.optimize_model()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰å›åˆç»“æŸ\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                # è®°å½•å›åˆç»“æœ\n",
    "                agent.writer.add_scalar(\"train/episode_reward\", episode_reward[i], agent.steps_done)\n",
    "                agent.writer.add_scalar(\"train/episode_length\", episode_length[i], agent.steps_done)\n",
    "                all_rewards.append(episode_reward[i])\n",
    "                \n",
    "                # é‡ç½®å›åˆç»Ÿè®¡\n",
    "                episode_reward[i] = 0\n",
    "                episode_length[i] = 0\n",
    "        \n",
    "        # æ›´æ–°ç›®æ ‡ç½‘ç»œ\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯\n",
    "        if frame_idx % 1000 == 0:\n",
    "            mean_reward = np.mean(all_rewards[-100:]) if all_rewards else 0\n",
    "            mean_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            agent.writer.add_scalar(\"train/epsilon\", agent.epsilon, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/loss\", mean_loss, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/mean_reward_100\", mean_reward, frame_idx)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"avg_reward\": f\"{mean_reward:.2f}\",\n",
    "                \"loss\": f\"{mean_loss:.5f}\",\n",
    "                \"epsilon\": f\"{agent.epsilon:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        if frame_idx % SAVE_INTERVAL == 0:\n",
    "            save_path = os.path.join(SAVE_PATH, f\"model_{frame_idx}.pt\")\n",
    "            agent.save_model(save_path)\n",
    "            print(f\"\\næ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}\")\n",
    "        \n",
    "        # è¯„ä¼°æ¨¡å‹\n",
    "        if frame_idx % EVAL_INTERVAL == 0:\n",
    "            print(\"\\nå¼€å§‹è¯„ä¼°...\")\n",
    "            eval_reward, eval_std = evaluate(\n",
    "                agent,\n",
    "                ENV_NAME,\n",
    "                num_episodes=EVAL_EPISODES,\n",
    "                video_prefix=f\"eval_{frame_idx}\"\n",
    "            )\n",
    "            agent.writer.add_scalar(\"eval/mean_reward\", eval_reward, frame_idx)\n",
    "            agent.writer.add_scalar(\"eval/reward_std\", eval_std, frame_idx)\n",
    "            print(f\"è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = {eval_reward:.2f} Â± {eval_std:.2f}\")\n",
    "        \n",
    "        # æ›´æ–°ä»£ç†çš„æ­¥æ•°è®¡æ•°å™¨\n",
    "        agent.steps_done += 1\n",
    "    \n",
    "    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    final_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "    agent.save_model(final_path)\n",
    "    print(f\"\\næœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}\")\n",
    "\n",
    "\n",
    "def load_demonstrations(agent, filepath):\n",
    "    \"\"\"åŠ è½½ç¤ºèŒƒè½¨è¿¹æ–‡ä»¶å¹¶æ³¨å…¥ agent çš„ replay buffer\"\"\"\n",
    "    import pickle\n",
    "\n",
    "    # å’Œè®­ç»ƒä¸­ä½¿ç”¨çš„ ALLOWED_ACTIONS ä¿æŒä¸€è‡´\n",
    "    ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]\n",
    "    action_to_index = {a: i for i, a in enumerate(ALLOWED_ACTIONS)}\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        all_trajectories = pickle.load(f)\n",
    "\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "    for traj in all_trajectories:\n",
    "        for s, a, r, ns, d in traj:\n",
    "            if a not in action_to_index:\n",
    "                print(f\"âš ï¸ éæ³•åŠ¨ä½œç¼–å· {a}ï¼Œè·³è¿‡\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            action_index = action_to_index[a]  # æ˜ å°„æˆ 0~7\n",
    "\n",
    "            agent.memory.push(\n",
    "                s.to(device),\n",
    "                action_index,\n",
    "                r.to(device),\n",
    "                ns.to(device),\n",
    "                d.to(device)\n",
    "            )\n",
    "            count += 1\n",
    "\n",
    "    print(f\"ğŸš€ å¯¼å…¥ç¤ºèŒƒè½¨è¿¹å®Œæˆï¼Œå…± {count} æ¡ transition å·²åŠ å…¥ replay bufferã€‚è·³è¿‡ {skipped} æ¡éæ³•æ•°æ®ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ä¸»è®­ç»ƒæµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è§‚å¯Ÿç©ºé—´å½¢çŠ¶: (4, 84, 84)\n",
      "åŠ¨ä½œç©ºé—´å¤§å°: 8\n",
      "ğŸš€ å¯¼å…¥ç¤ºèŒƒè½¨è¿¹å®Œæˆï¼Œå…± 142 æ¡ transition å·²åŠ å…¥ replay bufferã€‚è·³è¿‡ 0 æ¡éæ³•æ•°æ®ã€‚\n",
      "å¼€å§‹è®­ç»ƒ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f6854c96084bf78abced85e4712991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/15000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-1:\n",
      "Process ForkServerProcess-4:\n",
      "Process ForkServerProcess-2:\n",
      "Process ForkServerProcess-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "                                                       ^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/stateful_observation.py\", line 425, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 595, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 560, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 112, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 178, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 167, in step\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 31, in step\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 636, in step\n",
      "    return self.env.step(self.action(action))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/ale_py/env.py\", line 311, in step\n",
      "    reward += self.ale.act(action_idx, strength)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "                                                       ^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/stateful_observation.py\", line 425, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 595, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 560, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 112, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 178, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 167, in step\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 31, in step\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 636, in step\n",
      "    return self.env.step(self.action(action))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/ale_py/env.py\", line 311, in step\n",
      "    reward += self.ale.act(action_idx, strength)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "                                                       ^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/stateful_observation.py\", line 425, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 595, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 560, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 112, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 178, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 167, in step\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 31, in step\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 636, in step\n",
      "    return self.env.step(self.action(action))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/ale_py/env.py\", line 311, in step\n",
      "    reward += self.ale.act(action_idx, strength)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 35, in _worker\n",
      "    observation, reward, terminated, truncated, info = env.step(data)\n",
      "                                                       ^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/stateful_observation.py\", line 425, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 595, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 560, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 112, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/atari_wrappers.py\", line 178, in step\n",
      "    obs, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 167, in step\n",
      "  File \"/tmp/ipykernel_32564/2165886544.py\", line 31, in step\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 636, in step\n",
      "    return self.env.step(self.action(action))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/ale_py/env.py\", line 311, in step\n",
      "    reward += self.ale.act(action_idx, strength)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# å¼€å§‹è®­ç»ƒ\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33må¼€å§‹è®­ç»ƒ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_FRAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# å…³é—­ç¯å¢ƒ\u001b[39;00m\n\u001b[32m     24\u001b[39m envs.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(agent, envs, num_frames)\u001b[39m\n\u001b[32m     19\u001b[39m     actions.append(action.item())\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# æ‰§è¡ŒåŠ¨ä½œ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m next_obs, rewards, terminateds, truncateds = \u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# å¤„ç†æ¯ä¸ªç¯å¢ƒçš„æ•°æ®\u001b[39;00m\n\u001b[32m     25\u001b[39m dones = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[39m, in \u001b[36mVecEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03mStep the environments with the given action\u001b[39;00m\n\u001b[32m    202\u001b[39m \n\u001b[32m    203\u001b[39m \u001b[33;03m:param actions: the action\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[33;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28mself\u001b[39m.step_async(actions)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[39m, in \u001b[36mSubprocVecEnv.step_wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     results = \u001b[43m[\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mremotes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mself\u001b[39m.waiting = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    131\u001b[39m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m.reset_infos = \u001b[38;5;28mzip\u001b[39m(*results)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:129\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> VecEnvStepReturn:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     results = [\u001b[43mremote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.remotes]\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mself\u001b[39m.waiting = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    131\u001b[39m     obs, rews, dones, infos, \u001b[38;5;28mself\u001b[39m.reset_infos = \u001b[38;5;28mzip\u001b[39m(*results)  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py:250\u001b[39m, in \u001b[36m_ConnectionBase.recv\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ForkingPickler.loads(buf.getbuffer())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py:430\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m size == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py:395\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    393\u001b[39m remaining = size\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     chunk = read(handle, remaining)\n\u001b[32m    396\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå¹¶è¡Œç¯å¢ƒ\n",
    "envs = make_vec_env(ENV_NAME, NUM_ENVS)\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "obs_shape = (4, 84, 84)  # å †å çš„4å¸§ï¼Œæ¯å¸§84x84\n",
    "n_actions = envs.action_space.n\n",
    "\n",
    "print(f\"è§‚å¯Ÿç©ºé—´å½¢çŠ¶: {obs_shape}\")\n",
    "print(f\"åŠ¨ä½œç©ºé—´å¤§å°: {n_actions}\")\n",
    "\n",
    "# åˆ›å»ºDQNä»£ç†\n",
    "agent = DQNAgent(obs_shape, n_actions)\n",
    "\n",
    "# åŠ è½½ç¤ºèŒƒè½¨è¿¹\n",
    "if DEMO_PATH:\n",
    "    Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    load_demonstrations(agent, DEMO_PATH)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "print(\"å¼€å§‹è®­ç»ƒ...\")\n",
    "train(agent, envs, NUM_FRAMES)\n",
    "\n",
    "# å…³é—­ç¯å¢ƒ\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. åŠ è½½å’Œæµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_video(model_path, env_id, num_episodes=5):\n",
    "    # åˆ›å»ºç¯å¢ƒ\n",
    "    env = make_env(env_id, 0, capture_video=True, run_name=\"final_test\")()  \n",
    "    \n",
    "    # åˆ›å»ºä»£ç†å¹¶åŠ è½½æ¨¡å‹\n",
    "    obs_shape = (4, 84, 84)  # å †å çš„4å¸§ï¼Œæ¯å¸§84x84\n",
    "    n_actions = env.action_space.n\n",
    "    agent = DQNAgent(obs_shape, n_actions)\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    # æµ‹è¯•è®­ç»ƒå¥½çš„ä»£ç†\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "            \n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½å¹¶æµ‹è¯•æœ€ç»ˆæ¨¡å‹\n",
    "model_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "play_and_record_video(model_path, ENV_NAME, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. å¯è§†åŒ–è®­ç»ƒç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨TensorBoardå¯è§†åŒ–è®­ç»ƒç»“æœ\n",
    "print(f\"å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤åœ¨ç»ˆç«¯ä¸­å¯åŠ¨TensorBoardæŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡:\")\n",
    "print(f\"tensorboard --logdir={LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
