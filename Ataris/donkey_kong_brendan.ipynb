{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨DQNè®­ç»ƒAtari Donkey Kong\n",
    "\n",
    "æœ¬notebookå®ç°äº†ä¸€ä¸ªDQNä»£ç†æ¥ç©Atariæ¸¸æˆDonkey Kongï¼Œå¹¶åŒ…å«ä»¥ä¸‹ç‰¹æ€§ï¼š\n",
    "- å¹¶è¡Œè®­ç»ƒå¤šä¸ªæ¸¸æˆç¯å¢ƒ\n",
    "- é¢„å¤„ç†æ¸¸æˆå¸§ä»¥æé«˜è®­ç»ƒæ•ˆç‡\n",
    "- ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾æé«˜è®­ç»ƒè´¨é‡\n",
    "- è®­ç»ƒæ—¥å¿—è®°å½•\n",
    "- å®šæœŸä¿å­˜æ¨¡å‹\n",
    "- å®šæœŸè¯„ä¼°å¹¶å½•åˆ¶æ¸¸æˆè§†é¢‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å®‰è£…å¿…è¦çš„ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "# %pip install stable-baselines3[extra] gymnasium[atari] numpy matplotlib opencv-python tensorboard autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å¯¼å…¥åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, FrameStackObservation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "import ale_py\n",
    "from gymnasium import spaces\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨è®¾å¤‡: cuda\n"
     ]
    }
   ],
   "source": [
    "# ç¯å¢ƒå‚æ•°\n",
    "ENV_NAME = \"ALE/DonkeyKong-v5\"\n",
    "NUM_ENVS = 4  # å¹¶è¡Œç¯å¢ƒæ•°é‡\n",
    "FRAME_SKIP = 4  # è·³å¸§æ•°ï¼Œæ¯éš”4å¸§è¿›è¡Œä¸€æ¬¡å†³ç­–\n",
    "ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]  # æœ‰æ•ˆåŠ¨ä½œ\n",
    "\n",
    "# æ¨¡å‹å‚æ•°\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # æŠ˜æ‰£å› å­\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 100000  # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°\n",
    "TARGET_UPDATE = 10000  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "NUM_FRAMES = 10_000_000  # æ€»è®­ç»ƒå¸§æ•°\n",
    "NUM_FRAMES = 1000  # æ€»è®­ç»ƒå¸§æ•°\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 6_000_000\n",
    "DEMO_PATH = \"./demo/dk_demo_20250325_192148.pkl\"\n",
    "\n",
    "# ä¿å­˜å’Œè¯„ä¼°å‚æ•°\n",
    "SAVE_INTERVAL = 100_000  # ä¿å­˜æ¨¡å‹çš„é—´éš”ï¼ˆå¸§æ•°ï¼‰\n",
    "EVAL_INTERVAL = 20_000   # è¯„ä¼°æ¨¡å‹çš„é—´éš”ï¼ˆå¸§æ•°ï¼‰\n",
    "EVAL_INTERVAL = 500   # è¯„ä¼°æ¨¡å‹çš„é—´éš”ï¼ˆå¸§æ•°ï¼‰\n",
    "EVAL_EPISODES = 3       # æ¯æ¬¡è¯„ä¼°çš„æ¸¸æˆå±€æ•°\n",
    "\n",
    "# åˆ›å»ºä¿å­˜æ¨¡å‹å’Œæ—¥å¿—çš„ç›®å½•\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_PATH = f\"./models/donkey_kong_{timestamp}\"\n",
    "LOG_PATH = f\"./logs/donkey_kong_{timestamp}\"\n",
    "VIDEO_PATH = f\"./videos/donkey_kong_{timestamp}\"\n",
    "\n",
    "for path in [SAVE_PATH, LOG_PATH, VIDEO_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ç¯å¢ƒé¢„å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é™åˆ¶åŠ¨ä½œç©ºé—´,å‡å°‘ agent çš„æ— ç”¨åŠ¨ä½œ\n",
    "class ActionRestrictWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, allowed_actions):\n",
    "        super().__init__(env)\n",
    "        self.allowed_actions = allowed_actions\n",
    "        self.action_space = spaces.Discrete(len(self.allowed_actions))\n",
    "\n",
    "    def action(self, act):\n",
    "        # æŠŠ agent è¾“å‡ºçš„åŠ¨ä½œç´¢å¼•æ˜ å°„æˆåŸåŠ¨ä½œç¼–å·\n",
    "        return self.allowed_actions[act]\n",
    "\n",
    "    def reverse_action(self, act):\n",
    "        return self.allowed_actions.index(act)\n",
    "\n",
    "# å¼ºåˆ¶é¦–ä¸ªåŠ¨ä½œä¸ºFIREçš„åŒ…è£…å™¨\n",
    "class ForceFirstFireWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.first_action_done = False\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.first_action_done = False\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # å¦‚æœæ˜¯é¦–ä¸ªåŠ¨ä½œä¸”ä¸æ˜¯FIRE(1)ï¼Œåˆ™å¼ºåˆ¶æ›¿æ¢ä¸ºFIRE\n",
    "        if not self.first_action_done:\n",
    "            self.first_action_done = True\n",
    "            # ä½¿ç”¨FIREåŠ¨ä½œ(ç´¢å¼•1)ä»£æ›¿ä¼ å…¥çš„åŠ¨ä½œ\n",
    "            return self.env.step(1)  # 1å¯¹åº”FIREåŠ¨ä½œ\n",
    "        return self.env.step(action)\n",
    "\n",
    "# æ ¹æ®é¢œè‰²æ£€æµ‹äººç‰©ä½ç½®çš„å‡½æ•°\n",
    "def get_agent_position(frame): \n",
    "    \"\"\" æ ¹æ®é¢œè‰²æ£€æµ‹äººç‰©ä½ç½®ï¼Œè¿”å› (x, y) åæ ‡ã€‚æœªæ£€æµ‹åˆ°åˆ™è¿”å› Noneã€‚ \"\"\"\n",
    "    # ç¡®ä¿frameæ˜¯numpyæ•°ç»„ä¸”æ ¼å¼æ­£ç¡®\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # ç›®æ ‡é¢œè‰²ï¼ˆBGR æ ¼å¼ï¼‰\n",
    "    target_bgr = np.array([194, 64, 82], dtype=np.uint8)\n",
    "\n",
    "    # å®¹å·®èŒƒå›´ï¼ˆå¯è°ƒï¼Œ20~40 ä¸€èˆ¬æ¯”è¾ƒåˆé€‚ï¼‰\n",
    "    tolerance = 30\n",
    "    lower = np.array([max(0, c - tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "    upper = np.array([min(255, c + tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "\n",
    "    # ç”Ÿæˆæ©ç \n",
    "    mask = cv2.inRange(frame, lower, upper)\n",
    "    \n",
    "    # æŸ¥æ‰¾è½®å»“\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # æ‰¾é¢ç§¯æœ€å¤§è½®å»“\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "    M = cv2.moments(largest)\n",
    "\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "    return (cx, cy)\n",
    "\n",
    "# è‡ªå®šä¹‰è§†é¢‘æ˜¾ç¤ºåŒ…è£…å™¨ï¼Œç”¨äºåœ¨è§†é¢‘ä¸­æ˜¾ç¤ºåŠ¨ä½œå’Œä»£ç†ä½ç½®\n",
    "class VideoDisplayWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_action = None\n",
    "        self.action_names = {\n",
    "            0: \"\",\n",
    "            1: \"Jump\",\n",
    "            2: \"Up\",\n",
    "            3: \"Right\",\n",
    "            4: \"Left\",\n",
    "            5: \"Down\",\n",
    "            11: \"Jump Right\",\n",
    "            12: \"Jump Left\"\n",
    "        }\n",
    "        \n",
    "    def step(self, action):\n",
    "        # è®°å½•å½“å‰åŠ¨ä½œ\n",
    "        self.current_action = action\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.current_action = None\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def render(self):\n",
    "        # è·å–åŸå§‹æ¸²æŸ“å¸§\n",
    "        frame = self.env.render()\n",
    "        \n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        # ç¡®ä¿å¸§æ˜¯RGBæ ¼å¼\n",
    "        if len(frame.shape) == 2:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # 1. åœ¨å³ä¸Šè§’æ˜¾ç¤ºå½“å‰åŠ¨ä½œ\n",
    "        if self.current_action is not None:\n",
    "            action_name = self.action_names.get(self.current_action, f\"ACTION_{self.current_action}\")\n",
    "            cv2.putText(frame, \n",
    "                       action_name, \n",
    "                       (frame.shape[1] - 60, 20), # å³ä¸Šè§’ä½ç½®\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.3,\n",
    "                       (255, 255, 255), # ç™½è‰²æ–‡æœ¬\n",
    "                       1, \n",
    "                       cv2.LINE_AA)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "# è‡ªå®šä¹‰å¥–åŠ±åŒ…è£…å™¨ï¼Œç”¨äºæ ¹æ®Agentçš„ä½ç½®å˜åŒ–è°ƒæ•´å¥–åŠ±\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, y_static_penalty=0.1, up_success_reward=10,\n",
    "                 up_fail_penalty=0, x_static_penalty=0,\n",
    "                 y_threshold=20, x_threshold=3, \n",
    "                 y_static_frames=30, x_static_frames=30):\n",
    "        super().__init__(env)\n",
    "        # å¥–åŠ±å‚æ•°\n",
    "        self.y_static_penalty = y_static_penalty  # å‚ç›´é™æ­¢æƒ©ç½š\n",
    "        self.up_success_reward = up_success_reward  # æˆåŠŸå‘ä¸Šç§»åŠ¨å¥–åŠ±\n",
    "        self.up_fail_penalty = up_fail_penalty  # å‘ä¸Šå¤±è´¥æƒ©ç½š\n",
    "        self.x_static_penalty = x_static_penalty  # æ°´å¹³é™æ­¢æƒ©ç½š\n",
    "        \n",
    "        # é˜ˆå€¼å‚æ•°\n",
    "        self.y_threshold = y_threshold  # å‚ç›´ç§»åŠ¨é˜ˆå€¼\n",
    "        self.x_threshold = x_threshold  # æ°´å¹³ç§»åŠ¨é˜ˆå€¼\n",
    "        self.y_static_frames = y_static_frames  # å‚ç›´é™æ­¢åˆ¤å®šå¸§æ•°\n",
    "        self.x_static_frames = x_static_frames  # æ°´å¹³é™æ­¢åˆ¤å®šå¸§æ•°\n",
    "        \n",
    "        # çŠ¶æ€è®°å½•\n",
    "        self.prev_positions = []  # å­˜å‚¨è¿‡å»çš„ä½ç½® [(x, y), ...]\n",
    "        self.y_static_count = 0  # å‚ç›´é™æ­¢è®¡æ•°\n",
    "        self.x_static_count = 0  # æ°´å¹³é™æ­¢è®¡æ•°\n",
    "        self.prev_action = None  # ä¸Šä¸€ä¸ªåŠ¨ä½œ\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # é‡ç½®çŠ¶æ€è®°å½•\n",
    "        self.prev_positions = []\n",
    "        self.y_static_count = 0\n",
    "        self.x_static_count = 0\n",
    "        self.prev_action = None\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # è®°å½•å½“å‰åŠ¨ä½œ\n",
    "        self.prev_action = action\n",
    "        \n",
    "        # æ‰§è¡Œç¯å¢ƒæ­¥éª¤\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # ä»è§‚å¯Ÿä¸­æå–RGBå¸§\n",
    "        frame = None\n",
    "        try:\n",
    "            if isinstance(obs, np.ndarray):\n",
    "                if len(obs.shape) == 4:  # (stack, height, width, channel)\n",
    "                    frame = obs[-1]  # æœ€åä¸€å¸§\n",
    "                elif len(obs.shape) == 3:  # (height, width, channel)\n",
    "                    frame = obs\n",
    "                elif len(obs.shape) == 2:  # (height, width)\n",
    "                    frame = obs\n",
    "            elif hasattr(obs, '__getitem__'):\n",
    "                # å¯¹äº FrameStackObservation\n",
    "                try:\n",
    "                    frame = obs[-1]\n",
    "                except:\n",
    "                    try:\n",
    "                        frame = obs[3]  # å‡è®¾æ˜¯4å¸§å †å \n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # å¦‚æœä¸Šè¿°å°è¯•éƒ½å¤±è´¥ï¼Œå°è¯•æ¸²æŸ“ç¯å¢ƒ\n",
    "            if frame is None:\n",
    "                try:\n",
    "                    frame = self.env.render()\n",
    "                except:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract frame from observation: {e}\")\n",
    "            frame = None\n",
    "        \n",
    "        # æ£€æµ‹Agentä½ç½®\n",
    "        position = None\n",
    "        if frame is not None:\n",
    "            position = get_agent_position(frame)\n",
    "        \n",
    "        # å¦‚æœæ£€æµ‹åˆ°ä½ç½®ï¼Œåˆ™æ›´æ–°ä½ç½®å†å²å¹¶è®¡ç®—å¥–åŠ±è°ƒæ•´\n",
    "        additional_reward = 0\n",
    "        \n",
    "        if position is not None:\n",
    "            x, y = position\n",
    "            self.prev_positions.append((x, y))\n",
    "            \n",
    "            # ä¿æŒå†å²è®°å½•åœ¨åˆç†å¤§å°\n",
    "            if len(self.prev_positions) > max(self.y_static_frames, self.x_static_frames):\n",
    "                self.prev_positions.pop(0)\n",
    "            \n",
    "            # è‡³å°‘æœ‰ä¸¤ä¸ªä½ç½®è®°å½•æ‰èƒ½åˆ¤æ–­ç§»åŠ¨\n",
    "            if len(self.prev_positions) >= 2:\n",
    "                prev_x, prev_y = self.prev_positions[-2]\n",
    "                \n",
    "                # 1. æ£€æŸ¥å‚ç›´æ–¹å‘æ˜¯å¦é™æ­¢\n",
    "                if abs(y - prev_y) < self.y_threshold:\n",
    "                    self.y_static_count += 1\n",
    "                    if self.y_static_count >= self.y_static_frames:\n",
    "                        # çº¿æ€§å¢åŠ æƒ©ç½š\n",
    "                        additional_reward -= self.y_static_penalty * (self.y_static_count - self.y_static_frames + 1)\n",
    "                else:\n",
    "                    self.y_static_count = 0\n",
    "                \n",
    "                # 2. æ£€æŸ¥UPåŠ¨ä½œçš„æ•ˆæœ\n",
    "                if self.prev_action == 2:  # å‡è®¾2æ˜¯UPåŠ¨ä½œ\n",
    "                    if (prev_y - y) > self.y_threshold:  # æˆåŠŸå‘ä¸Šç§»åŠ¨\n",
    "                        additional_reward += self.up_success_reward\n",
    "                    else:  # æœªæˆåŠŸå‘ä¸Šç§»åŠ¨\n",
    "                        additional_reward -= self.up_fail_penalty\n",
    "                \n",
    "                # 3. æ£€æŸ¥æ°´å¹³æ–¹å‘æ˜¯å¦é™æ­¢\n",
    "                if abs(x - prev_x) < self.x_threshold:\n",
    "                    self.x_static_count += 1\n",
    "                    if self.x_static_count >= self.x_static_frames:\n",
    "                        # çº¿æ€§å¢åŠ æƒ©ç½š\n",
    "                        additional_reward -= self.x_static_penalty * (self.x_static_count - self.x_static_frames + 1)\n",
    "                else:\n",
    "                    self.x_static_count = 0\n",
    "        \n",
    "        # åº”ç”¨å¥–åŠ±è°ƒæ•´\n",
    "        adjusted_reward = reward + additional_reward\n",
    "        \n",
    "        return obs, adjusted_reward, terminated, truncated, info\n",
    "\n",
    "# åˆ›å»ºé¢„å¤„ç†åçš„ç¯å¢ƒçš„å‡½æ•°\n",
    "def make_env(env_id, idx, capture_video=False, run_name=None):\n",
    "    def thunk():\n",
    "        import ale_py\n",
    "        \n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            # æ·»åŠ è§†é¢‘æ˜¾ç¤ºåŒ…è£…å™¨\n",
    "            env = VideoDisplayWrapper(env)\n",
    "            env = RecordVideo(\n",
    "                env,\n",
    "                VIDEO_PATH,\n",
    "                episode_trigger=lambda x: True,\n",
    "                name_prefix=f\"donkey_kong_{run_name}\"\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        env = ActionRestrictWrapper(env, ALLOWED_ACTIONS)\n",
    "        env = ForceFirstFireWrapper(env)\n",
    "        # æ·»åŠ è‡ªå®šä¹‰å¥–åŠ±åŒ…è£…å™¨\n",
    "        env = CustomRewardWrapper(env)\n",
    "        env = AtariWrapper(env, terminal_on_life_loss=True, frame_skip=FRAME_SKIP)\n",
    "        env = FrameStackObservation(env, 4)  # å †å 4å¸§ä»¥æ•è·æ—¶é—´ä¿¡æ¯\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# åˆ›å»ºå¹¶è¡Œç¯å¢ƒ\n",
    "def make_vec_env(env_id, num_envs, seed=SEED):\n",
    "    env_fns = [make_env(env_id, i) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(env_fns)\n",
    "    envs.seed(seed)\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DQNç½‘ç»œæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # è¾“å…¥å½¢çŠ¶: (batch, stack_frames, height, width)\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ä¼˜å…ˆç»éªŒå›æ”¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾æé«˜è®­ç»ƒæ•ˆç‡\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # æ§åˆ¶ä¼˜å…ˆçº§çš„ç¨‹åº¦\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1  # å½“å‰å¸§ï¼Œç”¨äºbetaè®¡ç®—\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        # betaä»beta_startçº¿æ€§å¢åŠ åˆ°1.0\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # æ·»åŠ æ–°çš„ç»éªŒ\n",
    "        max_prio = np.max(self.priorities) if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(self.Transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.position] = self.Transition(*args)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        # è®¡ç®—é‡‡æ ·æ¦‚ç‡\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # è½¬æ¢ä¸ºæ‰¹é‡å¤„ç†æ ¼å¼\n",
    "        batch = self.Transition(*zip(*samples))\n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.tensor(batch.action, device=device)\n",
    "        rewards = torch.tensor(batch.reward, device=device, dtype=torch.float32)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        dones = torch.tensor(batch.done, device=device, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # æ›´æ–°ä¼˜å…ˆçº§\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DQNä»£ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # åˆ›å»ºç­–ç•¥ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ\n",
    "        self.policy_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # ç›®æ ‡ç½‘ç»œä¸éœ€è¦è®¡ç®—æ¢¯åº¦\n",
    "        \n",
    "        # è®¾ç½®ä¼˜åŒ–å™¨\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # åˆ›å»ºç»éªŒå›æ”¾ç¼“å†²åŒº\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # è®­ç»ƒç›¸å…³å‚æ•°\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # æ—¥å¿—è®°å½•å™¨\n",
    "        self.writer = SummaryWriter(LOG_PATH)\n",
    "    \n",
    "    def select_action(self, state, eval_mode=False):\n",
    "        # Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ\n",
    "        sample = random.random()\n",
    "        # åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œå§‹ç»ˆé€‰æ‹©æœ€ä½³åŠ¨ä½œ\n",
    "        if eval_mode:\n",
    "            eps_threshold = 0.05  # è¯„ä¼°æ—¶ä½¿ç”¨å°çš„epsilonï¼Œå¢åŠ ä¸€äº›æ¢ç´¢æ€§\n",
    "        else:\n",
    "            # çº¿æ€§è¡°å‡epsilon\n",
    "            self.epsilon = max(EPSILON_END, EPSILON_START - self.steps_done / EPSILON_DECAY)\n",
    "            eps_threshold = self.epsilon\n",
    "            \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0  # ç¼“å†²åŒºä¸­çš„æ ·æœ¬ä¸è¶³\n",
    "        \n",
    "        # ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # è®¡ç®—å½“å‰Qå€¼\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # ä½¿ç”¨Double DQNè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„Qå€¼\n",
    "        # ä½¿ç”¨ç­–ç•¥ç½‘ç»œé€‰æ‹©åŠ¨ä½œ\n",
    "        next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        # ä½¿ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°åŠ¨ä½œ\n",
    "        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "        # å°†ç»ˆæ­¢çŠ¶æ€çš„ä¸‹ä¸€ä¸ªQå€¼è®¾ä¸º0\n",
    "        next_q_values = next_q_values * (1 - dones)\n",
    "        # è®¡ç®—ç›®æ ‡Qå€¼\n",
    "        target_q_values = rewards + GAMMA * next_q_values\n",
    "        \n",
    "        # è®¡ç®—æŸå¤±ï¼ˆTDè¯¯å·®ï¼‰\n",
    "        td_error = torch.abs(q_values - target_q_values).detach().cpu().numpy()\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values, reduction='none') * weights\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # ä¼˜åŒ–æ¨¡å‹\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # æ›´æ–°ä¼˜å…ˆçº§\n",
    "        self.memory.update_priorities(indices, td_error + 1e-5)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps_done': self.steps_done,\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. é¢„å¤„ç†å’ŒçŠ¶æ€è½¬æ¢å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # æŠŠå †å çš„4å¸§å›¾åƒè½¬æ¢ä¸ºPyTorchçš„è¾“å…¥æ ¼å¼\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    return tensor / 255.0  # å½’ä¸€åŒ–\n",
    "\n",
    "def preprocess_batch_observation(obs):\n",
    "    # å¤„ç†æ‰¹é‡è§‚å¯Ÿæ•°æ®\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "    return tensor / 255.0  # å½’ä¸€åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. è¯„ä¼°å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env_id, num_episodes=5, video_prefix=\"evaluation\"):\n",
    "    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ŒåŒ…æ‹¬è§†é¢‘å½•åˆ¶\n",
    "    env = make_env(env_id, 0, capture_video=True, run_name=video_prefix)()  \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    env.close()\n",
    "    return np.mean(episode_rewards), np.std(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. è®­ç»ƒå‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, envs, num_frames):\n",
    "    # åˆå§‹åŒ–ç¯å¢ƒå’Œè¿›åº¦æ¡\n",
    "    obs = envs.reset()\n",
    "    obs_tensor = preprocess_batch_observation(obs)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = np.zeros(NUM_ENVS)\n",
    "    episode_length = np.zeros(NUM_ENVS)\n",
    "    \n",
    "    progress_bar = tqdm(range(1, num_frames + 1), desc=\"Training\")\n",
    "    \n",
    "    # è®­ç»ƒå¾ªç¯\n",
    "    for frame_idx in progress_bar:\n",
    "        # é€‰æ‹©åŠ¨ä½œ\n",
    "        actions = []\n",
    "        for i in range(NUM_ENVS):\n",
    "            action = agent.select_action(obs_tensor[i:i+1])\n",
    "            actions.append(action.item())\n",
    "        \n",
    "        # æ‰§è¡ŒåŠ¨ä½œ\n",
    "        next_obs, rewards, terminateds, truncateds = envs.step(actions)\n",
    "        \n",
    "        # å¤„ç†æ¯ä¸ªç¯å¢ƒçš„æ•°æ®\n",
    "        dones = []\n",
    "        for t, tr in zip(terminateds, truncateds):\n",
    "            if isinstance(tr, dict):\n",
    "                done = t or tr.get(\"TimeLimit.truncated\", False)\n",
    "            else:\n",
    "                done = t or tr\n",
    "            dones.append(done)\n",
    "\n",
    "        next_obs_tensor = preprocess_batch_observation(next_obs)\n",
    "        \n",
    "        # æ›´æ–°ç´¯è®¡å¥–åŠ±å’Œå›åˆé•¿åº¦\n",
    "        episode_reward += rewards\n",
    "        episode_length += 1\n",
    "        \n",
    "        # å°†æ•°æ®å­˜å…¥ç»éªŒå›æ”¾ç¼“å†²åŒº\n",
    "        for i in range(NUM_ENVS):\n",
    "            agent.memory.push(\n",
    "                obs_tensor[i:i+1],\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_obs_tensor[i:i+1],\n",
    "                float(dones[i])\n",
    "            )\n",
    "        \n",
    "        # æ›´æ–°è§‚å¯Ÿ\n",
    "        obs = next_obs\n",
    "        obs_tensor = next_obs_tensor\n",
    "        \n",
    "        # ä¼˜åŒ–æ¨¡å‹\n",
    "        loss = agent.optimize_model()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰å›åˆç»“æŸ\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                # è®°å½•å›åˆç»“æœ\n",
    "                agent.writer.add_scalar(\"train/episode_reward\", episode_reward[i], agent.steps_done)\n",
    "                agent.writer.add_scalar(\"train/episode_length\", episode_length[i], agent.steps_done)\n",
    "                all_rewards.append(episode_reward[i])\n",
    "                \n",
    "                # é‡ç½®å›åˆç»Ÿè®¡\n",
    "                episode_reward[i] = 0\n",
    "                episode_length[i] = 0\n",
    "        \n",
    "        # æ›´æ–°ç›®æ ‡ç½‘ç»œ\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # è®°å½•è®­ç»ƒç»Ÿè®¡ä¿¡æ¯\n",
    "        if frame_idx % 1000 == 0:\n",
    "            mean_reward = np.mean(all_rewards[-100:]) if all_rewards else 0\n",
    "            mean_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            agent.writer.add_scalar(\"train/epsilon\", agent.epsilon, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/loss\", mean_loss, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/mean_reward_100\", mean_reward, frame_idx)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"avg_reward\": f\"{mean_reward:.2f}\",\n",
    "                \"loss\": f\"{mean_loss:.5f}\",\n",
    "                \"epsilon\": f\"{agent.epsilon:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        if frame_idx % SAVE_INTERVAL == 0:\n",
    "            save_path = os.path.join(SAVE_PATH, f\"model_{frame_idx}.pt\")\n",
    "            agent.save_model(save_path)\n",
    "            print(f\"\\nFrame {frame_idx}: Model saved to {save_path}\")\n",
    "        \n",
    "        # è¯„ä¼°æ¨¡å‹\n",
    "        if frame_idx % EVAL_INTERVAL == 0:\n",
    "            print(f\"\\nFrame {frame_idx}: Evaluating...\")\n",
    "            eval_reward, eval_std = evaluate(\n",
    "                agent,\n",
    "                ENV_NAME,\n",
    "                num_episodes=EVAL_EPISODES,\n",
    "                video_prefix=f\"eval_{frame_idx}\"\n",
    "            )\n",
    "            agent.writer.add_scalar(\"eval/mean_reward\", eval_reward, frame_idx)\n",
    "            agent.writer.add_scalar(\"eval/reward_std\", eval_std, frame_idx)\n",
    "            print(f\"Evaluation results: Mean reward = {eval_reward:.2f} Â± {eval_std:.2f}\")\n",
    "        \n",
    "        # æ›´æ–°ä»£ç†çš„æ­¥æ•°è®¡æ•°å™¨\n",
    "        agent.steps_done += 1\n",
    "    \n",
    "    # è®­ç»ƒç»“æŸï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "    final_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "    agent.save_model(final_path)\n",
    "    print(f\"\\nFinal model saved to: {final_path}\")\n",
    "\n",
    "\n",
    "def load_demonstrations(agent, filepath):\n",
    "    \"\"\"åŠ è½½ç¤ºèŒƒè½¨è¿¹æ–‡ä»¶å¹¶æ³¨å…¥ agent çš„ replay buffer\"\"\"\n",
    "    import pickle\n",
    "\n",
    "    # å’Œè®­ç»ƒä¸­ä½¿ç”¨çš„ ALLOWED_ACTIONS ä¿æŒä¸€è‡´\n",
    "    ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]\n",
    "    action_to_index = {a: i for i, a in enumerate(ALLOWED_ACTIONS)}\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        all_trajectories = pickle.load(f)\n",
    "\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "    for traj in all_trajectories:\n",
    "        for s, a, r, ns, d in traj:\n",
    "            if a not in action_to_index:\n",
    "                print(f\"Skipping illegal action: {a}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            action_index = action_to_index[a]  # æ˜ å°„æˆ 0~7\n",
    "\n",
    "            agent.memory.push(\n",
    "                s.to(device),\n",
    "                action_index,\n",
    "                r.to(device),\n",
    "                ns.to(device),\n",
    "                d.to(device)\n",
    "            )\n",
    "            count += 1\n",
    "\n",
    "    print(f\"Demonstrations imported, {count} transitions added to replay buffer. Skipped {skipped} illegal actions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ä¸»è®­ç»ƒæµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è§‚å¯Ÿç©ºé—´å½¢çŠ¶: (4, 84, 84)\n",
      "åŠ¨ä½œç©ºé—´å¤§å°: 8\n",
      "ğŸš€ å¯¼å…¥ç¤ºèŒƒè½¨è¿¹å®Œæˆï¼Œå…± 2448 æ¡ transition å·²åŠ å…¥ replay bufferã€‚è·³è¿‡ 0 æ¡éæ³•æ•°æ®ã€‚\n",
      "å¼€å§‹è®­ç»ƒ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacfd5b52a9c4984b4a43874811e4869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/15000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¼€å§‹è¯„ä¼°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/f/workspace/RL-final-proj/Ataris/videos/donkey_kong_20250325_193425 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -16.67 Â± 9.74\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_100000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/f/workspace/RL-final-proj/Ataris/videos/donkey_kong_20250325_193425 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -44.67 Â± 29.01\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -34.00 Â± 7.48\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_200000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -20.33 Â± 9.39\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -14.67 Â± 6.55\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_300000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -31.67 Â± 4.64\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -34.33 Â± 25.00\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_400000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -25.67 Â± 15.17\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -31.33 Â± 17.75\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_500000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -37.33 Â± 10.40\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -34.00 Â± 20.93\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_600000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -14.00 Â± 2.45\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -28.00 Â± 16.31\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_700000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -25.67 Â± 3.68\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -16.00 Â± 9.93\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_800000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -15.00 Â± 2.94\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -25.67 Â± 6.34\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_900000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -25.67 Â± 16.05\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -28.67 Â± 9.57\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_1000000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -19.00 Â± 11.52\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -25.33 Â± 12.92\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_1100000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -16.00 Â± 2.94\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -18.00 Â± 3.74\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_1200000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -25.67 Â± 6.24\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -19.67 Â± 7.41\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_1300000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -33.33 Â± 3.30\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -35.67 Â± 4.99\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_1400000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -24.00 Â± 13.49\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -26.67 Â± 11.09\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_1500000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -26.00 Â± 10.80\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -14.33 Â± 7.54\n",
      "\n",
      "æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/donkey_kong_20250325_193425/model_1600000.pt\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -19.33 Â± 8.73\n",
      "\n",
      "å¼€å§‹è¯„ä¼°...\n",
      "è¯„ä¼°ç»“æœ: å¹³å‡å¥–åŠ± = -39.33 Â± 8.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# å¼€å§‹è®­ç»ƒ\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33må¼€å§‹è®­ç»ƒ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_FRAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# å…³é—­ç¯å¢ƒ\u001b[39;00m\n\u001b[32m     24\u001b[39m envs.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(agent, envs, num_frames)\u001b[39m\n\u001b[32m     51\u001b[39m obs_tensor = next_obs_tensor\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# ä¼˜åŒ–æ¨¡å‹\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m loss = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m losses.append(loss)\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# æ£€æŸ¥æ˜¯å¦æœ‰å›åˆç»“æŸ\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mDQNAgent.optimize_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     60\u001b[39m target_q_values = rewards + GAMMA * next_q_values\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# è®¡ç®—æŸå¤±ï¼ˆTDè¯¯å·®ï¼‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m td_error = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_q_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy()\n\u001b[32m     64\u001b[39m loss = F.smooth_l1_loss(q_values, target_q_values, reduction=\u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m) * weights\n\u001b[32m     65\u001b[39m loss = loss.mean()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå¹¶è¡Œç¯å¢ƒ\n",
    "envs = make_vec_env(ENV_NAME, NUM_ENVS)\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "obs_shape = (4, 84, 84)  # å †å çš„4å¸§ï¼Œæ¯å¸§84x84\n",
    "n_actions = envs.action_space.n\n",
    "\n",
    "print(f\"Observation shape: {obs_shape}\")\n",
    "print(f\"Action space size: {n_actions}\")\n",
    "\n",
    "# åˆ›å»ºDQNä»£ç†\n",
    "agent = DQNAgent(obs_shape, n_actions)\n",
    "\n",
    "# åŠ è½½ç¤ºèŒƒè½¨è¿¹\n",
    "if DEMO_PATH:\n",
    "    Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    load_demonstrations(agent, DEMO_PATH)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "print(\"Training started...\")\n",
    "\n",
    "# å¯ç”¨cudnnçš„benchmarkæ¨¡å¼ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "train(agent, envs, NUM_FRAMES)\n",
    "\n",
    "# å…³é—­ç¯å¢ƒ\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. åŠ è½½å’Œæµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_video(model_path, env_id, num_episodes=5):\n",
    "    # åˆ›å»ºç¯å¢ƒ\n",
    "    env = make_env(env_id, 0, capture_video=True, run_name=\"final_test\")()  \n",
    "    \n",
    "    # åˆ›å»ºä»£ç†å¹¶åŠ è½½æ¨¡å‹\n",
    "    obs_shape = (4, 84, 84)  # å †å çš„4å¸§ï¼Œæ¯å¸§84x84\n",
    "    n_actions = env.action_space.n\n",
    "    agent = DQNAgent(obs_shape, n_actions)\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    # æµ‹è¯•è®­ç»ƒå¥½çš„ä»£ç†\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "            \n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"Average reward: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -49.0\n",
      "Episode 2: Reward = -86.0\n",
      "Episode 3: Reward = -45.0\n",
      "Episode 4: Reward = -80.0\n",
      "Episode 5: Reward = -47.0\n",
      "å¹³å‡å¥–åŠ±: -61.40 Â± 17.78\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½å¹¶æµ‹è¯•æœ€ç»ˆæ¨¡å‹\n",
    "model_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "play_and_record_video(model_path, ENV_NAME, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. å¯è§†åŒ–è®­ç»ƒç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤åœ¨ç»ˆç«¯ä¸­å¯åŠ¨TensorBoardæŸ¥çœ‹è®­ç»ƒæŒ‡æ ‡:\n",
      "tensorboard --logdir=./logs/donkey_kong_20250325_192520\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨TensorBoardå¯è§†åŒ–è®­ç»ƒç»“æœ\n",
    "print(f\"To visualize training results, run the following command in the terminal:\")\n",
    "print(f\"tensorboard --logdir={LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
