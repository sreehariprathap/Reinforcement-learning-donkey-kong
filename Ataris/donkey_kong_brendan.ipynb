{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用DQN训练Atari Donkey Kong\n",
    "\n",
    "本notebook实现了一个DQN代理来玩Atari游戏Donkey Kong，并包含以下特性：\n",
    "- 并行训练多个游戏环境\n",
    "- 预处理游戏帧以提高训练效率\n",
    "- 使用优先经验回放提高训练质量\n",
    "- 训练日志记录\n",
    "- 定期保存模型\n",
    "- 定期评估并录制游戏视频"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 安装必要的依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要的库\n",
    "# %pip install stable-baselines3[extra] gymnasium[atari] numpy matplotlib opencv-python tensorboard autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, FrameStackObservation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "import ale_py\n",
    "\n",
    "# 设置随机种子，保证实验可复现\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "# 环境参数\n",
    "ENV_NAME = \"ALE/DonkeyKong-v5\"\n",
    "NUM_ENVS = 1  # 并行环境数量\n",
    "FRAME_SKIP = 4  # 跳帧数，每隔4帧进行一次决策\n",
    "\n",
    "# 模型参数\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99  # 折扣因子\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 100000  # 经验回放缓冲区大小\n",
    "TARGET_UPDATE = 10000  # 目标网络更新频率\n",
    "\n",
    "# 训练参数\n",
    "NUM_FRAMES = 10_000_000  # 总训练帧数\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 1000000\n",
    "\n",
    "# 保存和评估参数\n",
    "SAVE_INTERVAL = 100_000  # 保存模型的间隔（帧数）\n",
    "EVAL_INTERVAL = 50_000   # 评估模型的间隔（帧数）\n",
    "EVAL_EPISODES = 3       # 每次评估的游戏局数\n",
    "\n",
    "# 创建保存模型和日志的目录\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_PATH = f\"./models/donkey_kong_{timestamp}\"\n",
    "LOG_PATH = f\"./logs/donkey_kong_{timestamp}\"\n",
    "VIDEO_PATH = f\"./videos/donkey_kong_{timestamp}\"\n",
    "\n",
    "for path in [SAVE_PATH, LOG_PATH, VIDEO_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# 设置设备（GPU或CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 环境预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建预处理后的环境的函数\n",
    "def make_env(env_id, idx, capture_video=False, run_name=None):\n",
    "    def thunk():\n",
    "        import ale_py\n",
    "\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = RecordVideo(\n",
    "                env,\n",
    "                VIDEO_PATH,\n",
    "                episode_trigger=lambda x: True,\n",
    "                name_prefix=f\"donkey_kong_{run_name}\"\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = AtariWrapper(env, terminal_on_life_loss=True, frame_skip=FRAME_SKIP)\n",
    "        env = FrameStackObservation(env, 4)  # 堆叠4帧以捕获时间信息\n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# 创建并行环境\n",
    "def make_vec_env(env_id, num_envs, seed=SEED):\n",
    "    env_fns = [make_env(env_id, i) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(env_fns)\n",
    "    envs.seed(seed)\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DQN网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 输入形状: (batch, stack_frames, height, width)\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 优先经验回放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用优先经验回放提高训练效率\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # 控制优先级的程度\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1  # 当前帧，用于beta计算\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        # beta从beta_start线性增加到1.0\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # 添加新的经验\n",
    "        max_prio = np.max(self.priorities) if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(self.Transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.position] = self.Transition(*args)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        # 计算采样概率\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # 计算重要性采样权重\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # 转换为批量处理格式\n",
    "        batch = self.Transition(*zip(*samples))\n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.tensor(batch.action, device=device)\n",
    "        rewards = torch.tensor(batch.reward, device=device, dtype=torch.float32)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        dones = torch.tensor(batch.done, device=device, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # 更新优先级\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DQN代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # 创建策略网络和目标网络\n",
    "        self.policy_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # 目标网络不需要计算梯度\n",
    "        \n",
    "        # 设置优化器\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # 创建经验回放缓冲区\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # 训练相关参数\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # 日志记录器\n",
    "        self.writer = SummaryWriter(LOG_PATH)\n",
    "    \n",
    "    def select_action(self, state, eval_mode=False):\n",
    "        # ε-贪婪策略选择动作\n",
    "        sample = random.random()\n",
    "        # 在评估模式下，始终选择最佳动作\n",
    "        if eval_mode:\n",
    "            eps_threshold = 0.05  # 评估时使用小的epsilon，增加一些探索性\n",
    "        else:\n",
    "            # 线性衰减epsilon\n",
    "            self.epsilon = max(EPSILON_END, EPSILON_START - self.steps_done / EPSILON_DECAY)\n",
    "            eps_threshold = self.epsilon\n",
    "            \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0  # 缓冲区中的样本不足\n",
    "        \n",
    "        # 从经验回放缓冲区中采样\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # 计算当前Q值\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # 使用Double DQN计算下一个状态的Q值\n",
    "        # 使用策略网络选择动作\n",
    "        next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        # 使用目标网络评估动作\n",
    "        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "        # 将终止状态的下一个Q值设为0\n",
    "        next_q_values = next_q_values * (1 - dones)\n",
    "        # 计算目标Q值\n",
    "        target_q_values = rewards + GAMMA * next_q_values\n",
    "        \n",
    "        # 计算损失（TD误差）\n",
    "        td_error = torch.abs(q_values - target_q_values).detach().cpu().numpy()\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values, reduction='none') * weights\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # 优化模型\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 梯度裁剪，防止梯度爆炸\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 更新优先级\n",
    "        self.memory.update_priorities(indices, td_error + 1e-5)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps_done': self.steps_done,\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 预处理和状态转换函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # 把堆叠的4帧图像转换为PyTorch的输入格式\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    return tensor / 255.0  # 归一化\n",
    "\n",
    "def preprocess_batch_observation(obs):\n",
    "    # 处理批量观察数据\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "    return tensor / 255.0  # 归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env_id, num_episodes=5, video_prefix=\"evaluation\"):\n",
    "    # 创建评估环境，包括视频录制\n",
    "    env = make_env(env_id, 0, capture_video=True, run_name=video_prefix)()  \n",
    "    episode_rewards = []\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    env.close()\n",
    "    return np.mean(episode_rewards), np.std(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, envs, num_frames):\n",
    "    # 初始化环境和进度条\n",
    "    obs = envs.reset()\n",
    "    obs_tensor = preprocess_batch_observation(obs)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = np.zeros(NUM_ENVS)\n",
    "    episode_length = np.zeros(NUM_ENVS)\n",
    "    \n",
    "    progress_bar = tqdm(range(1, num_frames + 1), desc=\"Training\")\n",
    "    \n",
    "    # 训练循环\n",
    "    for frame_idx in progress_bar:\n",
    "        # 选择动作\n",
    "        actions = []\n",
    "        for i in range(NUM_ENVS):\n",
    "            action = agent.select_action(obs_tensor[i:i+1])\n",
    "            actions.append(action.item())\n",
    "        \n",
    "        # 执行动作\n",
    "        next_obs, rewards, terminateds, truncateds = envs.step(actions)\n",
    "        \n",
    "        # 处理每个环境的数据\n",
    "        # dones = [t or tr for t, tr in zip(terminateds, truncateds)]\n",
    "\n",
    "        dones = []\n",
    "        for t, tr in zip(terminateds, truncateds):\n",
    "            if isinstance(tr, dict):\n",
    "                done = t or tr.get(\"TimeLimit.truncated\", False)\n",
    "            else:\n",
    "                done = t or tr\n",
    "            dones.append(done)\n",
    "\n",
    "        next_obs_tensor = preprocess_batch_observation(next_obs)\n",
    "        \n",
    "        # 更新累计奖励和回合长度\n",
    "        episode_reward += rewards\n",
    "        episode_length += 1\n",
    "        \n",
    "        # 将数据存入经验回放缓冲区\n",
    "        for i in range(NUM_ENVS):\n",
    "            agent.memory.push(\n",
    "                obs_tensor[i:i+1],\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_obs_tensor[i:i+1],\n",
    "                float(dones[i])\n",
    "            )\n",
    "        \n",
    "        # 更新观察\n",
    "        obs = next_obs\n",
    "        obs_tensor = next_obs_tensor\n",
    "        \n",
    "        # 优化模型\n",
    "        loss = agent.optimize_model()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # 检查是否有回合结束\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                # 记录回合结果\n",
    "                agent.writer.add_scalar(\"train/episode_reward\", episode_reward[i], agent.steps_done)\n",
    "                agent.writer.add_scalar(\"train/episode_length\", episode_length[i], agent.steps_done)\n",
    "                all_rewards.append(episode_reward[i])\n",
    "                \n",
    "                # 重置回合统计\n",
    "                episode_reward[i] = 0\n",
    "                episode_length[i] = 0\n",
    "        \n",
    "        # 更新目标网络\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # 记录训练统计信息\n",
    "        if frame_idx % 1000 == 0:\n",
    "            mean_reward = np.mean(all_rewards[-100:]) if all_rewards else 0\n",
    "            mean_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            agent.writer.add_scalar(\"train/epsilon\", agent.epsilon, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/loss\", mean_loss, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/mean_reward_100\", mean_reward, frame_idx)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"avg_reward\": f\"{mean_reward:.2f}\",\n",
    "                \"loss\": f\"{mean_loss:.5f}\",\n",
    "                \"epsilon\": f\"{agent.epsilon:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # 保存模型\n",
    "        if frame_idx % SAVE_INTERVAL == 0:\n",
    "            save_path = os.path.join(SAVE_PATH, f\"model_{frame_idx}.pt\")\n",
    "            agent.save_model(save_path)\n",
    "            print(f\"\\n模型已保存到: {save_path}\")\n",
    "        \n",
    "        # 评估模型\n",
    "        if frame_idx % EVAL_INTERVAL == 0:\n",
    "            print(\"\\n开始评估...\")\n",
    "            eval_reward, eval_std = evaluate(\n",
    "                agent,\n",
    "                ENV_NAME,\n",
    "                num_episodes=EVAL_EPISODES,\n",
    "                video_prefix=f\"eval_{frame_idx}\"\n",
    "            )\n",
    "            agent.writer.add_scalar(\"eval/mean_reward\", eval_reward, frame_idx)\n",
    "            agent.writer.add_scalar(\"eval/reward_std\", eval_std, frame_idx)\n",
    "            print(f\"评估结果: 平均奖励 = {eval_reward:.2f} ± {eval_std:.2f}\")\n",
    "        \n",
    "        # 更新代理的步数计数器\n",
    "        agent.steps_done += 1\n",
    "    \n",
    "    # 训练结束，保存最终模型\n",
    "    final_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "    agent.save_model(final_path)\n",
    "    print(f\"\\n最终模型已保存到: {final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 主训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观察空间形状: (4, 84, 84)\n",
      "动作空间大小: 18\n",
      "开始训练...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cd7b51919648c69bcc89e270598011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/10000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始评估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.2+c9d4b19)\n",
      "[Powered by Stella]\n",
      "/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/f/workspace/RL-final-proj/Ataris/videos/donkey_kong_20250324_040030 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果: 平均奖励 = 0.33 ± 0.47\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_100000.pt\n",
      "\n",
      "开始评估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /mnt/f/workspace/RL-final-proj/Ataris/videos/donkey_kong_20250324_040030 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果: 平均奖励 = 0.67 ± 0.47\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.33 ± 1.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_200000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.00 ± 0.82\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 0.67 ± 0.47\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_300000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.00 ± 0.00\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.33 ± 2.62\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_400000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.33 ± 0.47\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.67 ± 2.05\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_500000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.33 ± 1.25\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 5.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_600000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.67 ± 0.47\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.00 ± 0.82\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_700000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.33 ± 0.47\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.00 ± 2.16\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_800000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.33 ± 4.78\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.33 ± 4.03\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_900000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.00 ± 3.27\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.67 ± 3.40\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1000000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.67 ± 5.91\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.67 ± 6.65\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1100000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 11.33 ± 7.36\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.33 ± 1.89\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1200000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.00 ± 2.83\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 10.33 ± 6.94\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1300000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.00 ± 2.16\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.67 ± 0.94\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1400000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.00 ± 6.48\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 12.67 ± 5.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1500000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 11.67 ± 7.93\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 9.00 ± 7.79\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1600000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.67 ± 1.25\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.00 ± 3.56\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1700000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 15.00 ± 7.07\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 1.70\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1800000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 10.33 ± 5.56\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.33 ± 1.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_1900000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.00 ± 2.45\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.00 ± 2.16\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2000000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.33 ± 1.70\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.67 ± 1.70\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2100000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.00 ± 1.63\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.33 ± 5.44\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2200000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 13.67 ± 7.54\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 11.67 ± 3.77\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2300000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.00 ± 2.45\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.67 ± 2.62\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2400000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.33 ± 0.47\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.67 ± 1.70\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2500000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.00 ± 4.97\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.33 ± 1.70\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2600000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 10.00 ± 7.79\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.33 ± 4.19\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2700000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 9.67 ± 2.49\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.67 ± 6.13\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2800000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.33 ± 6.85\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.00 ± 6.98\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_2900000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.33 ± 5.56\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.67 ± 1.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3000000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 12.33 ± 5.19\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.67 ± 1.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3100000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.67 ± 8.06\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.67 ± 5.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3200000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.33 ± 8.34\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.00 ± 0.82\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3300000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.33 ± 9.67\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.67 ± 2.62\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3400000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.67 ± 6.02\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.33 ± 2.62\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3500000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.00 ± 4.32\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.33 ± 1.70\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3600000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.33 ± 6.60\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.67 ± 0.94\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3700000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.00 ± 7.79\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.00 ± 4.08\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3800000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 11.00 ± 6.98\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.00 ± 1.41\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_3900000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.33 ± 4.50\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.00 ± 1.41\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4000000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 5.44\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.00 ± 0.82\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4100000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.00 ± 0.82\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 10.67 ± 7.41\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4200000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.33 ± 1.25\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.00 ± 5.35\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4300000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.67 ± 0.47\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.33 ± 0.47\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4400000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.00 ± 7.79\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.33 ± 0.47\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4500000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.67 ± 5.91\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 12.33 ± 5.73\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4600000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.00 ± 6.48\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 10.00 ± 7.79\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4700000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 13.00 ± 5.10\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.00 ± 6.38\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4800000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.67 ± 2.05\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.33 ± 1.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_4900000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 10.33 ± 4.71\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 1.70\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5000000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.33 ± 1.25\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 2.49\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5100000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.00 ± 1.41\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.33 ± 4.03\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5200000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 5.25\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.33 ± 4.71\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5300000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 12.00 ± 4.97\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.33 ± 7.72\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5400000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.00 ± 8.04\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.33 ± 6.85\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5500000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.00 ± 2.83\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.33 ± 3.30\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5600000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.00 ± 3.56\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 10.33 ± 6.60\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5700000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 9.33 ± 4.92\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 3.40\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5800000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.33 ± 0.47\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 2.67 ± 1.25\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_5900000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 1.33 ± 0.94\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 12.33 ± 5.44\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_6000000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 5.67 ± 1.70\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.67 ± 1.89\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_6100000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 3.67 ± 3.86\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 6.33 ± 2.05\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_6200000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 4.33 ± 2.87\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 7.00 ± 6.16\n",
      "\n",
      "模型已保存到: ./models/donkey_kong_20250324_040030/model_6300000.pt\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 8.00 ± 5.35\n",
      "\n",
      "开始评估...\n",
      "评估结果: 平均奖励 = 11.00 ± 1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/brendan/miniconda3/envs/rl-final/lib/python3.11/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 开始训练\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m开始训练...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUM_FRAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 关闭环境\u001b[39;00m\n\u001b[32m     19\u001b[39m envs.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(agent, envs, num_frames)\u001b[39m\n\u001b[32m     53\u001b[39m obs_tensor = next_obs_tensor\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 优化模型\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m loss = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m losses.append(loss)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 检查是否有回合结束\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mDQNAgent.optimize_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# 梯度裁剪，防止梯度爆炸\u001b[39;00m\n\u001b[32m     71\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.policy_net.parameters(), \u001b[32m10\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# 更新优先级\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.memory.update_priorities(indices, td_error + \u001b[32m1e-5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/torch/optim/optimizer.py:504\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    501\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/torch/optim/adam.py:943\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    941\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl-final/lib/python3.11/site-packages/torch/optim/adam.py:683\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    676\u001b[39m             device_grads = torch._foreach_add(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    677\u001b[39m                 device_grads, device_params, alpha=weight_decay\n\u001b[32m    678\u001b[39m             )\n\u001b[32m    680\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m    681\u001b[39m \u001b[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001b[39;00m\n\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# tensors are on the same device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    685\u001b[39m torch._foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[32m    688\u001b[39m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[32m    689\u001b[39m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[32m    690\u001b[39m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 创建并行环境\n",
    "envs = make_vec_env(ENV_NAME, NUM_ENVS)\n",
    "\n",
    "# 获取环境信息\n",
    "obs_shape = (4, 84, 84)  # 堆叠的4帧，每帧84x84\n",
    "n_actions = envs.action_space.n\n",
    "\n",
    "print(f\"观察空间形状: {obs_shape}\")\n",
    "print(f\"动作空间大小: {n_actions}\")\n",
    "\n",
    "# 创建DQN代理\n",
    "agent = DQNAgent(obs_shape, n_actions)\n",
    "\n",
    "# 开始训练\n",
    "print(\"开始训练...\")\n",
    "train(agent, envs, NUM_FRAMES)\n",
    "\n",
    "# 关闭环境\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 加载和测试训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_video(model_path, env_id, num_episodes=5):\n",
    "    # 创建环境\n",
    "    env = make_env(env_id, 0, capture_video=True, run_name=\"final_test\")()  \n",
    "    \n",
    "    # 创建代理并加载模型\n",
    "    obs_shape = (4, 84, 84)  # 堆叠的4帧，每帧84x84\n",
    "    n_actions = env.action_space.n\n",
    "    agent = DQNAgent(obs_shape, n_actions)\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    # 测试训练好的代理\n",
    "    rewards = []\n",
    "    for i in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "            \n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Episode {i+1}: Reward = {episode_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"平均奖励: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = 0.0\n",
      "Episode 2: Reward = 0.0\n",
      "Episode 3: Reward = 0.0\n",
      "Episode 4: Reward = 1.0\n",
      "Episode 5: Reward = 0.0\n",
      "平均奖励: 0.20 ± 0.40\n"
     ]
    }
   ],
   "source": [
    "# 加载并测试最终模型\n",
    "model_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "play_and_record_video(model_path, ENV_NAME, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. 可视化训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以通过以下命令在终端中启动TensorBoard查看训练指标:\n",
      "tensorboard --logdir=./logs/donkey_kong_20250324_034548\n"
     ]
    }
   ],
   "source": [
    "# 使用TensorBoard可视化训练结果\n",
    "print(f\"可以通过以下命令在终端中启动TensorBoard查看训练指标:\")\n",
    "print(f\"tensorboard --logdir={LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
