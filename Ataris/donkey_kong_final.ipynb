{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2baf0349",
   "metadata": {},
   "source": [
    "# Using DQN to Train Atari Donkey Kong\n",
    "\n",
    "This notebook implements a DQN (Deep Q-Network) agent to play the Atari game *Donkey Kong*. The implementation includes the following features:\n",
    "- Parallel training of multiple game environments\n",
    "- Preprocessing of game frames to improve training efficiency\n",
    "- Using prioritized experience replay to enhance training quality\n",
    "- Logging of training statistics\n",
    "- Periodic saving of the model\n",
    "- Periodic evaluation and recording of gameplay videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6d8df5",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f85f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Uncomment and run the following line if you haven't installed the dependencies.\n",
    "# %pip install stable-baselines3[extra] gymnasium[atari] numpy matplotlib opencv-python tensorboard autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a3d083",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea66e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "# Additional imports for the environment and DQN algorithm\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.logger import configure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b615ba3",
   "metadata": {},
   "source": [
    "## 3. Set Up Environment and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to preprocess game frames\n",
    "def preprocess_frame(frame):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "    # Resize to a fixed resolution (e.g., 84x84)\n",
    "    resized = cv2.resize(gray, (84, 84))\n",
    "    return resized\n",
    "\n",
    "# Create a custom wrapper for preprocessing frames\n",
    "from gymnasium import ObservationWrapper\n",
    "\n",
    "class PreprocessFrame(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        processed = preprocess_frame(obs)\n",
    "        return np.expand_dims(processed, axis=-1)\n",
    "\n",
    "# Set up the environment with the preprocessing wrapper\n",
    "def create_env():\n",
    "    env = gym.make(\"ALE/DonkeyKong-v5\", render_mode=\"rgb_array\")\n",
    "    env = PreprocessFrame(env)\n",
    "    return env\n",
    "\n",
    "# Use DummyVecEnv to create a vectorized environment for parallel training\n",
    "env = DummyVecEnv([create_env])\n",
    "# Optionally, stack frames if needed (e.g., stacking 4 consecutive frames)\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74ce06",
   "metadata": {},
   "source": [
    "## 4. Configure Training Callbacks and Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733d8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint callback to save the model periodically\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./models/',\n",
    "                                         name_prefix='dqn_donkeykong')\n",
    "\n",
    "# Configure the logger to record training metrics\n",
    "new_logger = configure('./logs/', [\"stdout\", \"csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749af2a",
   "metadata": {},
   "source": [
    "## 5. Define and Train the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59a7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN agent with the desired parameters\n",
    "model = DQN('CnnPolicy', env, learning_rate=1e-4, buffer_size=100000, learning_starts=1000,\n",
    "            batch_size=32, tau=1.0, gamma=0.99, train_freq=4, target_update_interval=1000,\n",
    "            exploration_fraction=0.1, exploration_final_eps=0.01, verbose=1)\n",
    "\n",
    "# Set the new logger to the model\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Train the agent. The total_timesteps parameter can be adjusted as needed\n",
    "model.learn(total_timesteps=200000, callback=checkpoint_callback)\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"dqn_donkeykong_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae16e87",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Video Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ab5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to evaluate the trained model\n",
    "def evaluate(model, num_episodes=5):\n",
    "    env = create_env()\n",
    "    total_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode {episode+1}: Reward = {episode_reward}\")\n",
    "    return total_rewards\n",
    "\n",
    "# Run evaluation\n",
    "evaluate(model, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7b7d3",
   "metadata": {},
   "source": [
    "## 7. Recording Gameplay Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53058253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, record a video of the trained agent playing the game\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "def record_video(model, video_folder='./videos/', episode_length=500):\n",
    "    # Create a new environment that records the gameplay\n",
    "    env = RecordVideo(create_env(), video_folder=video_folder, episode_trigger=lambda x: True)\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done and step < episode_length:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        step += 1\n",
    "    env.close()\n",
    "    print(f\"Video recorded in {video_folder}\")\n",
    "\n",
    "# Record a video for one episode\n",
    "record_video(model, episode_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddbd82b",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we implemented a DQN agent to play Atari *Donkey Kong*. We demonstrated how to preprocess game frames, set up a parallel environment, configure training callbacks, train the agent, evaluate its performance, and record gameplay videos. Adjust the parameters and training duration according to your needs for improved performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
