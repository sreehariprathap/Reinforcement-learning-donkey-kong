{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Atari Donkey Kong with DQN\n",
    "\n",
    "This notebook implements a DQN agent to play the Atari game Donkey Kong, with the following features:\n",
    "- Parallel training across multiple game environments\n",
    "- Game frame preprocessing for improved training efficiency\n",
    "- Prioritized experience replay to enhance training quality\n",
    "- Training log recording\n",
    "- Regular model saving\n",
    "- Periodic evaluation and game video recording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# %pip install stable-baselines3[extra] gymnasium[atari] numpy matplotlib opencv-python tensorboard autorom[accept-rom-license] ipywidgets gymnasium[other]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordVideo, FrameStackObservation\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque, namedtuple\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "import ale_py\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Environment parameters\n",
    "ENV_NAME = \"ALE/DonkeyKong-v5\"\n",
    "NUM_ENVS = 4  # Number of parallel environments\n",
    "FRAME_SKIP = 4  # Frame skip, make decisions every 4 frames\n",
    "ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]  # Valid actions\n",
    "\n",
    "# Model parameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  # Discount factor\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 100000  # Experience replay buffer size\n",
    "TARGET_UPDATE = 10000  # Target network update frequency\n",
    "\n",
    "# Training parameters\n",
    "NUM_FRAMES = 10_000_000  # Total training frames\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 6_000_000\n",
    "DEMO_PATH = \"./demo/dk_demo_20250325_192148.pkl\"\n",
    "\n",
    "# Save and evaluation parameters\n",
    "SAVE_INTERVAL = 100_000  # Model save interval (frames)\n",
    "EVAL_INTERVAL = 20_000   # Model evaluation interval (frames)\n",
    "EVAL_EPISODES = 3        # Number of episodes per evaluation\n",
    "\n",
    "# Create directories for saving models and logs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_PATH = f\"./models/donkey_kong_{timestamp}\"\n",
    "LOG_PATH = f\"./logs/donkey_kong_{timestamp}\"\n",
    "VIDEO_PATH = f\"./videos/donkey_kong_{timestamp}\"\n",
    "\n",
    "for path in [SAVE_PATH, LOG_PATH, VIDEO_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Set device (GPU or CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Environment Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict action space, reduce agent's useless actions\n",
    "class ActionRestrictWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env, allowed_actions):\n",
    "        super().__init__(env)\n",
    "        self.allowed_actions = allowed_actions\n",
    "        self.action_space = spaces.Discrete(len(self.allowed_actions))\n",
    "\n",
    "    def action(self, act):\n",
    "        # Map the action index output by the agent to the original action number\n",
    "        return self.allowed_actions[act]\n",
    "\n",
    "    def reverse_action(self, act):\n",
    "        return self.allowed_actions.index(act)\n",
    "\n",
    "# Wrapper that forces the first action to be FIRE\n",
    "class ForceFirstFireWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.first_action_done = False\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.first_action_done = False\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # If it's the first action and not RIGHT FIRE, force replace it with RIGHT FIRE\n",
    "        if not self.first_action_done:\n",
    "            self.first_action_done = True\n",
    "            # Use RIGHT FIRE action\n",
    "            action_idx = ALLOWED_ACTIONS.index(11)\n",
    "            return self.env.step(action_idx)\n",
    "        return self.env.step(action)\n",
    "\n",
    "# Function to detect player position based on color\n",
    "def get_agent_position(frame): \n",
    "    \"\"\" Detect player position by color, return (x, y) coordinates. Returns None if not detected. \"\"\"\n",
    "    # Ensure frame is numpy array with correct format\n",
    "    if frame is None:\n",
    "        return None\n",
    "    \n",
    "    # Target color (BGR format)\n",
    "    target_bgr = np.array([194, 64, 82], dtype=np.uint8)\n",
    "\n",
    "    # Tolerance range (adjustable, usually 20~40 works well)\n",
    "    tolerance = 30\n",
    "    lower = np.array([max(0, c - tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "    upper = np.array([min(255, c + tolerance) for c in target_bgr], dtype=np.uint8)\n",
    "\n",
    "    # Generate mask\n",
    "    mask = cv2.inRange(frame, lower, upper)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return None\n",
    "\n",
    "    # Find the largest contour by area\n",
    "    largest = max(contours, key=cv2.contourArea)\n",
    "    M = cv2.moments(largest)\n",
    "\n",
    "    if M[\"m00\"] == 0:\n",
    "        return None\n",
    "\n",
    "    cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "    cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "    return (cx, cy)\n",
    "\n",
    "# Custom video display wrapper for showing actions and agent position\n",
    "class VideoDisplayWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_action = None\n",
    "        self.action_names = {\n",
    "            0: \"\",\n",
    "            1: \"Jump\",\n",
    "            2: \"Up\",\n",
    "            3: \"Right\",\n",
    "            4: \"Left\",\n",
    "            5: \"Down\",\n",
    "            11: \"Jump R\",\n",
    "            12: \"Jump L\"\n",
    "        }\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Record current action\n",
    "        self.current_action = action\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        self.current_action = None\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def render(self):\n",
    "        # Get original rendered frame\n",
    "        frame = self.env.render()\n",
    "        \n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        # Ensure frame is RGB format\n",
    "        if len(frame.shape) == 2:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # 1. Display current action in top right corner\n",
    "        if self.current_action is not None:\n",
    "            action_name = self.action_names.get(self.current_action, f\"ACTION_{self.current_action}\")\n",
    "            cv2.putText(frame, \n",
    "                       action_name, \n",
    "                       (frame.shape[1] - 85, 28), # Top right position\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       0.3,\n",
    "                       (255, 255, 255), # White text\n",
    "                       1, \n",
    "                       cv2.LINE_AA)\n",
    "        \n",
    "        return frame\n",
    "\n",
    "# Custom reward wrapper to adjust rewards based on agent position changes\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, y_static_penalty=0.1, up_success_reward=10,\n",
    "                 up_fail_penalty=0, x_static_penalty=0,\n",
    "                 y_threshold=20, x_threshold=3, \n",
    "                 y_static_frames=30, x_static_frames=30):\n",
    "        super().__init__(env)\n",
    "        # Reward parameters\n",
    "        self.y_static_penalty = y_static_penalty  # Vertical static penalty\n",
    "        self.up_success_reward = up_success_reward  # Successful upward movement reward\n",
    "        self.up_fail_penalty = up_fail_penalty  # Failed upward movement penalty\n",
    "        self.x_static_penalty = x_static_penalty  # Horizontal static penalty\n",
    "        \n",
    "        # Threshold parameters\n",
    "        self.y_threshold = y_threshold  # Vertical movement threshold\n",
    "        self.x_threshold = x_threshold  # Horizontal movement threshold\n",
    "        self.y_static_frames = y_static_frames  # Vertical static frame count\n",
    "        self.x_static_frames = x_static_frames  # Horizontal static frame count\n",
    "        \n",
    "        # State tracking\n",
    "        self.prev_positions = []  # Store past positions [(x, y), ...]\n",
    "        self.y_static_count = 0  # Vertical static counter\n",
    "        self.x_static_count = 0  # Horizontal static counter\n",
    "        self.prev_action = None  # Previous action\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        # Reset state tracking\n",
    "        self.prev_positions = []\n",
    "        self.y_static_count = 0\n",
    "        self.x_static_count = 0\n",
    "        self.prev_action = None\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Record current action\n",
    "        self.prev_action = action\n",
    "        \n",
    "        # Execute environment step\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Extract RGB frame from observation\n",
    "        frame = None\n",
    "        try:\n",
    "            if isinstance(obs, np.ndarray):\n",
    "                if len(obs.shape) == 4:  # (stack, height, width, channel)\n",
    "                    frame = obs[-1]  # Last frame\n",
    "                elif len(obs.shape) == 3:  # (height, width, channel)\n",
    "                    frame = obs\n",
    "                elif len(obs.shape) == 2:  # (height, width)\n",
    "                    frame = obs\n",
    "            elif hasattr(obs, '__getitem__'):\n",
    "                # For FrameStackObservation\n",
    "                try:\n",
    "                    frame = obs[-1]\n",
    "                except:\n",
    "                    try:\n",
    "                        frame = obs[3]  # Assuming 4 frame stack\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # If above attempts fail, try rendering the environment\n",
    "            if frame is None:\n",
    "                try:\n",
    "                    frame = self.env.render()\n",
    "                except:\n",
    "                    pass\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to extract frame from observation: {e}\")\n",
    "            frame = None\n",
    "        \n",
    "        # Detect Agent position\n",
    "        position = None\n",
    "        if frame is not None:\n",
    "            position = get_agent_position(frame)\n",
    "        \n",
    "        # If position detected, update position history and calculate reward adjustment\n",
    "        additional_reward = 0\n",
    "        \n",
    "        if position is not None:\n",
    "            x, y = position\n",
    "            self.prev_positions.append((x, y))\n",
    "            \n",
    "            # Keep history at reasonable size\n",
    "            if len(self.prev_positions) > max(self.y_static_frames, self.x_static_frames):\n",
    "                self.prev_positions.pop(0)\n",
    "            \n",
    "            # Need at least two position records to determine movement\n",
    "            if len(self.prev_positions) >= 2:\n",
    "                prev_x, prev_y = self.prev_positions[-2]\n",
    "                \n",
    "                # 1. Check if vertically static\n",
    "                if abs(y - prev_y) < self.y_threshold:\n",
    "                    self.y_static_count += 1\n",
    "                    if self.y_static_count >= self.y_static_frames:\n",
    "                        # Linearly increasing penalty\n",
    "                        additional_reward -= self.y_static_penalty * (self.y_static_count - self.y_static_frames + 1)\n",
    "                else:\n",
    "                    self.y_static_count = 0\n",
    "                \n",
    "                # 2. Check UP action effect\n",
    "                if self.prev_action == 2:  # Assuming 2 is UP action\n",
    "                    if (prev_y - y) > self.y_threshold:  # Successful upward movement\n",
    "                        additional_reward += self.up_success_reward\n",
    "                    else:  # Failed upward movement\n",
    "                        additional_reward -= self.up_fail_penalty\n",
    "                \n",
    "                # 3. Check if horizontally static\n",
    "                if abs(x - prev_x) < self.x_threshold:\n",
    "                    self.x_static_count += 1\n",
    "                    if self.x_static_count >= self.x_static_frames:\n",
    "                        # Linearly increasing penalty\n",
    "                        additional_reward -= self.x_static_penalty * (self.x_static_count - self.x_static_frames + 1)\n",
    "                else:\n",
    "                    self.x_static_count = 0\n",
    "        \n",
    "        # Apply reward adjustment\n",
    "        adjusted_reward = reward + additional_reward\n",
    "        \n",
    "        return obs, adjusted_reward, terminated, truncated, info\n",
    "\n",
    "# Function to create preprocessed environment\n",
    "def make_env(env_id, idx, capture_video=False, run_name=None):\n",
    "    def thunk():\n",
    "        import ale_py\n",
    "        \n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            # Add video display wrapper\n",
    "            env = VideoDisplayWrapper(env)\n",
    "            env = RecordVideo(\n",
    "                env,\n",
    "                VIDEO_PATH,\n",
    "                episode_trigger=lambda x: True,\n",
    "                name_prefix=f\"{run_name}\"\n",
    "            )\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        \n",
    "        env = ActionRestrictWrapper(env, ALLOWED_ACTIONS)\n",
    "        env = ForceFirstFireWrapper(env)\n",
    "        env = CustomRewardWrapper(env)\n",
    "        env = AtariWrapper(env, terminal_on_life_loss=True, frame_skip=FRAME_SKIP)\n",
    "        env = FrameStackObservation(env, 4)  # Stack 4 frames to capture temporal information\n",
    "            \n",
    "        return env\n",
    "    return thunk\n",
    "\n",
    "# Create parallel environments\n",
    "def make_vec_env(env_id, num_envs, seed=SEED):\n",
    "    env_fns = [make_env(env_id, i) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(env_fns)\n",
    "    envs.seed(seed)\n",
    "    return envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. DQN Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, stack_frames, height, width)\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use prioritized experience replay to improve training efficiency\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Controls the degree of prioritization\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1  # Current frame, used for beta calculation\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    \n",
    "    def beta_by_frame(self, frame_idx):\n",
    "        # beta increases linearly from beta_start to 1.0\n",
    "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # Add new experience\n",
    "        max_prio = np.max(self.priorities) if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(self.Transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.position] = self.Transition(*args)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        # Calculate sampling probabilities\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        beta = self.beta_by_frame(self.frame)\n",
    "        self.frame += 1\n",
    "        \n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = torch.tensor(weights, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # Convert to batch processing format\n",
    "        batch = self.Transition(*zip(*samples))\n",
    "        states = torch.cat(batch.state)\n",
    "        actions = torch.tensor(batch.action, device=device)\n",
    "        rewards = torch.tensor(batch.reward, device=device, dtype=torch.float32)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        dones = torch.tensor(batch.done, device=device, dtype=torch.float32)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        # Update priorities\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions):\n",
    "        self.state_shape = state_shape\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        # Create policy network and target network\n",
    "        self.policy_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net = DQN(state_shape, n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target network doesn't need gradient calculation\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # Create experience replay buffer\n",
    "        self.memory = PrioritizedReplayBuffer(MEMORY_SIZE)\n",
    "        \n",
    "        # Training related parameters\n",
    "        self.steps_done = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "        \n",
    "        # Logger\n",
    "        self.writer = SummaryWriter(LOG_PATH)\n",
    "    \n",
    "    def select_action(self, state, eval_mode=False):\n",
    "        # ε-greedy policy for action selection\n",
    "        sample = random.random()\n",
    "        # In evaluation mode, always choose the best action\n",
    "        if eval_mode:\n",
    "            eps_threshold = 0.05  # Use small epsilon in eval mode for some exploration\n",
    "        else:\n",
    "            # Linear epsilon decay\n",
    "            self.epsilon = max(EPSILON_END, EPSILON_START - self.steps_done / EPSILON_DECAY)\n",
    "            eps_threshold = self.epsilon\n",
    "            \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return 0.0  # Not enough samples in buffer\n",
    "        \n",
    "        # Sample from experience replay buffer\n",
    "        states, actions, rewards, next_states, dones, indices, weights = self.memory.sample(BATCH_SIZE)\n",
    "        \n",
    "        # Calculate current Q values\n",
    "        q_values = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Use Double DQN to calculate next state Q values\n",
    "        # Use policy network to select actions\n",
    "        next_actions = self.policy_net(next_states).max(1)[1].unsqueeze(1)\n",
    "        # Use target network to evaluate actions\n",
    "        next_q_values = self.target_net(next_states).gather(1, next_actions).squeeze(1)\n",
    "        # Set next Q values for terminal states to 0\n",
    "        next_q_values = next_q_values * (1 - dones)\n",
    "        # Calculate target Q values\n",
    "        target_q_values = rewards + GAMMA * next_q_values\n",
    "        \n",
    "        # Calculate loss (TD error)\n",
    "        td_error = torch.abs(q_values - target_q_values).detach().cpu().numpy()\n",
    "        loss = F.smooth_l1_loss(q_values, target_q_values, reduction='none') * weights\n",
    "        loss = loss.mean()\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping to prevent explosion\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities\n",
    "        self.memory.update_priorities(indices, td_error + 1e-5)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'steps_done': self.steps_done,\n",
    "            'epsilon': self.epsilon\n",
    "        }, path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.steps_done = checkpoint['steps_done']\n",
    "        self.epsilon = checkpoint['epsilon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Observation Preprocessing and State Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    # Convert stacked 4 frames to PyTorch input format\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    return tensor / 255.0  # Normalize\n",
    "\n",
    "def preprocess_batch_observation(obs):\n",
    "    # Process batch observation data\n",
    "    frames = np.array(obs).squeeze(-1)\n",
    "    tensor = torch.tensor(frames, dtype=torch.float32, device=device)\n",
    "    return tensor / 255.0  # Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env_id, num_episodes=5, video_prefix=\"evaluation\"):\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # Create a new environment instance for each evaluation episode\n",
    "    for i in range(num_episodes):\n",
    "        # Create new environment instance for each game round\n",
    "        env = make_env(env_id, 0, capture_video=True, run_name=f\"{video_prefix}_episode_{i}\")()\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        obs_tensor = preprocess_observation(obs)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(obs_tensor, eval_mode=True).item()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            \n",
    "            obs = next_obs\n",
    "            obs_tensor = preprocess_observation(obs)\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        env.close()  # Close the environment after each episode\n",
    "\n",
    "        # Modify video filename, remove extra suffix\n",
    "        video_path = os.path.abspath(os.path.join(VIDEO_PATH, f\"{video_prefix}_episode_{i}-episode-0.mp4\"))\n",
    "        if os.path.exists(video_path):\n",
    "            new_video_path = video_path.replace(\"-episode-0.mp4\", \".mp4\")\n",
    "            os.rename(video_path, new_video_path)\n",
    "    \n",
    "    return np.mean(episode_rewards), np.std(episode_rewards), episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, envs, num_frames):\n",
    "    # Initialize environment and progress bar\n",
    "    obs = envs.reset()\n",
    "    obs_tensor = preprocess_batch_observation(obs)\n",
    "    \n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = np.zeros(NUM_ENVS)\n",
    "    episode_length = np.zeros(NUM_ENVS)\n",
    "    \n",
    "    progress_bar = tqdm(range(1, num_frames + 1), desc=\"Training\")\n",
    "    \n",
    "    # Training loop\n",
    "    for frame_idx in progress_bar:\n",
    "        # Select actions\n",
    "        actions = []\n",
    "        for i in range(NUM_ENVS):\n",
    "            action = agent.select_action(obs_tensor[i:i+1])\n",
    "            actions.append(action.item())\n",
    "        \n",
    "        # Execute actions\n",
    "        next_obs, rewards, terminateds, truncateds = envs.step(actions)\n",
    "        \n",
    "        # Process data for each environment\n",
    "        dones = []\n",
    "        for t, tr in zip(terminateds, truncateds):\n",
    "            if isinstance(tr, dict):\n",
    "                done = t or tr.get(\"TimeLimit.truncated\", False)\n",
    "            else:\n",
    "                done = t or tr\n",
    "            dones.append(done)\n",
    "\n",
    "        next_obs_tensor = preprocess_batch_observation(next_obs)\n",
    "        \n",
    "        # Update cumulative rewards and episode length\n",
    "        episode_reward += rewards\n",
    "        episode_length += 1\n",
    "        \n",
    "        # Store data in experience replay buffer\n",
    "        for i in range(NUM_ENVS):\n",
    "            agent.memory.push(\n",
    "                obs_tensor[i:i+1],\n",
    "                actions[i],\n",
    "                rewards[i],\n",
    "                next_obs_tensor[i:i+1],\n",
    "                float(dones[i])\n",
    "            )\n",
    "        \n",
    "        # Update observations\n",
    "        obs = next_obs\n",
    "        obs_tensor = next_obs_tensor\n",
    "        \n",
    "        # Optimize model\n",
    "        loss = agent.optimize_model()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Check for episode completion\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                # Record episode results\n",
    "                agent.writer.add_scalar(\"train/episode_reward\", episode_reward[i], agent.steps_done)\n",
    "                agent.writer.add_scalar(\"train/episode_length\", episode_length[i], agent.steps_done)\n",
    "                all_rewards.append(episode_reward[i])\n",
    "                \n",
    "                # Reset episode statistics\n",
    "                episode_reward[i] = 0\n",
    "                episode_length[i] = 0\n",
    "        \n",
    "        # Update target network\n",
    "        if frame_idx % TARGET_UPDATE == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Record training statistics\n",
    "        if frame_idx % 1000 == 0:\n",
    "            mean_reward = np.mean(all_rewards[-100:]) if all_rewards else 0\n",
    "            mean_loss = np.mean(losses[-100:]) if losses else 0\n",
    "            agent.writer.add_scalar(\"train/epsilon\", agent.epsilon, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/loss\", mean_loss, frame_idx)\n",
    "            agent.writer.add_scalar(\"train/mean_reward_100\", mean_reward, frame_idx)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"avg_reward\": f\"{mean_reward:.2f}\",\n",
    "                \"loss\": f\"{mean_loss:.5f}\",\n",
    "                \"epsilon\": f\"{agent.epsilon:.2f}\"\n",
    "            })\n",
    "        \n",
    "        # Save model\n",
    "        if frame_idx % SAVE_INTERVAL == 0:\n",
    "            save_path = os.path.join(SAVE_PATH, f\"model_{frame_idx}.pt\")\n",
    "            agent.save_model(save_path)\n",
    "            print(f\"\\nFrame {frame_idx}: Model saved to {save_path}\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        if frame_idx % EVAL_INTERVAL == 0:\n",
    "            print(f\"\\nFrame {frame_idx}: Evaluating...\")\n",
    "            eval_reward, eval_std, _ = evaluate(\n",
    "                agent,\n",
    "                ENV_NAME,\n",
    "                num_episodes=EVAL_EPISODES,\n",
    "                video_prefix=f\"eval_{frame_idx}\"\n",
    "            )\n",
    "            agent.writer.add_scalar(\"eval/mean_reward\", eval_reward, frame_idx)\n",
    "            agent.writer.add_scalar(\"eval/reward_std\", eval_std, frame_idx)\n",
    "            print(f\"Evaluation results: Mean reward = {eval_reward:.2f} ± {eval_std:.2f}\")\n",
    "        \n",
    "        # Update agent's step counter\n",
    "        agent.steps_done += 1\n",
    "    \n",
    "    # Save final model after training\n",
    "    final_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "    agent.save_model(final_path)\n",
    "    print(f\"\\nFinal model saved to: {final_path}\")\n",
    "\n",
    "\n",
    "def load_demonstrations(agent, filepath):\n",
    "    \"\"\"Load demonstration trajectories from file and inject them into the agent's replay buffer\"\"\"\n",
    "    import pickle\n",
    "\n",
    "    # Use the same ALLOWED_ACTIONS as in training\n",
    "    ALLOWED_ACTIONS = [0,1,2,3,4,5,11,12]\n",
    "    action_to_index = {a: i for i, a in enumerate(ALLOWED_ACTIONS)}\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        all_trajectories = pickle.load(f)\n",
    "\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "    for traj in all_trajectories:\n",
    "        for s, a, r, ns, d in traj:\n",
    "            if a not in action_to_index:\n",
    "                print(f\"Skipping illegal action: {a}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            action_index = action_to_index[a]  # Map to 0~7\n",
    "\n",
    "            agent.memory.push(\n",
    "                s.to(device),\n",
    "                action_index,\n",
    "                r.to(device),\n",
    "                ns.to(device),\n",
    "                d.to(device)\n",
    "            )\n",
    "            count += 1\n",
    "\n",
    "    print(f\"\\nDemonstrations imported, {count} transitions added to replay buffer. Skipped {skipped} illegal actions.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Main Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation shape: (4, 84, 84)\n",
      "Action space size: 8\n"
     ]
    }
   ],
   "source": [
    "# Create parallel environments\n",
    "envs = make_vec_env(ENV_NAME, NUM_ENVS)\n",
    "\n",
    "# Get environment information\n",
    "obs_shape = (4, 84, 84)  # 4 stacked frames, each 84x84\n",
    "n_actions = envs.action_space.n\n",
    "\n",
    "print(f\"\\nObservation shape: {obs_shape}\")\n",
    "print(f\"Action space size: {n_actions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Demonstrations imported, 2448 transitions added to replay buffer. Skipped 0 illegal actions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DQN agent\n",
    "agent = DQNAgent(obs_shape, n_actions)\n",
    "\n",
    "# Load demonstration trajectories if available\n",
    "if DEMO_PATH and os.path.exists(DEMO_PATH):\n",
    "    Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "    load_demonstrations(agent, DEMO_PATH)\n",
    "else:\n",
    "    print(f\"Demonstration file not found at {DEMO_PATH}, skipping demonstration loading.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start training\n",
    "# print(\"\\nTraining started...\\n\")\n",
    "\n",
    "# # Enable cudnn benchmark mode to improve training speed\n",
    "# torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# train(agent, envs, NUM_FRAMES)\n",
    "\n",
    "# # Close environments\n",
    "# envs.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load and Test Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record_video(model_path, env_id, num_episodes=5):\n",
    "    # Create agent and load model\n",
    "    obs_shape = (4, 84, 84)  # 4 stacked frames, each 84x84\n",
    "    env = make_env(env_id, 0)()\n",
    "    n_actions = env.action_space.n\n",
    "    agent = DQNAgent(obs_shape, n_actions)\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    # Test trained agent\n",
    "    mean, std, rewards = evaluate(agent, env_id, num_episodes=num_episodes, video_prefix=\"final_test\")\n",
    "\n",
    "    for i, reward in enumerate(rewards):\n",
    "        print(f\"Episode {i+1}: Reward = {reward}\")\n",
    "        \n",
    "    print(f\"\\nAverage reward: {mean:.2f} ± {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Reward = -34.0\n",
      "Episode 2: Reward = -34.0\n",
      "Episode 3: Reward = -33.0\n",
      "Episode 4: Reward = -32.0\n",
      "Episode 5: Reward = -38.0\n",
      "\n",
      "Average reward: -34.20 ± 2.04\n"
     ]
    }
   ],
   "source": [
    "# Load and test the final model\n",
    "# model_path = os.path.join(SAVE_PATH, \"model_final.pt\")\n",
    "model_path = \"./model_final.pt\"\n",
    "play_and_record_video(model_path, ENV_NAME, num_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualizing Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To visualize training results, run the following command in the terminal:\n",
      "tensorboard --logdir=./logs/donkey_kong_20250410_140343\n"
     ]
    }
   ],
   "source": [
    "# Use TensorBoard to visualize training results\n",
    "print(f\"To visualize training results, run the following command in the terminal:\")\n",
    "print(f\"tensorboard --logdir={LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Traceback (most recent call last):\n",
       "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m198\u001b[0m, in \u001b[35m_run_module_as_main\u001b[0m\n",
       "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35m_run_code\u001b[0m\n",
       "  File \u001b[35m\"C:\\Users\\sreeh\\miniconda3\\envs\\donkey2\\Scripts\\tensorboard.exe\\__main__.py\"\u001b[0m, line \u001b[35m4\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
       "    from tensorboard.main import run_main\n",
       "  File \u001b[35m\"C:\\Users\\sreeh\\miniconda3\\envs\\donkey2\\Lib\\site-packages\\tensorboard\\main.py\"\u001b[0m, line \u001b[35m27\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
       "    from tensorboard import default\n",
       "  File \u001b[35m\"C:\\Users\\sreeh\\miniconda3\\envs\\donkey2\\Lib\\site-packages\\tensorboard\\default.py\"\u001b[0m, line \u001b[35m40\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
       "    from tensorboard.plugins.image import images_plugin\n",
       "  File \u001b[35m\"C:\\Users\\sreeh\\miniconda3\\envs\\donkey2\\Lib\\site-packages\\tensorboard\\plugins\\image\\images_plugin.py\"\u001b[0m, line \u001b[35m18\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
       "    import imghdr\n",
       "\u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'imghdr'\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and plotting TensorBoard metrics...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdIAAAPdCAYAAACOcJpIAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAATtpJREFUeJzs3X9w1eWd6PFPEsiJTk3EZQk/NpbVrrWtChYkG63jdSfbzOjQ8sdOWe0Ay/hjbaljyd2t4A9Sa0tYqw5zC5aR6rV/1IXqqNMpTFybLdOxZi9TIDN2BR0LFtZpomyXhMU2keR7/+Aab0p4DiflnAB5vWbOH3z7fHOe03mkH989Oacsy7IsAAAAAACAEZWP9QYAAAAAAOB0JqQDAAAAAECCkA4AAAAAAAlCOgAAAAAAJAjpAAAAAACQIKQDAAAAAECCkA4AAAAAAAlCOgAAAAAAJAjpAAAAAACQIKQDAAAAAEBCwSH9Zz/7WcyfPz+mT58eZWVl8cILL+S9Z9u2bfHpT386crlcfOxjH4unnnpqFFsFAAA+YC4HAIDSKTikHzlyJGbNmhXr168/qfX79u2LG2+8Ma6//vro7OyMr371q3HrrbfGiy++WPBmAQCAY8zlAABQOmVZlmWjvrmsLJ5//vlYsGDBCdfcfffdsWXLlvjlL385dO1v//Zv49ChQ9HW1jbapwYAAP4fczkAABTXhGI/QUdHRzQ2Ng671tTUFF/96ldPeE9fX1/09fUN/XlwcDB++9vfxp/8yZ9EWVlZsbYKAACjlmVZHD58OKZPnx7l5affVxGZywEAGA+KNZcXPaR3dXVFbW3tsGu1tbXR29sbv/vd7+Kcc8457p7W1tZ44IEHir01AAA45Q4cOBB/9md/NtbbOI65HACA8eRUz+VFD+mjsXLlymhubh76c09PT1x44YVx4MCBqK6uHsOdAQDAyHp7e6Ouri7OO++8sd7KKWMuBwDgTFOsubzoIX3q1KnR3d097Fp3d3dUV1eP+K6XiIhcLhe5XO6469XV1QZ2AABOa6frR56YywEAGE9O9Vxe9A9vbGhoiPb29mHXXnrppWhoaCj2UwMAAP+PuRwAAEav4JD+3//939HZ2RmdnZ0REbFv377o7OyM/fv3R8SxX/9cvHjx0Po77rgj9u7dG1/72tdiz5498dhjj8UPf/jDWL58+al5BQAAMA6ZywEAoHQKDum/+MUv4sorr4wrr7wyIiKam5vjyiuvjFWrVkVExG9+85uh4T0i4s///M9jy5Yt8dJLL8WsWbPikUceie9973vR1NR0il4CAACMP+ZyAAAonbIsy7Kx3kQ+vb29UVNTEz09PT6LEQCA09J4mFnHw2sEAODMVqyZteifkQ4AAAAAAGcyIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBhVSF+/fn3MnDkzqqqqor6+PrZv355cv3bt2vj4xz8e55xzTtTV1cXy5cvj97///ag2DAAAHGMuBwCA0ig4pG/evDmam5ujpaUldu7cGbNmzYqmpqZ45513Rlz/9NNPx4oVK6KlpSV2794dTzzxRGzevDnuueeeP3rzAAAwXpnLAQCgdMqyLMsKuaG+vj6uuuqqWLduXUREDA4ORl1dXdx5552xYsWK49Z/5Stfid27d0d7e/vQtf/5P/9n/J//83/i5ZdfHvE5+vr6oq+vb+jPvb29UVdXFz09PVFdXV3IdgEAoCR6e3ujpqamZDOruRwAAI5XrLm8oHek9/f3x44dO6KxsfHDH1BeHo2NjdHR0THiPVdffXXs2LFj6NdM9+7dG1u3bo0bbrjhhM/T2toaNTU1Q4+6urpCtgkAAGc1czkAAJTWhEIWHzx4MAYGBqK2tnbY9dra2tizZ8+I99x8881x8ODB+MxnPhNZlsXRo0fjjjvuSP4K6cqVK6O5uXnozx+88wUAADCXAwBAqY3qy0YLsW3btli9enU89thjsXPnznjuuediy5Yt8eCDD57wnlwuF9XV1cMeAADA6JnLAQBg9Ap6R/rkyZOjoqIiuru7h13v7u6OqVOnjnjP/fffH4sWLYpbb701IiIuv/zyOHLkSNx+++1x7733Rnl50Vs+AACcVczlAABQWgVNy5WVlTFnzpxhX1A0ODgY7e3t0dDQMOI977333nFDeUVFRUREFPg9pwAAQJjLAQCg1Ap6R3pERHNzcyxZsiTmzp0b8+bNi7Vr18aRI0di6dKlERGxePHimDFjRrS2tkZExPz58+PRRx+NK6+8Murr6+PNN9+M+++/P+bPnz80uAMAAIUxlwMAQOkUHNIXLlwY7777bqxatSq6urpi9uzZ0dbWNvRFR/v37x/2Tpf77rsvysrK4r777ou33347/vRP/zTmz58f3/rWt07dqwAAgHHGXA4AAKVTlp0Bv8fZ29sbNTU10dPT4wuOAAA4LY2HmXU8vEYAAM5sxZpZfaMQAAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAwqpC+fv36mDlzZlRVVUV9fX1s3749uf7QoUOxbNmymDZtWuRyubjkkkti69ato9owAABwjLkcAABKY0KhN2zevDmam5tjw4YNUV9fH2vXro2mpqZ4/fXXY8qUKcet7+/vj7/+67+OKVOmxLPPPhszZsyIX//613H++eefiv0DAMC4ZC4HAIDSKcuyLCvkhvr6+rjqqqti3bp1ERExODgYdXV1ceedd8aKFSuOW79hw4b49re/HXv27ImJEyeOapO9vb1RU1MTPT09UV1dPaqfAQAAxVTqmdVcDgAAxyvWzFrQR7v09/fHjh07orGx8cMfUF4ejY2N0dHRMeI9P/rRj6KhoSGWLVsWtbW1cdlll8Xq1atjYGDghM/T19cXvb29wx4AAMAx5nIAACitgkL6wYMHY2BgIGpra4ddr62tja6urhHv2bt3bzz77LMxMDAQW7dujfvvvz8eeeSR+OY3v3nC52ltbY2ampqhR11dXSHbBACAs5q5HAAASmtUXzZaiMHBwZgyZUo8/vjjMWfOnFi4cGHce++9sWHDhhPes3Llyujp6Rl6HDhwoNjbBACAs5q5HAAARq+gLxudPHlyVFRURHd397Dr3d3dMXXq1BHvmTZtWkycODEqKiqGrn3iE5+Irq6u6O/vj8rKyuPuyeVykcvlCtkaAACMG+ZyAAAorYLekV5ZWRlz5syJ9vb2oWuDg4PR3t4eDQ0NI95zzTXXxJtvvhmDg4ND1954442YNm3aiMM6AACQZi4HAIDSKvijXZqbm2Pjxo3x/e9/P3bv3h1f+tKX4siRI7F06dKIiFi8eHGsXLlyaP2XvvSl+O1vfxt33XVXvPHGG7Fly5ZYvXp1LFu27NS9CgAAGGfM5QAAUDoFfbRLRMTChQvj3XffjVWrVkVXV1fMnj072trahr7oaP/+/VFe/mGfr6urixdffDGWL18eV1xxRcyYMSPuuuuuuPvuu0/dqwAAgHHGXA4AAKVTlmVZNtabyKe3tzdqamqip6cnqqurx3o7AABwnPEws46H1wgAwJmtWDNrwR/tAgAAAAAA44mQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkjCqkr1+/PmbOnBlVVVVRX18f27dvP6n7Nm3aFGVlZbFgwYLRPC0AAPAHzOYAAFB8BYf0zZs3R3Nzc7S0tMTOnTtj1qxZ0dTUFO+8807yvrfeeiv+4R/+Ia699tpRbxYAAPiQ2RwAAEqj4JD+6KOPxm233RZLly6NT37yk7Fhw4Y499xz48knnzzhPQMDA/HFL34xHnjggbjooovyPkdfX1/09vYOewAAAMMVezY3lwMAwDEFhfT+/v7YsWNHNDY2fvgDysujsbExOjo6TnjfN77xjZgyZUrccsstJ/U8ra2tUVNTM/Soq6srZJsAAHDWK8Vsbi4HAIBjCgrpBw8ejIGBgaitrR12vba2Nrq6uka85+WXX44nnngiNm7ceNLPs3Llyujp6Rl6HDhwoJBtAgDAWa8Us7m5HAAAjplQzB9++PDhWLRoUWzcuDEmT5580vflcrnI5XJF3BkAAIwvo5nNzeUAAHBMQSF98uTJUVFREd3d3cOud3d3x9SpU49b/6tf/SreeuutmD9//tC1wcHBY088YUK8/vrrcfHFF49m3wAAMK6ZzQEAoHQK+miXysrKmDNnTrS3tw9dGxwcjPb29mhoaDhu/aWXXhqvvvpqdHZ2Dj0+97nPxfXXXx+dnZ0+YxEAAEbJbA4AAKVT8Ee7NDc3x5IlS2Lu3Lkxb968WLt2bRw5ciSWLl0aERGLFy+OGTNmRGtra1RVVcVll1027P7zzz8/IuK46wAAQGHM5gAAUBoFh/SFCxfGu+++G6tWrYqurq6YPXt2tLW1DX3J0f79+6O8vKA3ugMAAKNgNgcAgNIoy7IsG+tN5NPb2xs1NTXR09MT1dXVY70dAAA4zniYWcfDawQA4MxWrJnV21MAAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEkYV0tevXx8zZ86MqqqqqK+vj+3bt59w7caNG+Paa6+NSZMmxaRJk6KxsTG5HgAAOHlmcwAAKL6CQ/rmzZujubk5WlpaYufOnTFr1qxoamqKd955Z8T127Zti5tuuil++tOfRkdHR9TV1cVnP/vZePvtt//ozQMAwHhmNgcAgNIoy7IsK+SG+vr6uOqqq2LdunURETE4OBh1dXVx5513xooVK/LePzAwEJMmTYp169bF4sWLR1zT19cXfX19Q3/u7e2Nurq66Onpierq6kK2CwAAJdHb2xs1NTUlnVmLPZubywEAONMUay4v6B3p/f39sWPHjmhsbPzwB5SXR2NjY3R0dJzUz3jvvffi/fffjwsuuOCEa1pbW6OmpmboUVdXV8g2AQDgrFeK2dxcDgAAxxQU0g8ePBgDAwNRW1s77HptbW10dXWd1M+4++67Y/r06cMG/j+0cuXK6OnpGXocOHCgkG0CAMBZrxSzubkcAACOmVDKJ1uzZk1s2rQptm3bFlVVVSdcl8vlIpfLlXBnAAAwvpzMbG4uBwCAYwoK6ZMnT46Kioro7u4edr27uzumTp2avPfhhx+ONWvWxE9+8pO44oorCt8pAAAwxGwOAAClU9BHu1RWVsacOXOivb196Nrg4GC0t7dHQ0PDCe976KGH4sEHH4y2traYO3fu6HcLAABEhNkcAABKqeCPdmlubo4lS5bE3LlzY968ebF27do4cuRILF26NCIiFi9eHDNmzIjW1taIiPinf/qnWLVqVTz99NMxc+bMoc9r/MhHPhIf+chHTuFLAQCA8cVsDgAApVFwSF+4cGG8++67sWrVqujq6orZs2dHW1vb0Jcc7d+/P8rLP3yj+3e/+93o7++Pv/mbvxn2c1paWuLrX//6H7d7AAAYx8zmAABQGmVZlmVjvYl8ent7o6amJnp6eqK6unqstwMAAMcZDzPreHiNAACc2Yo1sxb0GekAAAAAADDeCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAwqpC+fv36mDlzZlRVVUV9fX1s3749uf6ZZ56JSy+9NKqqquLyyy+PrVu3jmqzAADAcGZzAAAovoJD+ubNm6O5uTlaWlpi586dMWvWrGhqaop33nlnxPWvvPJK3HTTTXHLLbfErl27YsGCBbFgwYL45S9/+UdvHgAAxjOzOQAAlEZZlmVZITfU19fHVVddFevWrYuIiMHBwairq4s777wzVqxYcdz6hQsXxpEjR+LHP/7x0LW//Mu/jNmzZ8eGDRtO6jl7e3ujpqYmenp6orq6upDtAgBASYzFzFrq2dxcDgDA6a5YM+uEQhb39/fHjh07YuXKlUPXysvLo7GxMTo6Oka8p6OjI5qbm4dda2pqihdeeOGEz9PX1xd9fX1Df+7p6YmIY/8lAADA6eiDWbXA96mMWilmc3M5AABnmmLN5QWF9IMHD8bAwEDU1tYOu15bWxt79uwZ8Z6urq4R13d1dZ3weVpbW+OBBx447npdXV0h2wUAgJL7z//8z6ipqSn685RiNjeXAwBwpjrVc3lBIb1UVq5cOeydMocOHYqPfvSjsX///pL8Swlnnt7e3qirq4sDBw74NWNG5IyQjzNCPs4I+fT09MSFF14YF1xwwVhv5ZQxl1Mof1eSjzNCPs4I+Tgj5FOsubygkD558uSoqKiI7u7uYde7u7tj6tSpI94zderUgtZHRORyucjlcsddr6mp8Q8ISdXV1c4ISc4I+Tgj5OOMkE95eXlJnqcUs7m5nNHydyX5OCPk44yQjzNCPqd6Li/op1VWVsacOXOivb196Nrg4GC0t7dHQ0PDiPc0NDQMWx8R8dJLL51wPQAAkJ/ZHAAASqfgj3Zpbm6OJUuWxNy5c2PevHmxdu3aOHLkSCxdujQiIhYvXhwzZsyI1tbWiIi466674rrrrotHHnkkbrzxxti0aVP84he/iMcff/zUvhIAABhnzOYAAFAaBYf0hQsXxrvvvhurVq2Krq6umD17drS1tQ19adH+/fuHvW3+6quvjqeffjruu+++uOeee+Iv/uIv4oUXXojLLrvspJ8zl8tFS0vLiL9WChHOCPk5I+TjjJCPM0I+Y3FGSj2b++eAfJwR8nFGyMcZIR9nhHyKdUbKsizLTulPBAAAAACAs0hpvgkJAAAAAADOUEI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQMJpE9LXr18fM2fOjKqqqqivr4/t27cn1z/zzDNx6aWXRlVVVVx++eWxdevWEu2UsVLIGdm4cWNce+21MWnSpJg0aVI0NjbmPVOc+Qr9e+QDmzZtirKysliwYEFxN8iYKvR8HDp0KJYtWxbTpk2LXC4Xl1xyif+tOcsVekbWrl0bH//4x+Occ86Jurq6WL58efz+978v0W4ptZ/97Gcxf/78mD59epSVlcULL7yQ955t27bFpz/96cjlcvGxj30snnrqqaLv81Qwl5OPuZx8zOXkYzYnH7M5KWM2m2engU2bNmWVlZXZk08+mf37v/97dtttt2Xnn39+1t3dPeL6n//851lFRUX20EMPZa+99lp23333ZRMnTsxeffXVEu+cUin0jNx8883Z+vXrs127dmW7d+/O/u7v/i6rqanJ/uM//qPEO6dUCj0jH9i3b182Y8aM7Nprr80+//nPl2azlFyh56Ovry+bO3dudsMNN2Qvv/xytm/fvmzbtm1ZZ2dniXdOqRR6Rn7wgx9kuVwu+8EPfpDt27cve/HFF7Np06Zly5cvL/HOKZWtW7dm9957b/bcc89lEZE9//zzyfV79+7Nzj333Ky5uTl77bXXsu985ztZRUVF1tbWVpoNj5K5nHzM5eRjLicfszn5mM3JZ6xm89MipM+bNy9btmzZ0J8HBgay6dOnZ62trSOu/8IXvpDdeOONw67V19dnf//3f1/UfTJ2Cj0jf+jo0aPZeeedl33/+98v1hYZY6M5I0ePHs2uvvrq7Hvf+162ZMkSA/tZrNDz8d3vfje76KKLsv7+/lJtkTFW6BlZtmxZ9ld/9VfDrjU3N2fXXHNNUffJ6eFkhvWvfe1r2ac+9alh1xYuXJg1NTUVcWd/PHM5+ZjLycdcTj5mc/Ixm1OIUs7mY/7RLv39/bFjx45obGwculZeXh6NjY3R0dEx4j0dHR3D1kdENDU1nXA9Z7bRnJE/9N5778X7778fF1xwQbG2yRga7Rn5xje+EVOmTIlbbrmlFNtkjIzmfPzoRz+KhoaGWLZsWdTW1sZll10Wq1evjoGBgVJtmxIazRm5+uqrY8eOHUO/Yrp3797YunVr3HDDDSXZM6e/M3FeNZeTj7mcfMzl5GM2Jx+zOcVwqmbWCadyU6Nx8ODBGBgYiNra2mHXa2trY8+ePSPe09XVNeL6rq6uou2TsTOaM/KH7r777pg+ffpx/9BwdhjNGXn55ZfjiSeeiM7OzhLskLE0mvOxd+/e+Nd//df44he/GFu3bo0333wzvvzlL8f7778fLS0tpdg2JTSaM3LzzTfHwYMH4zOf+UxkWRZHjx6NO+64I+65555SbJkzwInm1d7e3vjd734X55xzzhjt7MTM5eRjLicfczn5mM3Jx2xOMZyq2XzM35EOxbZmzZrYtGlTPP/881FVVTXW2+E0cPjw4Vi0aFFs3LgxJk+ePNbb4TQ0ODgYU6ZMiccffzzmzJkTCxcujHvvvTc2bNgw1lvjNLFt27ZYvXp1PPbYY7Fz58547rnnYsuWLfHggw+O9dYATlvmcv6QuZyTYTYnH7M5pTLm70ifPHlyVFRURHd397Dr3d3dMXXq1BHvmTp1akHrObON5ox84OGHH441a9bET37yk7jiiiuKuU3GUKFn5Fe/+lW89dZbMX/+/KFrg4ODERExYcKEeP311+Piiy8u7qYpmdH8HTJt2rSYOHFiVFRUDF37xCc+EV1dXdHf3x+VlZVF3TOlNZozcv/998eiRYvi1ltvjYiIyy+/PI4cORK333573HvvvVFe7r0K492J5tXq6urT8t3oEeZy8jOXk4+5nHzM5uRjNqcYTtVsPuYnqbKyMubMmRPt7e1D1wYHB6O9vT0aGhpGvKehoWHY+oiIl1566YTrObON5oxERDz00EPx4IMPRltbW8ydO7cUW2WMFHpGLr300nj11Vejs7Nz6PG5z30urr/++ujs7Iy6urpSbp8iG83fIddcc028+eabQ/8iFxHxxhtvxLRp0wzqZ6HRnJH33nvvuIH8g3+5O/Z9N4x3Z+K8ai4nH3M5+ZjLycdsTj5mc4rhlM2sBX01aZFs2rQpy+Vy2VNPPZW99tpr2e23356df/75WVdXV5ZlWbZo0aJsxYoVQ+t//vOfZxMmTMgefvjhbPfu3VlLS0s2ceLE7NVXXx2rl0CRFXpG1qxZk1VWVmbPPvts9pvf/Gbocfjw4bF6CRRZoWfkDy1ZsiT7/Oc/X6LdUmqFno/9+/dn5513XvaVr3wle/3117Mf//jH2ZQpU7JvfvObY/USKLJCz0hLS0t23nnnZf/8z/+c7d27N/uXf/mX7OKLL86+8IUvjNVLoMgOHz6c7dq1K9u1a1cWEdmjjz6a7dq1K/v1r3+dZVmWrVixIlu0aNHQ+r1792bnnntu9o//+I/Z7t27s/Xr12cVFRVZW1vbWL2Ek2IuJx9zOfmYy8nHbE4+ZnPyGavZ/LQI6VmWZd/5zneyCy+8MKusrMzmzZuX/du//dvQf3bddddlS5YsGbb+hz/8YXbJJZdklZWV2ac+9alsy5YtJd4xpVbIGfnoRz+aRcRxj5aWltJvnJIp9O+R/5+B/exX6Pl45ZVXsvr6+iyXy2UXXXRR9q1vfSs7evRoiXdNKRVyRt5///3s61//enbxxRdnVVVVWV1dXfblL385+6//+q/Sb5yS+OlPfzribPHBuViyZEl23XXXHXfP7Nmzs8rKyuyiiy7K/vf//t8l3/domMvJx1xOPuZy8jGbk4/ZnJSxms3LsszvOAAAAAAAwImM+WekAwAAAADA6UxIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAASCg7pP/vZz2L+/Pkxffr0KCsrixdeeCHvPdu2bYtPf/rTkcvl4mMf+1g89dRTo9gqAADwAXM5AACUTsEh/ciRIzFr1qxYv379Sa3ft29f3HjjjXH99ddHZ2dnfPWrX41bb701XnzxxYI3CwAAHGMuBwCA0inLsiwb9c1lZfH888/HggULTrjm7rvvji1btsQvf/nLoWt/+7d/G4cOHYq2trYR7+nr64u+vr6hPw8ODsZvf/vb+JM/+ZMoKysb7XYBAKBosiyLw4cPx/Tp06O8vLSfoGguBwCAY4o1l084ZT/pBDo6OqKxsXHYtaampvjqV796wntaW1vjgQceKPLOAADg1Dtw4ED82Z/92Vhv4zjmcgAAxpNTPZcXPaR3dXVFbW3tsGu1tbXR29sbv/vd7+Kcc8457p6VK1dGc3Pz0J97enriwgsvjAMHDkR1dXWxtwwAAAXr7e2Nurq6OO+888Z6KyMylwMAMB4Uay4vekgfjVwuF7lc7rjr1dXVBnYAAE5rZ9NHnpjLAQA4U53qubzoH944derU6O7uHnatu7s7qqurR3zXCwAAcOqZywEAYPSKHtIbGhqivb192LWXXnopGhoaiv3UAADA/2MuBwCA0Ss4pP/3f/93dHZ2RmdnZ0RE7Nu3Lzo7O2P//v0RcexzFBcvXjy0/o477oi9e/fG1772tdizZ0889thj8cMf/jCWL19+al4BAACMQ+ZyAAAonYJD+i9+8Yu48sor48orr4yIiObm5rjyyitj1apVERHxm9/8Zmh4j4j48z//89iyZUu89NJLMWvWrHjkkUfie9/7XjQ1NZ2ilwAAAOOPuRwAAEqnLMuybKw3kU9vb2/U1NRET0+PLzUCAOC0NB5m1vHwGgEAOLMVa2Yt+mekAwAAAADAmUxIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACABCEdAAAAAAAShHQAAAAAAEgQ0gEAAAAAIEFIBwAAAACAhFGF9PXr18fMmTOjqqoq6uvrY/v27cn1a9eujY9//ONxzjnnRF1dXSxfvjx+//vfj2rDAADAMeZyAAAojYJD+ubNm6O5uTlaWlpi586dMWvWrGhqaop33nlnxPVPP/10rFixIlpaWmL37t3xxBNPxObNm+Oee+75ozcPAADjlbkcAABKp+CQ/uijj8Ztt90WS5cujU9+8pOxYcOGOPfcc+PJJ58ccf0rr7wS11xzTdx8880xc+bM+OxnPxs33XRT3nfLAAAAJ2YuBwCA0ikopPf398eOHTuisbHxwx9QXh6NjY3R0dEx4j1XX3117NixY2hA37t3b2zdujVuuOGGEz5PX19f9Pb2DnsAAADHmMsBAKC0JhSy+ODBgzEwMBC1tbXDrtfW1saePXtGvOfmm2+OgwcPxmc+85nIsiyOHj0ad9xxR/JXSFtbW+OBBx4oZGsAADBumMsBAKC0RvVlo4XYtm1brF69Oh577LHYuXNnPPfcc7Fly5Z48MEHT3jPypUro6enZ+hx4MCBYm8TAADOauZyAAAYvYLekT558uSoqKiI7u7uYde7u7tj6tSpI95z//33x6JFi+LWW2+NiIjLL788jhw5Erfffnvce++9UV5+fMvP5XKRy+UK2RoAAIwb5nIAACitgt6RXllZGXPmzIn29vaha4ODg9He3h4NDQ0j3vPee+8dN5RXVFRERESWZYXuFwAAxj1zOQAAlFZB70iPiGhubo4lS5bE3LlzY968ebF27do4cuRILF26NCIiFi9eHDNmzIjW1taIiJg/f348+uijceWVV0Z9fX28+eabcf/998f8+fOHBncAAKAw5nIAACidgkP6woUL4913341Vq1ZFV1dXzJ49O9ra2oa+6Gj//v3D3uly3333RVlZWdx3333x9ttvx5/+6Z/G/Pnz41vf+tapexUAADDOmMsBAKB0yrIz4Pc4e3t7o6amJnp6eqK6unqstwMAAMcZDzPreHiNAACc2Yo1sxb0GekAAAAAADDeCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQMKoQvr69etj5syZUVVVFfX19bF9+/bk+kOHDsWyZcti2rRpkcvl4pJLLomtW7eOasMAAMAx5nIAACiNCYXesHnz5mhubo4NGzZEfX19rF27NpqamuL111+PKVOmHLe+v78//vqv/zqmTJkSzz77bMyYMSN+/etfx/nnn38q9g8AAOOSuRwAAEqnLMuyrJAb6uvr46qrrop169ZFRMTg4GDU1dXFnXfeGStWrDhu/YYNG+Lb3/527NmzJyZOnHhSz9HX1xd9fX1Df+7t7Y26urro6emJ6urqQrYLAAAl0dvbGzU1NSWbWc3lAABwvGLN5QV9tEt/f3/s2LEjGhsbP/wB5eXR2NgYHR0dI97zox/9KBoaGmLZsmVRW1sbl112WaxevToGBgZO+Dytra1RU1Mz9KirqytkmwAAcFYzlwMAQGkVFNIPHjwYAwMDUVtbO+x6bW1tdHV1jXjP3r1749lnn42BgYHYunVr3H///fHII4/EN7/5zRM+z8qVK6Onp2foceDAgUK2CQAAZzVzOQAAlFbBn5FeqMHBwZgyZUo8/vjjUVFREXPmzIm33347vv3tb0dLS8uI9+RyucjlcsXeGgAAjBvmcgAAGL2CQvrkyZOjoqIiuru7h13v7u6OqVOnjnjPtGnTYuLEiVFRUTF07ROf+ER0dXVFf39/VFZWjmLbAAAwfpnLAQCgtAr6aJfKysqYM2dOtLe3D10bHByM9vb2aGhoGPGea665Jt58880YHBwcuvbGG2/EtGnTDOsAADAK5nIAACitgkJ6RERzc3Ns3Lgxvv/978fu3bvjS1/6Uhw5ciSWLl0aERGLFy+OlStXDq3/0pe+FL/97W/jrrvuijfeeCO2bNkSq1evjmXLlp26VwEAAOOMuRwAAEqn4M9IX7hwYbz77ruxatWq6OrqitmzZ0dbW9vQFx3t378/yss/7PN1dXXx4osvxvLly+OKK66IGTNmxF133RV33333qXsVAAAwzpjLAQCgdMqyLMvGehP59Pb2Rk1NTfT09ER1dfVYbwcAAI4zHmbW8fAaAQA4sxVrZi34o10AAAAAAGA8EdIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBBSAcAAAAAgAQhHQAAAAAAEoR0AAAAAABIENIBAAAAACBhVCF9/fr1MXPmzKiqqor6+vrYvn37Sd23adOmKCsriwULFozmaQEAgD9gNgcAgOIrOKRv3rw5mpubo6WlJXbu3BmzZs2KpqameOedd5L3vfXWW/EP//APce211456swAAwIfM5gAAUBoFh/RHH300brvttli6dGl88pOfjA0bNsS5554bTz755AnvGRgYiC9+8YvxwAMPxEUXXfRHbRgAADjGbA4AAKVRUEjv7++PHTt2RGNj44c/oLw8Ghsbo6Oj44T3feMb34gpU6bELbfcclLP09fXF729vcMeAADAh0oxm5vLAQDgmIJC+sGDB2NgYCBqa2uHXa+trY2urq4R73n55ZfjiSeeiI0bN57087S2tkZNTc3Qo66urpBtAgDAWa8Us7m5HAAAjhnVl42erMOHD8eiRYti48aNMXny5JO+b+XKldHT0zP0OHDgQBF3CQAAZ7/RzObmcgAAOGZCIYsnT54cFRUV0d3dPex6d3d3TJ069bj1v/rVr+Ktt96K+fPnD10bHBw89sQTJsTrr78eF1988XH35XK5yOVyhWwNAADGlVLM5uZyAAA4pqB3pFdWVsacOXOivb196Nrg4GC0t7dHQ0PDcesvvfTSePXVV6Ozs3Po8bnPfS6uv/766Ozs9KuhAAAwSmZzAAAonYLekR4R0dzcHEuWLIm5c+fGvHnzYu3atXHkyJFYunRpREQsXrw4ZsyYEa2trVFVVRWXXXbZsPvPP//8iIjjrgMAAIUxmwMAQGkUHNIXLlwY7777bqxatSq6urpi9uzZ0dbWNvQlR/v374/y8qJ+9DoAABBmcwAAKJWyLMuysd5EPr29vVFTUxM9PT1RXV091tsBAIDjjIeZdTy8RgAAzmzFmlm9PQUAAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgQUgHAAAAAIAEIR0AAAAAABKEdAAAAAAASBDSAQAAAAAgYVQhff369TFz5syoqqqK+vr62L59+wnXbty4Ma699tqYNGlSTJo0KRobG5PrAQCAk2c2BwCA4is4pG/evDmam5ujpaUldu7cGbNmzYqmpqZ45513Rly/bdu2uOmmm+KnP/1pdHR0RF1dXXz2s5+Nt99++4/ePAAAjGdmcwAAKI2yLMuyQm6or6+Pq666KtatWxcREYODg1FXVxd33nlnrFixIu/9AwMDMWnSpFi3bl0sXrz4pJ6zt7c3ampqoqenJ6qrqwvZLgAAlMRYzKylns3N5QAAnO6KNbMW9I70/v7+2LFjRzQ2Nn74A8rLo7GxMTo6Ok7qZ7z33nvx/vvvxwUXXHDCNX19fdHb2zvsAQAAfKgUs7m5HAAAjikopB88eDAGBgaitrZ22PXa2tro6uo6qZ9x9913x/Tp04cN/H+otbU1ampqhh51dXWFbBMAAM56pZjNzeUAAHDMqL5sdLTWrFkTmzZtiueffz6qqqpOuG7lypXR09Mz9Dhw4EAJdwkAAGe/k5nNzeUAAHDMhEIWT548OSoqKqK7u3vY9e7u7pg6dWry3ocffjjWrFkTP/nJT+KKK65Irs3lcpHL5QrZGgAAjCulmM3N5QAAcExB70ivrKyMOXPmRHt7+9C1wcHBaG9vj4aGhhPe99BDD8WDDz4YbW1tMXfu3NHvFgAAiAizOQAAlFJB70iPiGhubo4lS5bE3LlzY968ebF27do4cuRILF26NCIiFi9eHDNmzIjW1taIiPinf/qnWLVqVTz99NMxc+bMoc9r/MhHPhIf+chHTuFLAQCA8cVsDgAApVFwSF+4cGG8++67sWrVqujq6orZs2dHW1vb0Jcc7d+/P8rLP3yj+3e/+93o7++Pv/mbvxn2c1paWuLrX//6H7d7AAAYx8zmAABQGmVZlmVjvYl8ent7o6amJnp6eqK6unqstwMAAMcZDzPreHiNAACc2Yo1sxb0GekAAAAAADDeCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQIKQDgAAAAAACUI6AAAAAAAkCOkAAAAAAJAgpAMAAAAAQMKoQvr69etj5syZUVVVFfX19bF9+/bk+meeeSYuvfTSqKqqissvvzy2bt06qs0CAADDmc0BAKD4Cg7pmzdvjubm5mhpaYmdO3fGrFmzoqmpKd55550R17/yyitx0003xS233BK7du2KBQsWxIIFC+KXv/zlH715AAAYz8zmAABQGmVZlmWF3FBfXx9XXXVVrFu3LiIiBgcHo66uLu68885YsWLFcesXLlwYR44ciR//+MdD1/7yL/8yZs+eHRs2bBjxOfr6+qKvr2/ozz09PXHhhRfGgQMHorq6upDtAgBASfT29kZdXV0cOnQoampqSvKcxZ7NzeUAAJxpijWXTyhkcX9/f+zYsSNWrlw5dK28vDwaGxujo6NjxHs6Ojqiubl52LWmpqZ44YUXTvg8ra2t8cADDxx3va6urpDtAgBAyf3nf/5nSUJ6KWZzczkAAGeqUz2XFxTSDx48GAMDA1FbWzvsem1tbezZs2fEe7q6ukZc39XVdcLnWbly5bAB/9ChQ/HRj3409u/fX7J393Bm+eD/afLuKE7EGSEfZ4R8nBHy+eDd2hdccEFJnq8Us7m5nEL5u5J8nBHycUbIxxkhn2LN5QWF9FLJ5XKRy+WOu15TU+MfEJKqq6udEZKcEfJxRsjHGSGf8vKCv4botGUuZ7T8XUk+zgj5OCPk44yQz6meywv6aZMnT46Kioro7u4edr27uzumTp064j1Tp04taD0AAJCf2RwAAEqnoJBeWVkZc+bMifb29qFrg4OD0d7eHg0NDSPe09DQMGx9RMRLL710wvUAAEB+ZnMAACidgj/apbm5OZYsWRJz586NefPmxdq1a+PIkSOxdOnSiIhYvHhxzJgxI1pbWyMi4q677orrrrsuHnnkkbjxxhtj06ZN8Ytf/CIef/zxk37OXC4XLS0tI/5aKUQ4I+TnjJCPM0I+zgj5jMUZKfVs7p8D8nFGyMcZIR9nhHycEfIp1hkpy7IsK/SmdevWxbe//e3o6uqK2bNnx//6X/8r6uvrIyLif/yP/xEzZ86Mp556amj9M888E/fdd1+89dZb8Rd/8Rfx0EMPxQ033HDKXgQAAIxXZnMAACi+UYV0AAAAAAAYL07tV5cCAAAAAMBZRkgHAAAAAIAEIR0AAAAAABKEdAAAAAAASDhtQvr69etj5syZUVVVFfX19bF9+/bk+meeeSYuvfTSqKqqissvvzy2bt1aop0yVgo5Ixs3boxrr702Jk2aFJMmTYrGxsa8Z4ozX6F/j3xg06ZNUVZWFgsWLCjuBhlThZ6PQ4cOxbJly2LatGmRy+Xikksu8b81Z7lCz8jatWvj4x//eJxzzjlRV1cXy5cvj9///vcl2i2l9rOf/Szmz58f06dPj7KysnjhhRfy3rNt27b49Kc/HblcLj72sY/FU089VfR9ngrmcvIxl5OPuZx8zObkYzYnZcxm8+w0sGnTpqyysjJ78skns3//93/Pbrvttuz888/Puru7R1z/85//PKuoqMgeeuih7LXXXsvuu+++bOLEidmrr75a4p1TKoWekZtvvjlbv359tmvXrmz37t3Z3/3d32U1NTXZf/zHf5R455RKoWfkA/v27ctmzJiRXXvttdnnP//50myWkiv0fPT19WVz587Nbrjhhuzll1/O9u3bl23bti3r7Ows8c4plULPyA9+8IMsl8tlP/jBD7J9+/ZlL774YjZt2rRs+fLlJd45pbJ169bs3nvvzZ577rksIrLnn38+uX7v3r3ZueeemzU3N2evvfZa9p3vfCerqKjI2traSrPhUTKXk4+5nHzM5eRjNicfszn5jNVsflqE9Hnz5mXLli0b+vPAwEA2ffr0rLW1dcT1X/jCF7Ibb7xx2LX6+vrs7//+74u6T8ZOoWfkDx09ejQ777zzsu9///vF2iJjbDRn5OjRo9nVV1+dfe9738uWLFliYD+LFXo+vvvd72YXXXRR1t/fX6otMsYKPSPLli3L/uqv/mrYtebm5uyaa64p6j45PZzMsP61r30t+9SnPjXs2sKFC7OmpqYi7uyPZy4nH3M5+ZjLycdsTj5mcwpRytl8zD/apb+/P3bs2BGNjY1D18rLy6OxsTE6OjpGvKejo2PY+oiIpqamE67nzDaaM/KH3nvvvXj//ffjggsuKNY2GUOjPSPf+MY3YsqUKXHLLbeUYpuMkdGcjx/96EfR0NAQy5Yti9ra2rjsssti9erVMTAwUKptU0KjOSNXX3117NixY+hXTPfu3Rtbt26NG264oSR75vR3Js6r5nLyMZeTj7mcfMzm5GM2pxhO1cw64VRuajQOHjwYAwMDUVtbO+x6bW1t7NmzZ8R7urq6Rlzf1dVVtH0ydkZzRv7Q3XffHdOnTz/uHxrODqM5Iy+//HI88cQT0dnZWYIdMpZGcz727t0b//qv/xpf/OIXY+vWrfHmm2/Gl7/85Xj//fejpaWlFNumhEZzRm6++eY4ePBgfOYzn4ksy+Lo0aNxxx13xD333FOKLXMGONG82tvbG7/73e/inHPOGaOdnZi5nHzM5eRjLicfszn5mM0phlM1m4/5O9Kh2NasWRObNm2K559/PqqqqsZ6O5wGDh8+HIsWLYqNGzfG5MmTx3o7nIYGBwdjypQp8fjjj8ecOXNi4cKFce+998aGDRvGemucJrZt2xarV6+Oxx57LHbu3BnPPfdcbNmyJR588MGx3hrAactczh8yl3MyzObkYzanVMb8HemTJ0+OioqK6O7uHna9u7s7pk6dOuI9U6dOLWg9Z7bRnJEPPPzww7FmzZr4yU9+EldccUUxt8kYKvSM/OpXv4q33nor5s+fP3RtcHAwIiImTJgQr7/+elx88cXF3TQlM5q/Q6ZNmxYTJ06MioqKoWuf+MQnoqurK/r7+6OysrKoe6a0RnNG7r///li0aFHceuutERFx+eWXx5EjR+L222+Pe++9N8rLvVdhvDvRvFpdXX1avhs9wlxOfuZy8jGXk4/ZnHzM5hTDqZrNx/wkVVZWxpw5c6K9vX3o2uDgYLS3t0dDQ8OI9zQ0NAxbHxHx0ksvnXA9Z7bRnJGIiIceeigefPDBaGtri7lz55Ziq4yRQs/IpZdeGq+++mp0dnYOPT73uc/F9ddfH52dnVFXV1fK7VNko/k75Jprrok333xz6F/kIiLeeOONmDZtmkH9LDSaM/Lee+8dN5B/8C93x77vhvHuTJxXzeXkYy4nH3M5+ZjNycdsTjGcspm1oK8mLZJNmzZluVwue+qpp7LXXnstu/3227Pzzz8/6+rqyrIsyxYtWpStWLFiaP3Pf/7zbMKECdnDDz+c7d69O2tpackmTpyYvfrqq2P1EiiyQs/ImjVrssrKyuzZZ5/NfvOb3ww9Dh8+PFYvgSIr9Iz8oSVLlmSf//znS7RbSq3Q87F///7svPPOy77yla9kr7/+evbjH/84mzJlSvbNb35zrF4CRVboGWlpacnOO++87J//+Z+zvXv3Zv/yL/+SXXzxxdkXvvCFsXoJFNnhw4ezXbt2Zbt27coiInv00UezXbt2Zb/+9a+zLMuyFStWZIsWLRpav3fv3uzcc8/N/vEf/zHbvXt3tn79+qyioiJra2sbq5dwUszl5GMuJx9zOfmYzcnHbE4+YzWbnxYhPcuy7Dvf+U524YUXZpWVldm8efOyf/u3fxv6z6677rpsyZIlw9b/8Ic/zC655JKssrIy+9SnPpVt2bKlxDum1Ao5Ix/96EeziDju0dLSUvqNUzKF/j3y/zOwn/0KPR+vvPJKVl9fn+Vyueyiiy7KvvWtb2VHjx4t8a4ppULOyPvvv599/etfzy6++OKsqqoqq6ury7785S9n//Vf/1X6jVMSP/3pT0ecLT44F0uWLMmuu+664+6ZPXt2Vln5f9u5YxsAYRiIoqJhkCzuUc0EnKUUhOK9CVKevuTcvdbqqvr83TvsciZ2ORO7nIltzsQ2Jzm1za9uNw4AAAAAAPDm+B/pAAAAAADwZ0I6AAAAAAAEQjoAAAAAAARCOgAAAAAABEI6AAAAAAAEQjoAAAAAAARCOgAAAAAABEI6AAAAAAAEQjoAAAAAAARCOgAAAAAABEI6AAAAAAAED8/20qzBoD6eAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Summary Statistics:\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "\n",
    "# Visualize TensorBoard logs in Jupyter notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard in the notebook\n",
    "%tensorboard --logdir={LOG_PATH}\n",
    "\n",
    "# Alternatively, extract and plot data directly with matplotlib\n",
    "def plot_tensorboard_data(log_dir):\n",
    "    \"\"\"Extract and plot key metrics from TensorBoard logs\"\"\"\n",
    "    \n",
    "    # Load the event file\n",
    "    ea = event_accumulator.EventAccumulator(log_dir, \n",
    "        size_guidance={\n",
    "            event_accumulator.SCALARS: 0,  # 0 means load all\n",
    "        })\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Get available tags (metrics)\n",
    "    tags = ea.Tags()['scalars']\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Metrics to plot\n",
    "    key_metrics = [\n",
    "        'train/mean_reward_100', \n",
    "        'train/epsilon', \n",
    "        'train/loss',\n",
    "        'eval/mean_reward'\n",
    "    ]\n",
    "    \n",
    "    # Plot each metric\n",
    "    for i, metric in enumerate(key_metrics):\n",
    "        if metric in tags:\n",
    "            events = ea.Scalars(metric)\n",
    "            steps = [event.step for event in events]\n",
    "            values = [event.value for event in events]\n",
    "            \n",
    "            ax = axes[i]\n",
    "            ax.plot(steps, values)\n",
    "            ax.set_title(metric)\n",
    "            ax.set_xlabel('Training Steps')\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a dataframe with all metrics for more detailed analysis\n",
    "    metrics_dict = {}\n",
    "    for tag in tags:\n",
    "        events = ea.Scalars(tag)\n",
    "        metrics_dict[tag] = {event.step: event.value for event in events}\n",
    "    \n",
    "    # Convert to pandas DataFrame for easy analysis\n",
    "    all_steps = sorted(set().union(*[set(d.keys()) for d in metrics_dict.values()]) if metrics_dict else [])\n",
    "    df = pd.DataFrame(index=all_steps)\n",
    "    \n",
    "    for tag, values in metrics_dict.items():\n",
    "        df[tag] = pd.Series(values)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Plot the data\n",
    "print(\"Extracting and plotting TensorBoard metrics...\")\n",
    "try:\n",
    "    metrics_df = plot_tensorboard_data(LOG_PATH)\n",
    "\n",
    "    # Display summary statistics\n",
    "    print(\"\\nTraining Summary Statistics:\")\n",
    "    for col in metrics_df.columns:\n",
    "        if 'reward' in col:\n",
    "            print(f\"{col}:\")\n",
    "            print(f\"  Max: {metrics_df[col].max():.2f}\")\n",
    "            print(f\"  Last: {metrics_df[col].iloc[-1] if not pd.isna(metrics_df[col].iloc[-1]) else np.nan:.2f}\")\n",
    "            print(f\"  Mean: {metrics_df[col].mean():.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing TensorBoard data: {e}\")\n",
    "    print(\"Check if the log directory exists and contains valid TensorBoard event files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
